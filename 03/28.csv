"1303.6868","Bradley James Kavanagh","Bradley J. Kavanagh, Anne M. Green","Model independent determination of the dark matter mass from direct
  detection experiments","5 pages, 3 figures. Minor changes. Matches version published in PRL","Phys. Rev. Lett. 111, 031302 (2013)","10.1103/PhysRevLett.111.031302",,"astro-ph.CO hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Determining the dark matter (DM) mass is of paramount importance for
understanding dark matter. We present a novel parametrization of the DM speed
distribution which will allow the DM mass to be accurately measured using data
from Weakly Interacting Massive Particle (WIMP) direct detection experiments.
Specifically, we parametrize the natural logarithm of the speed distribution as
a polynomial in the speed v. We demonstrate, using mock data from upcoming
experiments, that by fitting the WIMP mass and interaction cross-section, along
with the polynomial coefficients, we can accurately reconstruct both the WIMP
mass and speed distribution. This new method is the first demonstration that an
accurate, unbiased reconstruction of the WIMP mass is possible without prior
assumptions about the distribution function. We anticipate that this technique
will be invaluable in the analysis of future experimental data.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:02:33 GMT""},{""version"":""v2"",""created"":""Thu, 18 Jul 2013 09:48:22 GMT""}]","2013-07-19"
"1303.6869","Nicolae Radu Zabet","Nicolae Radu Zabet, Robert Foy and Boris Adryan","The influence of transcription factor competition on the relationship
  between occupancy and affinity","28 pages, 14 figures, 6 tables","PLoS ONE 8:9 (2013) e73714","10.1371/journal.pone.0073714",,"q-bio.MN q-bio.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transcription factors (TFs) are proteins that bind to specific sites on the
DNA and regulate gene activity. Identifying where TF molecules bind and how
much time they spend on their target sites is key for understanding
transcriptional regulation. It is usually assumed that the free energy of
binding of a TF to the DNA (the affinity of the site) is highly correlated to
the amount of time the TF remains bound (the occupancy of the site). However,
knowing the binding energy is not sufficient to infer actual binding site
occupancy. This mismatch between the occupancy predicted by the affinity and
the observed occupancy may be caused by various factors, such as TF abundance,
competition between TFs or the arrangement of the sites on the DNA. We
investigated the relationship between the affinity of a TF for a set of binding
sites and their occupancy. In particular, we considered the case of lac
repressor (lacI) in E.coli and performed stochastic simulations of the TF
dynamics on the DNA for various combinations of lacI abundance in competition
with TFs that contribute to macromolecular crowding. Our results showed that
for medium and high affinity sites, TF competition does not play a significant
role in genomic occupancy, except in cases when the abundance of lacI is
significantly increased or when a low-information content PWM was used.
Nevertheless, for medium and low affinity sites, an increase in TF abundance
(for both lacI or other molecules) leads to an increase in occupancy at several
sites. Keywords: facilitated diffusion, Position Weight Matrix, thermodynamic
equilibrium, motif information content, molecular crowding
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:03:09 GMT""}]","2013-10-01"
"1303.6870","Brian Kloppenborg","Brian K. Kloppenborg, Roger Pieri, Heinz-Bernd Eggenstein, Grigoris
  Maravelias, Tom Pearson","A Demonstration of Accurate Wide-field V-band Photometry Using a
  Consumer-grade DSLR Camera","Published in JAAVSO v40 n2: http://www.aavso.org/ejaavso402815. See
  also: http://adsabs.harvard.edu/abs/2012JAVSO..40..815K","2012JAVSO..40..815K",,,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The authors examined the suitability of using a Digital Single Lens Reflex
(DSLR) camera for stellar photometry and, in particular, investigated wide
field exposures made with minimal equipment for analysis of bright variable
stars. A magnitude-limited sample of stars was evaluated exhibiting a wide
range of (B-V) colors taken from four fields between Cygnus and Draco.
Experiments comparing green channel DSLR photometry with VT photometry of the
Tycho 2 catalogue showed very good agreement. Encouraged by the results of
these comparisons, a method for performing color-based transformations to the
more widely used Johnson V filter band was developed and tested. This method is
similar to that recommended for Tycho 2 VT data. The experimental evaluation of
the proposed method led to recommendations concerning the feasibility of high
precision DSLR photometry for certain types of variable star projects. Most
importantly, we have demonstrated that DSLR cameras can be used as accurate,
wide field photometers with only a minimal investment of funds and time.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:03:19 GMT""}]","2013-03-28"
"1303.6871","Brian Kloppenborg","Brian Kloppenborg, Jeffery Hopkins, Robert Stencel","An Analysis of the Long-term Photometric Behavior of epsilon Aurigae","Published in JAAVSO v40n2 http://www.aavso.org/jaavso-v40n2 Also see
  http://adsabs.harvard.edu/abs/2012JAVSO..40..647K","2012JAVSO..40..647K",,,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The lure of a 50 percent reduction in light has brought a multitude of
observers and researchers to epsilon Aur every twenty-seven years, but few have
paid attention to the system outside of eclipse. As early as the late 1800s, it
was clear that the system undergoes some form of quasi-periodic variation
outside of totality, but few considered this effect in their research until the
mid-1950s. In this work we focus exclusively on the out-of-eclipse (OOE)
variations seen in this system. We have digitized twenty-seven sources of
historic photometry from eighty-one different observers. Two of these sources
provide twenty-seven years of inter-eclipse UBV photometry which we have
analyzed using modern period finding techniques. We have discovered the F-star
variations are multiperiodic with at least two periods that evolve in time at
delta(Period) approximately -1.5 day/year. These periods are detected when they
manifest as near-sinusoidal variations at 3,200-day intervals. We discuss our
work in an evolutionary context by comparing the behavior found in e Aur with
bona-fide supergiant and post-AGB stars of similar spectral type. Based upon
our qualitative comparison, we find the photometric behavior of the F-star in
the e Aur system is more indicative of supergiant behavior. Therefore the star
is more likely to be a ""traditional supergiant"" than a post-AGB object. We
encourage continued photometric monitoring of this system to test our
predictions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:08:55 GMT""}]","2013-03-28"
"1303.6872","Jakub Radoszewski","Maxime Crochemore, Costas S. Iliopoulos, Tomasz Kociumaka, Marcin
  Kubica, Alessio Langiu, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter,
  Tomasz Walen","Order-Preserving Suffix Trees and Their Algorithmic Applications",,,,,"cs.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recently Kubica et al. (Inf. Process. Let., 2013) and Kim et al. (submitted
to Theor. Comp. Sci.) introduced order-preserving pattern matching. In this
problem we are looking for consecutive substrings of the text that have the
same ""shape"" as a given pattern. These results include a linear-time
order-preserving pattern matching algorithm for polynomially-bounded alphabet
and an extension of this result to pattern matching with multiple patterns. We
make one step forward in the analysis and give an
$O(\frac{n\log{n}}{\log\log{n}})$ time randomized algorithm constructing suffix
trees in the order-preserving setting. We show a number of applications of
order-preserving suffix trees to identify patterns and repetitions in time
series.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:13:03 GMT""}]","2013-03-28"
"1303.6873","Robin Stephenson","Robin Stephenson (CEREMADE)","General Fragmentation Trees",,,,,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that the genealogy of any self-similar fragmentation process can be
encoded in a compact measured real tree. Under some Malthusian hypotheses, we
compute the fractal Hausdorff dimension of this tree through the use of a
natural measure on the set of its leaves. This generalizes previous work of
Haas and Miermont which was restricted to conservative fragmentation processes.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:13:06 GMT""},{""version"":""v2"",""created"":""Sun, 31 Mar 2013 08:51:42 GMT""}]","2013-04-02"
"1303.6874","Elisa Gorla","Emanuela De Negri and Elisa Gorla","Invariants of ideals generated by pfaffians",,"Commutative Algebra and Its Connections to Geometry, A. Corso and
  C. Polini Editors, Contemporary Mathematics 555 (2011), 47-62",,,"math.AC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ideals generated by pfaffians are of interest in commutative algebra and
algebraic geometry, as well as in combinatorics. In this article we compute
multiplicity and Castelnuovo-Mumford regularity of pfaffian ideals of ladders.
We give explicit formulas for some families of ideals, and indicate a procedure
that allows to recursively compute the invariants of any pfaffian ideal of
ladder. Our approach makes an essential use of liaison theory.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:15:43 GMT""}]","2013-03-28"
"1303.6875","Serge Bouc","Serge Bouc (LAMFA)","Fused Mackey functors",,,,,"math.GR math.CT math.RA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Let $G$ be a finite group. In [HTW], Hambleton, Taylor and Williams have
considered the question of comparing Mackey functors for $G$ and biset functors
defined on subgroups of $G$ and bifree bisets as morphisms. This paper proposes
a different approach to this problem, from the point of view of various
categories of $G$-sets. In particular, the category of fused $G$-sets is
introduced, as well its category of spans. The fused Mackey functors for $G$
over a commutative ring $R$ are defined as $R$-linear functors from this
($R$-linearized) category of spans to $R$-modules. They form an abelian
subcategory of the category of Mackey functors for $G$ over $R$, equivalent
(for $R=Z$) to the category to the category of conjugation Mackey functors of
[HTW]. The category of fused Mackey functors is also equivalent to the category
of modules over the fused Mackey algebra, which is a quotient of the usual
Mackey algebra of $G$ over $R$. Reference: [HTW] I. Hambleton, L. R. Taylor,
and E. B. Williams. Mackey functors and bisets. Geom. Dedicata, 148:157--174,
2010.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:17:01 GMT""}]","2013-03-28"
"1303.6876","Ben Van Duppen","B. Van Duppen, F. M. Peeters","Four band tunneling in bilayer graphene",,"Phys. Rev. B 87, 205427 (2013)","10.1103/PhysRevB.87.205427",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The conductance, the transmission and the reflection probabilities through
rectangular potential barriers and pn-junctions are obtained for bilayer
graphene taking into account the four bands of the energy spectrum. We have
evaluated the importance of the skew hopping parameters {\gamma}3 and {\gamma}4
to these properties and show that for energies E>{\gamma}1/100 their effect is
negligible. For high energies two modes of propagation exist and we investigate
scattering between these modes. For perpendicular incidence both propagation
modes are decoupled and scattering between them is forbidden. This extends the
concept of pseudospin as defined within the two band approximation to a four
band model and corresponds to the (anti)symmetry of the wavefunctions under
in-plane mirroring. New transmission resonances are found that appear as sharp
peaks in the conductance which are absent in the two band approximation. The
application of an interlayer bias to the system: 1) breaks the pseudospin
structure, 2) opens a bandgap that results in a distinct feature of suppressed
transmission in the conductance, and 3) breaks the angular symmetry with
respect to normal incidence in the transmission and reflection.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:19:28 GMT""}]","2013-05-28"
"1303.6877","Alexander Enzenh\""ofer","Alexander Enzenh\""ofer (for the KM3NeT Collaboration)","Acoustic Calibration for the KM3NeT Pre-Production Module","4 pages, 4 figures. To be published in: Nuclear Inst. and Methods in
  Physics Research, A. Conference proceedings of the ""VLVnT11 - Very Large
  Volume Neutrino Telescope Workshop (2011)""","Nuclear Inst. and Methods in Physics Research, A (2013), pp.
  211-214","10.1016/j.nima.2012.12.074",,"astro-ph.IM physics.ins-det","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The proposed large scale Cherenkov neutrino telescope KM3NeT will carry
photo-sensors on flexible structures, the detection units. The Mediterranean
Sea, where KM3NeT will be installed, constitutes a highly dynamic environment
in which the detection units are constantly in motion. Thus it is necessary to
monitor the exact sensor positions continuously to achieve the desired
resolution for the neutrino telescope. A common way to perform this monitoring
is the use of acoustic positioning systems with emitters and receivers based on
the piezoelectric effect. The acoustic receivers are attached to detection
units whereas the emitters are located at known positions on the sea floor.
There are complete commercial systems for this application with sufficient
precision. But these systems are limited in the use of their data and
inefficient as they were designed to perform only this single task. Several
working groups in the KM3NeT consortium are cooperating to custom-design a
positioning system for the specific requirements of KM3NeT. Most of the studied
solutions hold the possibility to extend the application area from positioning
to additional tasks like acoustic particle detection or monitoring of the
deep-sea acoustic environment. The KM3NeT Pre-Production Module (PPM) is a test
system to verify the correct operation and interoperability of the major
involved hardware and software components developed for KM3NeT. In the context
of the PPM, alternative designs of acoustic sensors including small
piezoelectric elements equipped with preamplifiers inside the same housing as
the optical sensors will be tested. These will be described in this article.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:22:01 GMT""}]","2019-08-14"
"1303.6878","Christof Wetterich","C. Wetterich","A Universe without expansion","new references, extended discussion of absence of big bang
  singularity, 5 pages. arXiv admin note: text overlap with arXiv:1308.1019",,"10.1016/j.dark.2013.10.002",,"astro-ph.CO gr-qc hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We discuss a cosmological model where the universe shrinks rather than
expands during the radiation and matter dominated periods. Instead, the Planck
mass and all particle masses grow exponentially, with the size of atoms
shrinking correspondingly. Only dimensionless ratios as the distance between
galaxies divided by the atom radius are observable. Then the cosmological
increase of this ratio can also be attributed to shrinking atoms. We present a
simple model where the masses of particles arise from a scalar ""cosmon"" field,
similar to the Higgs scalar. The potential of the cosmon is responsible for
inflation and the present dark energy. Our model is compatible with all present
observations. While the value of the cosmon field increases, the curvature
scalar is almost constant during all cosmological epochs. Cosmology has no big
bang singularity. There exist other, equivalent choices of field variables for
which the universe shows the usual expansion or is static during the radiation
or matter dominated epochs. For those ""field coordinates"" the big bang is
singular. Thus the big bang singularity turns out to be related to a singular
choice of field coordinates.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:22:39 GMT""},{""version"":""v2"",""created"":""Fri, 5 Jul 2013 12:14:14 GMT""},{""version"":""v3"",""created"":""Tue, 30 Jul 2013 12:34:29 GMT""},{""version"":""v4"",""created"":""Tue, 12 Nov 2013 09:20:52 GMT""}]","2013-11-13"
"1303.6880","Hassan Ghozlan","Hassan Ghozlan and Gerhard Kramer","Multi-sample Receivers Increase Information Rates for Wiener Phase Noise
  Channels","Submitted to Globecom 2013",,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A waveform channel is considered where the transmitted signal is corrupted by
Wiener phase noise and additive white Gaussian noise (AWGN). A discrete-time
channel model is introduced that is based on a multi-sample receiver. Tight
lower bounds on the information rates achieved by the multi-sample receiver are
computed by means of numerical simulations. The results show that oversampling
at the receiver is beneficial for both strong and weak phase noise at high
signal-to-noise ratios. The results are compared with results obtained when
using other discrete-time models.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:24:08 GMT""}]","2013-03-28"
"1303.6881","Richard Clegg","Eleni Mykoniati, Laurence Latif, Raul Landa, Ben Yang, Richard G.
  Clegg, David Griffin, Miguel Rio","Distributed Overlay Anycast Table using Space filling curves","7 pages, 4 figures","Proceedings of IEEE INFOCOM Workshop, Global Internet Symposium GI
  2009","10.1109/INFCOMW.2009.5072131",,"cs.NI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we present the \emph{Distributed Overlay Anycast Table}, a
structured overlay that implements application-layer anycast, allowing the
discovery of the closest host that is a member of a given group. One
application is in locality-aware peer-to-peer networks, where peers need to
discover low-latency peers participating in the distribution of a particular
file or stream. The DOAT makes use of network delay coordinates and a space
filling curve to achieve locality-aware routing across the overlay, and Bloom
filters to aggregate group identifiers. The solution is designed to optimise
both accuracy and query time, which are essential for real-time applications.
We simulated DOAT using both random and realistic node distributions. The
results show that accuracy is high and query time is low.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:24:21 GMT""}]","2013-03-28"
"1303.6882","Romain Abraham","Romain Abraham (MAPMO), Jean-Francois Delmas (CERMICS)","$\beta$-coalescents and stable Galton-Watson trees",,,,,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Representation of coalescent process using pruning of trees has been used by
Goldschmidt and Martin for the Bolthausen-Sznitman coalescent and by Abraham
and Delmas for the $\beta(3/2,1/2)$-coalescent. By considering a pruning
procedure on stable Galton-Watson tree with $n$ labeled leaves, we give a
representation of the discrete $\beta(1+\alpha,1-\alpha)$-coalescent, with
$\alpha\in [1/2,1)$ starting from the trivial partition of the $n$ first
integers. The construction can also be made directly on the stable continuum
L{\'e}vy tree, with parameter $1/\alpha$, simultaneously for all $n$. This
representation allows to use results on the asymptotic number of coalescence
events to get the asymptotic number of cuts in stable Galton-Watson tree (with
infinite variance for the reproduction law) needed to isolate the root. Using
convergence of the stable Galton-Watson tree conditioned to have infinitely
many leaves, one can get the asymptotic distribution of blocks in the last
coalescence event in the $\beta(1+\alpha,1-\alpha)$-coalescent.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:25:15 GMT""},{""version"":""v2"",""created"":""Wed, 7 Jan 2015 17:43:10 GMT""}]","2015-01-08"
"1303.6883","Thomas Dufaud","Thomas Dufaud (IRISA / INRIA Rennes), Tromeur-Dervout Damien (ICJ)","ARAS: Fully algebraic Two-level domain decomposition precondition
  technique with approximation on course interfaces Fully","33 pages",,,,"math.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper focuses on the development of a two-level preconditioner based on
a fully algebraical enhancement of a Schwarz domain decomposition method. We
consider the purely divergence of a Restricted Additive Scwharz iterative
process leading to the preconditioner developped by X.-C. Cai and M. Sarkis in
SIAM Journal of Scientific Computing, Vol. 21 no. 2, 1999. The convergence of
vectorial sequence of traces of this process on the artificial interface can be
accelerated by an Aitken acceleration technique as proposed in the work of M.
Garbey and D. Tromeur-Dervout, in International Journal for Numerical Methods
in Fluids, Vol. 40, no. 12,2002. We propose a formulation of the Aitken-Schwarz
technique as a preconditioning technique called Aitken-RAS 1 . The Aitken
acceleration is performed in a reduced space to save computing or permit fully
algebraic computation of the accelerated solution without knowledge of the
underlying equations. A convergence study of the Aitken-RAS preconditioner is
proposed also application on industrial problem.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:26:22 GMT""}]","2013-03-28"
"1303.6884","Arnaud Guillin","Patrick Cattiaux (IMT), Arnaud Guillin","Semi Log-Concave Markov Diffusions",,,,,"math.PR math.FA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we intend to give a comprehensive approach of functional
inequalities for diffusion processes under some ""curvature"" assumptions. Our
notion of curvature coincides with the usual $\Gamma_2$ curvature of Bakry and
Emery in the case of a (reversible) drifted Brownian motion, but differs for
more general diffusion processes. Our approach using simple coupling arguments
together with classical stochastic tools, allows us to obtain new results, to
recover and to extend already known results, giving in many situations explicit
(though non optimal) bounds. In particular, we show new results for
gradient/semigroup commutation in the log concave case. Some new convergence to
equilibrium in the granular media equation is also exhibited.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:27:59 GMT""}]","2013-03-28"
"1303.6885","Hui Kong","Hui Kong, Fei He, Xiaoyu Song, William N. N. Hung and Ming Gu","Exponential-Condition-Based Barrier Certificate Generation for Safety
  Verification of Hybrid Systems","18 pages, 7 figures",,,,"cs.SE math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A barrier certificate is an inductive invariant function which can be used
for the safety verification of a hybrid system. Safety verification based on
barrier certificate has the benefit of avoiding explicit computation of the
exact reachable set which is usually intractable for nonlinear hybrid systems.
In this paper, we propose a new barrier certificate condition, called
Exponential Condition, for the safety verification of semi-algebraic hybrid
systems. The most important benefit of Exponential Condition is that it has a
lower conservativeness than the existing convex condition and meanwhile it
possesses the property of convexity. On the one hand, a less conservative
barrier certificate forms a tighter over-approximation for the reachable set
and hence is able to verify critical safety properties. On the other hand, the
property of convexity guarantees its solvability by semidefinite programming
method. Some examples are presented to illustrate the effectiveness and
practicality of our method.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:28:35 GMT""}]","2013-03-28"
"1303.6886","Jordan Raddick","M. Jordan Raddick, Georgia Bracey, Pamela L. Gay, Chris J. Lintott,
  Carie Cardamone, Phil Murray, Kevin Schawinski, Alexander S. Szalay, Jan
  Vandenberg","Galaxy Zoo: Motivations of Citizen Scientists","41 pages, including 6 figures and one appendix. In press at Astronomy
  Education Review",,,,"physics.ed-ph astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Citizen science, in which volunteers work with professional scientists to
conduct research, is expanding due to large online datasets. To plan projects,
it is important to understand volunteers' motivations for participating. This
paper analyzes results from an online survey of nearly 11,000 volunteers in
Galaxy Zoo, an astronomy citizen science project. Results show that volunteers'
primary motivation is a desire to contribute to scientific research. We
encourage other citizen science projects to study the motivations of their
volunteers, to see whether and how these results may be generalized to inform
the field of citizen science.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:28:51 GMT""}]","2013-03-28"
"1303.6887","Richard Clegg","Eleni Mykoniati, Raul Landa, Spiros Spirou, Richard G. Clegg, Lawrence
  Latif, David Griffin, Miguel Rio","Scalable peer-to-peer streaming for live entertainment content","11 pages, 6 figures","IEEE Communications 46(12) pp40-46 2008","10.1109/MCOM.2008.4689206",,"cs.NI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a system for streaming live entertainment content over the
Internet originating from a single source to a scalable number of consumers
without resorting to centralised or provider- provisioned resources. The system
creates a peer-to-peer overlay network, which attempts to optimise use of
existing capacity to ensure quality of service, delivering low start-up delay
and lag in playout of the live content. There are three main aspects of our
solution. Firstly, a swarming mechanism that constructs an overlay topology for
minimising propagation delays from the source to end consumers. Secondly, a
distributed overlay anycast system that uses a location-based search algorithm
for peers to quickly find the closest peers in a given stream. Finally, a novel
incentives mechanism that encourages peers to donate capacity even when the
user is not actively consuming content.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:34:25 GMT""}]","2013-03-28"
"1303.6888","Oktay Mukhtarov","O. Sh. Mukhtarov and K.Aydemir","Asymptotic formulas for eigenvalues and eigenfunctions of a new
  boundary-value-transmission problem",,,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we are concerned with a new class of BVP' s consisting of
eigendependent boundary conditions and two supplementary transmission
conditions at one interior point. By modifying some techniques of classical
Sturm-Liouville theory and suggesting own approaches we find asymptotic
formulas for the eigenvalues and eigenfunction.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:42:21 GMT""}]","2013-03-28"
"1303.6889","Samuel Taylor","Samuel J. Taylor","Right-angled Artin groups and Out(F_n) I: quasi-isometric embeddings","37 pages, 4 figures",,,,"math.GT math.GR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We construct quasi-isometric embeddings from right-angled Artin groups into
the outer automorphism group of a free group. These homomorphisms are in
analogy with those constructed in \cite{CLM}, where the target group is the
mapping class group of a surface. Toward this goal, we develop tools in the
free group setting that mirror those for surface groups as well as discuss
various analogs of subsurface projection; these may be of independent interest.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:44:17 GMT""}]","2013-03-28"
"1303.6890","Oktay Mukhtarov","K. Aydemir and O.Sh.Mukhtarov","Selfadjoint realization of boundary-value problems with interior
  singularities",,,,,"math.CA math-ph math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The purpose of this paper is to investigate some spectral properties of
Sturm-Liouville type problems with interior singularities. Some of the
mathematical aspects necessary for developing own technique presented. By
applying this technique we construct some special solutions of the homogeneous
equation and present a formula and the existence conditions of Green's
function. Further based on this results and introducing operator treatment in
adequate Hilbert space we derive the resolvent operator and prove
selfadjointness of the considered problem.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:45:15 GMT""}]","2013-03-28"
"1303.6891","Aaron Manalaysay","Laura Baudis, Hrvoje Dujmovic, Christopher Geis, Andreas James,
  Alexander Kish, Aaron Manalaysay, Teresa Marrodan Undagoitia, Marc Schumann","Response of liquid xenon to Compton electrons down to 1.5 keV","14 pages, 10 figures. v2: Figure 8 updated, now including the NEST
  prediction for field quenching. v3: modified values of derived energy
  thresholds in the four DM experiments considered","Phys.Rev.D87:115015,2013","10.1103/PhysRevD.87.115015",,"astro-ph.IM physics.ins-det","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The response of liquid xenon to low-energy electronic recoils is relevant in
the search for dark-matter candidates which interact predominantly with atomic
electrons in the medium, such as axions or axion-like particles, as opposed to
weakly interacting massive particles which are predicted to scatter with atomic
nuclei. Recently, liquid-xenon scintillation light has been observed from
electronic recoils down to 2.1 keV, but without applied electric fields that
are used in most xenon dark matter searches. Applied electric fields can reduce
the scintillation yield by hindering the electron-ion recombination process
that produces most of the scintillation photons. We present new results of
liquid xenon's scintillation emission in response to electronic recoils as low
as 1.5 keV, with and without an applied electric field. At zero field, a
reduced scintillation output per unit deposited energy is observed below 10
keV, dropping to nearly 40% of its value at higher energies. With an applied
electric field of 450 V/cm, we observe a reduction of the scintillation output
to about 75% relative to the value at zero field. We see no significant energy
dependence of this value between 1.5 keV and 7.8 keV. With these results, we
estimate the electronic-recoil energy thresholds of ZEPLIN-III, XENON10,
XENON100, and XMASS to be 2.8 keV, 2.5 keV, 2.3 keV, and 1.1 keV, respectively,
validating their excellent sensitivity to low-energy electronic recoils.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:46:45 GMT""},{""version"":""v2"",""created"":""Thu, 4 Apr 2013 09:29:05 GMT""},{""version"":""v3"",""created"":""Wed, 12 Jun 2013 09:50:06 GMT""}]","2013-06-21"
"1303.6892","Oktay Mukhtarov","O. Sh. Mukhtarov and K. Aydemir","Green's function for Sturm-Liouville problem",,,,,"math.CA math-ph math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The purpose of this study is to investigate a new class of boundary value
transmission problems (BVTP's) for Sturm-Liouville equation on two separate
intervals. We introduce modified inner product in direct sum space
$L_{2}[a,c)\oplus L_{2}(c,b]\oplus\mathbb{C}^{2}$ and define symmetric linear
operator in it such a way that the considered problem can be interpreted as an
eigenvalue problem of this operator. Then by suggesting an own approaches we
construct Green's function for problem under consideration and find the
resolvent function for corresponding inhomogeneous problem.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:49:06 GMT""}]","2013-03-28"
"1303.6893","Oktay Mukhtarov","O. Sh. Mukhtarov and K. Aydemir","Expansion Theorem for Sturm-Liouville problems transmission conditions",,,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The purpose of this paper is to extend some spectral properties of regular
Sturm-Liouville problems to the special type discontinuous boundary-value
problem, which consists of a Sturm-Liouville equation together with
eigenparameter-dependent boundary conditions and two supplementary transmission
conditions. We construct the resolvent operator and Green's function and prove
theorems about expansions in terms of eigenfunctions in modified Hilbert space
$L_{2}[a, b]$. \vskip0.3cm\noindent Keywords: Boundary-value problems,
transmission conditions, Resolvent operator, expansion theorem.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:51:47 GMT""}]","2013-03-28"
"1303.6894","Sean Ledger","Sean Ledger","Sharp regularity near an absorbing boundary for solutions to second
  order SPDEs in a half-line with constant coefficients",,"Stochastic Partial Differential Equations: Analysis and
  Computations, 2014, Volume 2, Issue 1, pp 1-26",,,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We prove that the weak version of the SPDE problem \begin{align*} dV_{t}(x) &
= [-\mu V_{t}'(x) + \frac{1}{2} (\sigma_{M}^{2} + \sigma_{I}^{2})V_{t}""(x)]dt -
\sigma_{M} V_{t}'(x)dW^{M}_{t}, \quad x > 0, \\ V_{t}(0) &= 0 \end{align*} with
a specified bounded initial density, $V_{0}$, and $W$ a standard Brownian
motion, has a unique solution in the class of finite-measure valued processes.
The solution has a smooth density process which has a probabilistic
representation and shows degeneracy near the absorbing boundary. In the
language of weighted Sobolev spaces, we describe the precise order of
integrability of the density and its derivatives near the origin, and we relate
this behaviour to a two-dimensional Brownian motion in a wedge whose angle is a
function of the ratio $\sigma_{M}/\sigma_{I}$. Our results are sharp: we
demonstrate that better regularity is unattainable.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:51:48 GMT""},{""version"":""v2"",""created"":""Thu, 23 Jul 2015 15:06:20 GMT""}]","2015-07-24"
"1303.6895","Ilias Amrani","Ilias Amrani","The mapping space of unbounded differential graded algebras","We extend our previous results to any ground commutative ring k",,,,"math.AT math.AG math.KT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we give a concrete description of the higher homotopy groups
(n>0) of the mapping space Map_{Alg}(R,S) for R and S unbounded differential
graded algebras (DGA) over a commutative ring k. In the connective case, we
describe the relation between the higher (negative) Hochschild cohomology
$HH^{-n+1}(R,S)$ and higher homotopy groups $\pi_{n} Map_{Alg}(R,S)$, when
$n>1$.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:55:17 GMT""},{""version"":""v2"",""created"":""Tue, 18 Jun 2013 11:55:46 GMT""},{""version"":""v3"",""created"":""Mon, 27 Jan 2014 19:22:00 GMT""}]","2014-01-28"
"1303.6896","Pierre Magain Mr","Pierre Magain and Virginie Chantry","Gravitational lensing evidence against extended dark matter halos","9 pages, 2 figures, 1 table, submitted to ApJ Letters",,,,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is generally thought that galaxies are embedded in dark matter halos
extending well beyond their luminous matter. The existence of these galactic
halos is mainly derived from the larger than expected velocities of stars and
gas in the outskirts of spiral galaxies. Much less is known about dark matter
around early-type (elliptical or lenticular) galaxies. We use gravitational
lensing to derive the masses of early-type galaxies deflecting light of
background quasars. This provides a robust measurement of the total mass within
the Einstein ring, a circle whose diameter is comparable to the separation of
the different quasar images. We find that the mass-to-light ratio of the
lensing galaxies does not depend on radius, from inner galactic regions out to
several half-light radii. Moreover, its value does not exceed the value
predicted by stellar population models by more than a factor two, which may be
explained by baryonic dark matter alone, without any need for exotic matter.
Our results thus suggest that, if dark matter is present in early-type
galaxies, its amount does not exceed the amount of luminous matter and its
density follows that of luminous matter, in sharp contrast to what is found
from rotation curves of spiral galaxies.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:55:54 GMT""}]","2013-03-28"
"1303.6897","Kazem Azizi","T. M. Aliev, K. Azizi, M. Savci","Analysis of $\gamma^\ast \Lambda \to \Sigma^0$ transition in QCD","13 Pages and 4 Figures",,"10.1103/PhysRevD.87.096013",,"hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The $\gamma^\ast \Lambda \to \Sigma^0$ transition form factors are
investigated within the light--cone QCD sum rules method. Using the most
general form of the interpolating current of $\Sigma^0$ baryon and the
distribution amplitudes of $\Lambda$ baryon we calculate the $Q^2$ dependence
of the electromagnetic form factors. Our result are compared with the
predictions of the covariant spectator quark model.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:56:41 GMT""},{""version"":""v2"",""created"":""Tue, 14 May 2013 13:50:22 GMT""}]","2015-06-15"
"1303.6898","Oktay Mukhtarov","K. Aydemir and O. Sh. Mukhtarov","Modified Expansion Theorem for Sturm-Liouville problem with transmission
  conditions",,,,,"math.CA math.SP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper is devoted to the derivation of expansion a associated with a
discontinuous Sturm-Liouville problems defined on $[-\pi, 0)\cup(0,\pi]$. We
derive an eigenfunction expansion theorem for the Green's function of the
problem as well as a theorem of uniform convergence of a certain class of
functions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:01:43 GMT""}]","2013-03-28"
"1303.6900","Tsorng-Whay Pan Dr.","Suchung Hou, Tsorng-Whay Pan and Roland Glowinski","Circular band formation for incompressible viscous fluid--rigid particle
  mixtures in a rotating cylinder","12 pages; 12 figures",,"10.1103/PhysRevE.89.023013",,"physics.flu-dyn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we have investigated a circular band formation of fluid-rigid
particle mixtures in a fully filled cylinder horizontally rotating about its
cylinder axis by direct numerical simulation. These phenomena are modeled by
the Navier-Stokes equations coupled to the Euler-Newton equations describing
the rigid solid motion of the non-neutrally particles. The formation of
circular bands studied in this paper is not resulted by mutual interaction
between the particles and the periodic inertial waves in the cylinder axis
direction (as suggested in Phys. Rev. E, 72, 021407 (2005)), but due to the
interaction of particles. When a circular band is forming, the part of the band
formed by the particles moving downward becomes more compact due to the
particle interaction strengthened by the downward acceleration from the
gravity. The part of a band formed by the particles moving upward is always
loosening up due to the slow down of the particle motion by the counter effect
of the gravity. To form a compact circular band (not a loosely one), enough
particles are needed to interact among themselves continuously through the
entire circular band at a rotating rate so that the upward diffusion of
particles can be balanced by the compactness process when these particles
moving downward.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:27:00 GMT""}]","2015-06-15"
"1303.6901","Zsolt Paragi","Z. Paragi, A. J. van der Horst, T. Belloni, J. C. A. Miller-Jones, J.
  Linford, G. Taylor, J. Yang, M. A. Garrett, J. Granot, C. Kouveliotou, E.
  Kuulkers, R. A. M. J. Wijers","VLBI observations of the shortest orbital period black hole binary, MAXI
  J1659-152","12 pages, 6 figures, 1 table. Accepted for publication in MNRAS",,"10.1093/mnras/stt545",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The X-ray transient MAXI J1659-152 was discovered by Swift/BAT and it was
initially identified as a GRB. Soon its Galactic origin and binary nature were
established. There exists a wealth of multi-wavelength monitoring data for this
source, providing a great coverage of the full X-ray transition in this
candidate black hole binary system. We obtained two epochs of European VLBI
Network (EVN) electronic-VLBI (e-VLBI) and four epochs of Very Long Baseline
Array (VLBA) data of MAXI J1659-152 which show evidence for outflow in the
early phases. The overall source properties (polarization, milliarcsecond-scale
radio structure, flat radio spectrum) are described well with the presence of a
compact jet in the system through the transition from the hard-intermediate to
the soft X-ray spectral state. The apparent dependence of source size and the
radio core position on the observed flux density (luminosity dependent core
shift) support this interpretation as well. We see no evidence for major
discrete ejecta during the outburst. For the source proper motion we derive 2
sigma upper limits of 115 microas/day in right ascension, and 37 microas/day in
declination, over a time baseline of 12 days. These correspond to velocities of
1400 km/s and 440 km/s, respectively, assuming a source distance of 7 kpc.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:14:01 GMT""}]","2015-06-15"
"1303.6902","Junhua Zhang","Junhua Zhang, Joerg Schmalian, Tianqi Li, Jigang Wang","Transient Charge and Energy Balance in Graphene Induced by Ultrafast
  Photoexcitation","28 pages, 16 figures. Special section on ultrafast and nonlinear
  optics in carbon nanomaterials","J. Phys.: Condens. Matter 25 (2013) 314201","10.1088/0953-8984/25/31/314201",,"cond-mat.mes-hall cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Ultrafast optical pump-probe spectroscopy measurement on monolayer graphene
observes significant optical nonlinearities. We show that strongly photoexcited
graphene monolayers with 35 fs pulses quasi-instantaneously build up a
broadband, inverted Dirac fermion population. Optical gain emerges and directly
manifests itself via a negative conductivity at the near-infrared region for
the first 200fs, where stimulated emission completely compensates absorption
loss in the graphene layer. To quantitatively investigate this transient,
extremely dense photoexcited Dirac-fermion state, we construct a
two-chemical-potential model, in addition to a time-dependent transient carrier
temperature above lattice temperature, to describe the population inverted
electronic state metastable on the time scale of tens of femtoseconds generated
by a strong exciting pulse. The calculated transient optical conductivity
reveals a complete bleaching of absorption, which sets the saturation density
during the pulse propagation. Particularly, the model calculation reproduces
the negative optical conductivity at lower frequencies in the states close to
saturation, corroborating the observed femtosecond stimulated emission and
optical gain in the wide near-infrared window.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:14:45 GMT""},{""version"":""v2"",""created"":""Tue, 30 Jul 2013 18:55:00 GMT""}]","2015-06-15"
"1303.6903","Yun Li","Yun Li, Giovanni I. Martone, Lev P. Pitaevskii, and Sandro Stringari","Superstripes and the excitation spectrum of a spin-orbit-coupled
  Bose-Einstein condensate","5 pages, 5 figures","Phys. Rev. Lett. 110, 235302 (2013)","10.1103/PhysRevLett.110.235302",,"cond-mat.quant-gas","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using Bogoliubov theory we calculate the excitation spectrum of a spinor
Bose-Einstein condensed gas with equal Rashba and Dresselhaus spin-orbit
coupling in the stripe phase. The emergence of a double gapless band structure
is pointed out as a key signature of Bose-Einstein condensation and of the
spontaneous breaking of translational invariance symmetry. In the long
wavelength limit the lower and upper branches exhibit, respectively, a clear
spin and density nature. For wave vectors close to the first Brillouin zone,
the lower branch acquires an important density character responsible for the
divergent behavior of the structure factor and of the static response function,
reflecting the occurrence of crystalline order. The sound velocities are
calculated as functions of the Raman coupling for excitations propagating
orthogonal and parallel to the stripes. Our predictions provide new
perspectives for the identification of supersolid phenomena in ultracold atomic
gases.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:24:42 GMT""},{""version"":""v2"",""created"":""Fri, 7 Jun 2013 21:49:23 GMT""}]","2013-06-11"
"1303.6904","Delfim F. M. Torres","Helena Sofia Rodrigues, M. Teresa T. Monteiro, Delfim F. M. Torres","Bioeconomic Perspectives to an Optimal Control Dengue Model","This is a preprint of a paper whose final and definitive form will
  appear in International Journal of Computer Mathematics, DOI:
  10.1080/00207160.2013.790536. Paper submitted 09-Aug-2012; revised
  07-Jan-2013; accepted for publication 21-Mar-2013. arXiv admin note:
  substantial text overlap with arXiv:1207.1949","Int. J. Comput. Math. 90 (2013), no. 10, 2126--2136","10.1080/00207160.2013.790536",,"math.OC q-bio.PE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A model with six mutually-exclusive compartments related to dengue is
studied. Three vector control tools are considered: insecticides (larvicide and
adulticide) and mechanical control. The basic reproduction number associated to
the model is presented. The problem is studied using an optimal control
approach. The human data is based on the dengue outbreak that occurred in Cape
Verde. Control measures are simulated in different scenarios and their
consequences analyzed.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:36:02 GMT""}]","2013-11-26"
"1303.6905","Aleksey Tishin","Andrey Bushev, Sergey Vlasenko, Ilya Glotov, Yuri Monakhov, Aleksey
  Tishin","The development of the architecture of Distributed Network Intrusion
  Detection System (D-NIDS)",,,,,"cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents the development of the architecture of Distributed
Network Intrusion Detection System.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:38:47 GMT""}]","2013-03-28"
"1303.6907","Florian Sikora","Cristina Bazgan, Morgan Chopin, Andr\'e Nichterlein, Florian Sikora","Parameterized Approximability of Maximizing the Spread of Influence in
  Networks",,"Journal of Discrete Algorithms (27), 2014, 54--65","10.1016/j.jda.2014.05.001",,"cs.DS cs.SI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we consider the problem of maximizing the spread of influence
through a social network. Given a graph with a threshold value~$thr(v)$
attached to each vertex~$v$, the spread of influence is modeled as follows: A
vertex~$v$ becomes ""active"" (influenced) if at least $thr(v)$ of its neighbors
are active. In the corresponding optimization problem the objective is then to
find a fixed number of vertices to activate such that the number of activated
vertices at the end of the propagation process is maximum. We show that this
problem is strongly inapproximable in fpt-time with respect to (w.r.t.)
parameter $k$ even for very restrictive thresholds. In the case that the
threshold of each vertex equals its degree, we prove that the problem is
inapproximable in polynomial time and it becomes $r(n)$-approximable in
fpt-time w.r.t. parameter $k$ for any strictly increasing function $r$.
  Moreover, we show that the decision version is W[1]-hard w.r.t. parameter $k$
but becomes fixed-parameter tractable on bounded degree graphs.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:41:51 GMT""},{""version"":""v2"",""created"":""Sun, 17 Aug 2014 09:13:01 GMT""}]","2014-08-19"
"1303.6908","Richard Clegg","R. G. Clegg, M.S. Withall, A.W. Moore, I.W. Phillips, D.J. Parish, M.
  Rio, R. Landa, H. Haddadi, K. Kyriakopoulos, J. Auge, R. Clayton, D.Salmon","Challenges in the capture and dissemination of measurements from
  high-speed networks","21 pages, 4 figures","IET Communications, Vol 3, Issue 6, June 2009 pp 957-966","10.1049/iet-com.2008.0068",,"cs.NI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The production of a large-scale monitoring system for a high-speed network
leads to a number of challenges. These challenges are not purely techinical but
also socio-political and legal. The number of stakeholders in a such a
monitoring activity is large including the network operators, the users, the
equipment manufacturers and of course the monitoring researchers. The MASTS
project (Measurement at All Scales in Time and Space) was created to instrument
the high-speed JANET Lightpath network, and has been extended to incorporate
other paths supported by JANET(UK).
  Challenges the project has faced have included: simple access to the network;
legal issues involved in the storage and dissemination of the captured
information, which may be personal; the volume of data captured and the rate at
which this data appears at store. To this end the MASTS system will have
established four monitoring points each capturing packets on a high speed link.
Traffic header data will be continuously collected, anonymised, indexed, stored
and made available to the research community. A legal framework for the capture
and storage of network measurement data has been developed which allows the
anonymised IP traces to be used for research pur poses.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:42:31 GMT""}]","2013-03-28"
"1303.6909","Burkhard Eden","James Drummond, Claude Duhr, Burkhard Eden, Paul Heslop, Jeffrey
  Pennington, Vladimir A. Smirnov","Leading singularities and off-shell conformal integrals","60 pages, LaTeX, 2 figures, references added",,"10.1007/JHEP08(2013)133","HU-Mathematik:2013-06, HU-EP-13/15, IPPP/13/09, DCPT/13/18,
  SLAC-PUB-15409, LAPTH-016/13, CERN-PH-TH/2013-058","hep-th hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The three-loop four-point function of stress-tensor multiplets in N=4 super
Yang-Mills theory contains two so far unknown, off-shell, conformal integrals,
in addition to the known, ladder-type integrals. In this paper we evaluate the
unknown integrals, thus obtaining the three-loop correlation function
analytically. The integrals have the generic structure of rational functions
multiplied by (multiple) polylogarithms. We use the idea of leading
singularities to obtain the rational coefficients, the symbol - with an
appropriate ansatz for its structure - as a means of characterising multiple
polylogarithms, and the technique of asymptotic expansion of Feynman integrals
to obtain the integrals in certain limits. The limiting behaviour uniquely
fixes the symbols of the integrals, which we then lift to find the
corresponding polylogarithmic functions. The final formulae are numerically
confirmed. The techniques we develop can be applied more generally, and we
illustrate this by analytically evaluating one of the integrals contributing to
the same four-point function at four loops. This example shows a connection
between the leading singularities and the entries of the symbol.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:44:32 GMT""},{""version"":""v2"",""created"":""Tue, 4 Jun 2013 14:52:32 GMT""}]","2015-06-15"
"1303.6911","Thomas W. Mattman","Jamison Barsotti and Thomas W. Mattman","Intrinsically knotted graphs with 21 edges","21 pages, 11 figures",,,,"math.GT math.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that the 14 graphs obtained by $\nabla\mathrm{Y}$ moves on K_7
constitute a complete list of the minor minimal intrinsically knotted graphs on
21 edges. We also present evidence in support of a conjecture that the 20 graph
Heawood family, obtained by a combination of $\nabla\mathrm{Y}$ and
$\mathrm{Y}\nabla$ moves on K_7, is the list of graphs of size 21 that are
minor minimal with respect to the property not 2--apex.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:48:59 GMT""}]","2013-03-28"
"1303.6912","Marco Drewes","Marco Drewes","The Phenomenology of Right Handed Neutrinos","Invited review for the International Journal of Modern Physics E.
  This preprint is identical to the published article up to a few rephrasings
  and a number of additional references that appeared after the article had
  been accepted for publication","Int. J. Mod. Phys. E, 22, 1330019 (2013)","10.1142/S0218301313300191","TUM-HEP-881/13","hep-ph astro-ph.CO hep-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Neutrinos are the only particles in the Standard Model of particle physics
that have only been observed with left handed chirality to date. If right
handed neutrinos exist, they could be responsible for several phenomena that
have no explanation within the Standard Model, including neutrino oscillations,
the baryon asymmetry of the universe, dark matter and dark radiation. After a
pedagogical introduction, we review recent progress in the phenomenology of
right handed neutrinos. We in particular discuss the mass ranges suggested by
hints for neutrino oscillation anomalies and dark radiation (eV), sterile
neutrino dark matter scenarios (keV) and experimentally testable theories of
baryogenesis (GeV to TeV). We summarize constraints from theoretical
considerations, laboratory experiments, astrophysics and cosmology for each of
these.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:49:15 GMT""},{""version"":""v2"",""created"":""Wed, 15 May 2013 20:00:10 GMT""},{""version"":""v3"",""created"":""Wed, 18 Sep 2013 20:00:03 GMT""}]","2013-09-20"
"1303.6913","Adam Vellender","A. Vellender, G. Mishuris, A. Piccolroaz","Perturbation analysis for an imperfect interface crack problem using
  weight function techniques","Accepted version",,,,"math-ph math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We analyse a problem of anti-plane shear in a bi-material plane containing a
semi-infinite crack situated on a soft imperfect interface. The plane also
contains a small thin inclusion (for instance an ellipse with high
eccentricity) whose influence on the propagation of the main crack we
investigate. An important element of our approach is the derivation of a new
weight function (a special solution to a homogeneous boundary value problem) in
the imperfect interface setting. The weight function is derived using Fourier
transform and Wiener-Hopf techniques and allows us to obtain an expression for
an important constant (which may be used in a fracture criterion) that
describes the leading order of tractions near the crack tip for the unperturbed
problem. We present computations that demonstrate how this constant varies
depending on the extent of interface imperfection and contrast in material
stiffness. We then perform perturbation analysis to derive an expression for
the change in the leading order of tractions near the tip of the main crack
induced by the presence of the small defect, whose sign can be interpreted as
the inclusion's presence having an amplifying or shielding effect on the
propagation of the main crack.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:50:03 GMT""},{""version"":""v2"",""created"":""Thu, 28 Mar 2013 12:38:32 GMT""},{""version"":""v3"",""created"":""Wed, 24 Apr 2013 12:53:46 GMT""},{""version"":""v4"",""created"":""Tue, 20 Aug 2013 15:05:41 GMT""}]","2013-08-21"
"1303.6914","Giorgio Ottaviani","Luca Chiantini, Massimiliano Mella, Giorgio Ottaviani","One example of general unidentifiable tensors","7 pages, one Macaulay2 script as ancillary file, two references added",,,,"math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The identifiability of parameters in a probabilistic model is a crucial
notion in statistical inference. We prove that a general tensor of rank 8 in
C^3\otimes C^6\otimes C^6 has at least 6 decompositions as sum of simple
tensors, so it is not 8-identifiable. This is the highest known example of
balanced tensors of dimension 3, which are not k-identifiable, when k is
smaller than the generic rank.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:02:00 GMT""},{""version"":""v2"",""created"":""Thu, 28 Mar 2013 07:57:35 GMT""},{""version"":""v3"",""created"":""Thu, 25 Apr 2013 10:54:32 GMT""}]","2013-04-26"
"1303.6915","Giorgio Ottaviani","Cristiano Bocci, Luca Chiantini, Giorgio Ottaviani","Refined methods for the identifiability of tensors","12 pages, three Macaulay2 scripts as ancillary files. v3: final
  version to appear in Annali di Matematica Pura e Applicata",,,,"math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We prove that the general tensor of size 2^n and rank k has a unique
decomposition as the sum of decomposable tensors if k<= 0.9997 (2^n)/(n+1) (the
constant 1 being the optimal value). Similarly, the general tensor of size 3^n
and rank k has a unique decomposition as the sum of decomposable tensors if k<=
0.998 (3^n)/(2n+1) (the constant 1 being the optimal value).
  Some results of this flavor are obtained for tensors of any size, but the
explicit bounds obtained are weaker.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:04:38 GMT""},{""version"":""v2"",""created"":""Thu, 28 Mar 2013 07:59:17 GMT""},{""version"":""v3"",""created"":""Mon, 13 May 2013 12:13:50 GMT""}]","2013-05-14"
"1303.6916","Sebastiano Peotta","Sebastiano Peotta, Massimiliano Di Ventra","Quantum Shock Waves and Population Inversion in Collisions of Ultracold
  Atomic Clouds","11 pages, 6 figures. Enlarged and substantially revised version for
  publication on Physical Review A. Supplementary online material available
  upon request","Phys. Rev. A 89, 013621 (2014)","10.1103/PhysRevA.89.013621",,"cond-mat.quant-gas cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using Time-Dependent Density Matrix Renormalization Group (TDMRG) we study
the collision of one-dimensional atomic clouds confined in a harmonic trap and
evolving with the Lieb-Liniger Hamiltonian. It is observed that the motion is
essentially periodic with the clouds bouncing elastically, at least on the time
scale of the first few oscillations that can be resolved with high accuracy.
This is in agreement with the results of the ""quantum Newton cradle"" experiment
of Kinoshita et al. [Nature 440, 900 (2006)]. We compare the results for the
density profile against a hydrodynamic description, or generalized nonlinear
Schr\""odinger equation, with the pressure term taken from the Bethe Ansatz
solution of the Lieb-Liniger model. We find that hydrodynamics can describe the
breathing mode of a harmonically trapped cloud for arbitrary long times while
it breaks down almost immediately for the collision of two clouds due to the
formation of shock waves (gradient catastrophe). In the case of the clouds'
collision TDMRG alone allows to extract the oscillation period which is found
to be measurably different from the breathing mode period. Concomitantly with
the shock waves formation we observe a local energy distribution typical of
population inversion, i.e., an effective negative temperature. Our results are
an important step towards understanding the hydrodynamics of quantum many-body
systems out of equilibrium and the role of integrability in their dynamics.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:05:53 GMT""},{""version"":""v2"",""created"":""Fri, 3 Jan 2014 23:23:43 GMT""}]","2014-01-27"
"1303.6917","Anton Kapustin","Anton Kapustin","Is there life beyond Quantum Mechanics?","23 pages, latex. v3: commentaries on the axioms expanded, a
  non-technical summary added, references added, typos fixed. v4: version
  accepted for publication in Journal of Mathematical Physic (under a different
  title). Axiomatics is simplified and the number of axioms reduced, some
  proofs clarified, typos fixed",,,,"quant-ph hep-th math-ph math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We formulate physically-motivated axioms for a physical theory which for
systems with a finite number of degrees of freedom uniquely lead to Quantum
Mechanics as the only nontrivial consistent theory. Complex numbers and the
existence of the Planck constant common to all systems arise naturally in this
approach. The axioms are divided into two groups covering kinematics and basic
measurement theory respectively. We show that even if the second group of
axioms is dropped, there are no deformations of Quantum Mechanics which
preserve the kinematic axioms. Thus any theory going beyond Quantum Mechanics
must represent a radical departure from the usual a priori assumptions about
the laws of Nature.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:11:00 GMT""},{""version"":""v2"",""created"":""Thu, 28 Mar 2013 00:53:53 GMT""},{""version"":""v3"",""created"":""Tue, 9 Apr 2013 17:04:33 GMT""},{""version"":""v4"",""created"":""Mon, 3 Jun 2013 17:57:28 GMT""}]","2013-06-04"
"1303.6918","P. S. Bhupal Dev","K.S. Babu, P.S. Bhupal Dev, Elaine C.F.S. Fortes and R.N. Mohapatra","Post-Sphaleron Baryogenesis and an Upper Limit on the
  Neutron-Antineutron Oscillation Time","22 pages, 3 tables, 10 figures; clarification added on the
  baryogenesis calculation; version accepted for publication in Phys. Rev. D",,"10.1103/PhysRevD.87.115019","OSU-HEP-13-02, MAN/HEP/2012/017, UMD-PP-013-003","hep-ph hep-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A recently proposed scenario for baryogenesis, called post--sphaleron
baryogenesis (PSB) is discussed within a class of quark--lepton unified
framework based on the gauge symmetry SU(2)_L x SU(2)_R x SU(4)_c realized in
the multi--TeV scale. The baryon asymmetry of the universe in this model is
produced below the electroweak phase transition temperature after the
sphalerons have decoupled from the Hubble expansion. These models embed
naturally the seesaw mechanism for neutrino masses, and predict color-sextet
scalar particles in the TeV range which may be accessible to the LHC
experiments. A necessary consequence of this scenario is the baryon number
violating \Delta B=2 process of neutron--antineutron (n-\bar{n}) oscillations.
In this paper we show that the constraints of PSB, when combined with the
neutrino oscillation data and restrictions from flavor changing neutral
currents mediated by the colored scalars imply an upper limit on the n-\bar{n}
oscillation time of 5 x 10^{10} sec. regardless of the quark--lepton
unification scale. If this scale is relatively low, in the (200-250) TeV range,
\tau_{n-\bar{n}} is predicted to be less than 10^{10} sec., which is accessible
to the next generation of proposed experiments.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:12:41 GMT""},{""version"":""v2"",""created"":""Thu, 6 Jun 2013 14:56:17 GMT""}]","2013-06-26"
"1303.6919","Yao Tang","Yao Tang and Mai Vu","A Partial Decode-Forward Scheme For A Network with N relays","Presented in 47th Annual Conference on Information Sciences and
  Systems (CISS) 2013",,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study a discrete-memoryless relay network consisting of one source, one
destination and N relays, and design a scheme based on partial decode-forward
relaying. The source splits its message into one common and N+1 private parts,
one intended for each relay. It encodes these message parts using Nth-order
block Markov coding, in which each private message part is independently
superimposed on the common parts of the current and N previous blocks. Using
simultaneous sliding window decoding, each relay fully recovers the common
message and its intended private message with the same block index, then
forwards them to the following nodes in the next block. This scheme can be
applied to any network topology. We derive its achievable rate in a compact
form. The result reduces to a known decode-forward lower bound for an N-relay
network and partial decode-forward lower bound for a two-level relay network.
We then apply the scheme to a Gaussian two-level relay network and obtain its
capacity lower bound considering power constraints at the transmitting nodes.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:17:31 GMT""}]","2013-03-28"
"1303.6920","Piotr Garbaczewski","Mariusz Zaba, Piotr Garbaczewski and Vladimir Stephanovich","Trajectory statistics of confined L\'{e}vy flights and Boltzmann-type
  equilibria","Presented at 25th Marian Smoluchowski Symposium on Statistical
  Physics, Cracow, Sept. 10-13, 2012","Acta Phys. Pol. B 44, (2013), 1109-1122","10.5506/APhysPolB.44.1109",,"cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We analyze a specific class of random systems that are driven by a symmetric
L\'{e}vy stable noise, where Langevin representation is absent. In view of the
L\'{e}vy noise sensitivity to environmental inhomogeneities, the pertinent
random motion asymptotically sets down at the Boltzmann-type equilibrium,
represented by a probability density function (pdf) $\rho_*(x) \sim \exp [-\Phi
(x)]$. Here, we infer pdf $\rho (x,t)$ based on numerical path-wise simulation
of the underlying jump-type process. A priori given data are jump transition
rates entering the master equation for $\rho (x,t)$ and its target pdf
$\rho_*(x)$. To simulate the above processes, we construct a suitable
modification of the Gillespie algorithm, originally invented in the chemical
kinetics context. We exemplified our algorithm simulating different jump-type
processes and discuss the dynamics of real physical systems where it can be
useful.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:24:57 GMT""}]","2015-06-15"
"1303.6921","Satyabrata Patnaik","G. Sharma, J. Saha and S. Patnaik","Magnetism driven ferroelectricity above liquid nitrogen temperature in
  Y2CoMnO6",,,"10.1063/1.4812728",,"cond-mat.mtrl-sci cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We report multiferroic behavior in double perovskite Y2CoMnO6 with
ferroelectric transition temperature Tc = 80K. The origin of ferroelectricity
is associated with magnetic ordering of Co2+ and Mn4+ moments in a
up-up-down-down arrangement. The saturation polarization and magnetization are
estimated to be 65 uC/m2 and 6.2 Bohr magneton/f.u. respectively. The
magnetoelectric coupling parameter, on the other hand, is small as a 5 Tesla
field suppresses the electric polarization by only ~8%. This is corroborated
with observed hysteretic behaviour at 5K that remains unsaturated even upto 7
Tesla. A model based on exchange-striction is proposed to explain the observed
high temperature ferroelectricity.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:29:49 GMT""}]","2015-06-15"
"1303.6922","Cheng Yu","Dehua Wang and Cheng Yu","Global weak solutions to the inhomogeneous Navier-Stokes-Vlasov
  equations",,,,,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A fluid-particle system of the inhomogeneous Navier-Stokes equations and
Vlasov equation in the three dimensional space is considered in this paper. The
coupling arises from the drag force in the fluid equations and the acceleration
in the Vlasov equation. An initial-boundary value problem is studied in a
bounded domain with large data. The existence of global weak solutions is
established through an approximation scheme, energy estimates, and weak
convergence.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:32:20 GMT""},{""version"":""v2"",""created"":""Wed, 17 Apr 2013 03:41:05 GMT""}]","2013-04-18"
"1303.6923","Fabien Casenave","Fabien Casenave, Alexandre Ern and Guillaume Sylvand","Coupled BEM-FEM for the convected Helmholtz equation with non-uniform
  flow in a bounded domain","23 pages, 9 figures","J. Comput. Phys. 257 (2014), 627-644","10.1016/j.jcp.2013.10.016",,"math.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider the convected Helmholtz equation modeling linear acoustic
propagation at a fixed frequency in a subsonic flow around a scattering object.
The flow is supposed to be uniform in the exterior domain far from the object,
and potential in the interior domain close to the object. Our key idea is the
reformulation of the original problem using the Prandtl--Glauert transformation
on the whole flow domain, yielding (i) the classical Helmholtz equation in the
exterior domain and (ii) an anisotropic diffusive PDE with skew-symmetric
first-order perturbation in the interior domain such that its transmission
condition at the coupling boundary naturally fits the Neumann condition from
the classical Helmholtz equation. Then, efficient off-the-shelf tools can be
used to perform the BEM-FEM coupling, leading to two novel variational
formulations for the convected Helmholtz equation. The first formulation
involves one surface unknown and can be affected by resonant frequencies, while
the second formulation avoids resonant frequencies and involves two surface
unknowns. Numerical simulations are presented to compare the two formulations.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:46:43 GMT""},{""version"":""v2"",""created"":""Mon, 7 Oct 2013 13:27:52 GMT""},{""version"":""v3"",""created"":""Wed, 30 Oct 2013 16:28:24 GMT""}]","2014-05-16"
"1303.6924","Elena Zaninoni","Elena Zaninoni, Maria Grazia Bernardini, Raffaella Margutti, Samantha
  Oates, and Guido Chincarini","Gamma-ray burst optical light-curve zoo: comparison with X-ray
  observations","55 pages, 37 figures, accepted for publication in A&A (this version
  includes changes made at Proofs stage)",,"10.1051/0004-6361/201321221",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a comprehensive analysis of the optical and X-ray light curves
(LCs) and spectral energy distributions (SEDs) of a large sample of gamma-ray
burst (GRB) afterglows to investigate the relationship between the optical and
X-ray emission after the prompt phase. We collected the optical data from the
literature and determined the shapes of the optical LCs. Then, using previously
presented X-ray data we modeled the optical/X-ray SEDs. We studied the SED
parameter distributions and compared the optical and X-ray LC slopes and
shapes. The optical and X-ray spectra become softer as a function of time while
the gas-to-dust ratios of GRBs are higher than the values calculated for the
Milky Way and the Large and Magellanic Clouds. For 20% of the GRBs the
difference between the optical and X-ray slopes is consistent with 0 or 1=4
within the uncertainties (we did it not consider the steep decay phase), while
in the remaining 80% the optical and X-ray afterglows show significantly
different temporal behaviors. Interestingly, we find an indication that the
onset of the forward shock in the optical LCs (initial peaks or shallow phases)
could be linked to the presence of the X-ray flares. Indeed, when X-ray flares
are present during the steep decay, the optical LC initial peak or end plateau
occurs during the steep decay; if instead the X-ray flares are absent or occur
during the plateau, the optical initial peak or end plateau takes place during
the X-ray plateau. The forward-shock model cannot explain all features of the
optical (e.g. bumps, late re-brightenings) and X-ray (e.g. flares, plateaus)
LCs. However, the synchrotron model is a viable mechanism for GRBs at late
times. In particular, we found a relationship between the presence of the X-ray
flares and the shape of the optical LC that indicates a link between the prompt
emission and the optical afterglow.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:46:52 GMT""},{""version"":""v2"",""created"":""Thu, 9 May 2013 20:25:28 GMT""}]","2015-06-15"
"1303.6925","R\'emi Lassalle Phd","R\'emi Lassalle","Causal transference plans and their Monge-Kantorovich problems","This is a preprint",,,,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper investigates causal optimal transportation problems, in the
framework of two Polish spaces, both endowed with filtrations. Specific
concretizations yield primal problems equivalent to several classical problems
of stochastic control, and of stochastic calculus ; trivial filtrations yield
usual problems of optimal transport. Within this framework, primal attainments
and dual formulations are obtained, under standard hypothesis, for the related
variational problems. These problems are intrinsically related to martingales.
Finally, we investigate applications to stochastic frameworks. A
straightforward equivalence between specific causal optimization problems, and
problems of stochastic control, is obtained. Solutions to a class of stochastic
differential equations are characterized, as optimum to specific causal
Monge-Kantorovich problems ; the existence of a unique strong solution is
related to corresponding Monge problems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:55:01 GMT""},{""version"":""v2"",""created"":""Fri, 2 Oct 2015 14:15:20 GMT""}]","2015-10-05"
"1303.6926","Arun P V Arun P V","Dr. S.K. Katiyar, Arun P. V.","A Comparative Analysis on the Applicability of Entropy in remote sensing",,,,,"cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Entropy is the measure of uncertainty in any data and is adopted for
maximisation of mutual information in many remote sensing operations. The
availability of wide entropy variations motivated us for an investigation over
the suitability preference of these versions to specific operations.
Methodologies were implemented in Matlab and were enhanced with entropy
variations. Evaluation of various implementations was based on different
statistical parameters with reference to the study area The popular available
versions like Tsalli's, Shanon's, and Renyi's entropies were analysed in
context of various remote sensing operations namely thresholding, clustering
and registration.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 18:57:12 GMT""}]","2014-05-25"
"1303.6927","Arun P V Arun P V","Arun P. V., Dr. S.K. Katiyar","An investigation towards wavelet based optimization of automatic image
  registration techniques",,,,,"cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Image registration is the process of transforming different sets of data into
one coordinate system and is required for various remote sensing applications
like change detection, image fusion, and other related areas. The effect of
increased relief displacement, requirement of more control points, and
increased data volume are the challenges associated with the registration of
high resolution image data. The objective of this research work is to study the
most efficient techniques and to investigate the extent of improvement
achievable by enhancing them with Wavelet transform. The SIFT feature based
method uses the Eigen value for extracting thousands of key points based on
scale invariant features and these feature points when further enhanced by the
wavelet transform yields the best results.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:02:02 GMT""}]","2013-03-28"
"1303.6928","Blazenka Melic","Ernest Ma, Blazenka Melic","Updated S3 Model of Quarks","14 pages, 2 figures",,"10.1016/j.physletb.2013.07.015","UCRHEP-T526","hep-ph hep-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A model proposed in 2004 using the non-Abelian discrete symmetry S3 for
understanding the flavor structure of quarks and leptons is updated, with
special focus on the quark and scalar sectors. We show how the approximate
residual symmetries of this model explain both the pattern of the quark mixing
matrix and why the recently observed particle of 126 GeV at the Large Hadron
Collider is so much like the one Higgs boson of the Standard Model. We identify
the strongest phenomenological bounds on the scalar masses of this model, and
predict a possibly observable decay b -> s tau- mu+, but not b -> s tau+ mu-.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:07:37 GMT""}]","2015-06-15"
"1303.6929","Danning Li","Danning Li and Mei Huang","Dynamical holographic QCD model for glueball and light meson spectra","49 pages, 23 figures, version accepted by JHEP","JHEP11(2013)088","10.1007/JHEP11(2013)088",,"hep-ph hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this work, we offer a dynamical soft-wall model to describe the
gluodynamics and chiral dynamics in one systematical framework. We firstly
construct a quenched dynamical holographic QCD (hQCD) model in the
graviton-dilaton framework for the pure gluon system, then develop a dynamical
hQCD model for the two flavor system in the graviton-dilaton-scalar framework
by adding light flavors on the gluodynamical background. For two forms of
dilaton background field $\Phi=\mu_G^2z^2$ and
$\Phi=\mu_G^2z^2\tanh(\mu_{G^2}^4z^2/\mu_G^2)$, the quadratic correction to
dilaton background field at infrared encodes important non-perturbative
gluodynamics and naturally induces a deformed warp factor of the metric. By
self-consistently solving the deformed metric induced by the dilaton background
field, we find that the scalar glueball spectra in the quenched dynamical model
is in very well agreement with lattice data. For two flavor system in the
graviton-dilaton-scalar framework, the deformed metric is self-consistently
solved by considering both the chiral condensate and nonperturbative
gluodynamics in the vacuum, which are responsible for the chiral symmetry
breaking and linear confinement, respectively. It is found that the mixing
between the chiral condensate and gluon condensate is important to produce the
correct light flavor meson spectra. The pion form factor and the vector
couplings are also investigated in the dynamical hQCD model. Besides, we give
the criteria for the existence of linear quark potential from the metric
structure, and show a negative quadratic dilaton background field is not
favored in the graviton-dilaton framework.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:08:22 GMT""},{""version"":""v2"",""created"":""Wed, 30 Oct 2013 01:41:57 GMT""}]","2013-11-15"
"1303.6930","Edward Crane","Edward Crane","Intrinsic circle domains","This version differs from version 1 only in the addition of a line to
  say that the paper has been accepted for publication in Conformal Geometry
  and Dynamics",,,,"math.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using quasiconformal mappings, we prove that any Riemann surface of finite
connectivity and finite genus is conformally equivalent to an intrinsic circle
domain U in a compact Riemann surface S. This means that each connected
component B of S \ U is either a point or a closed geometric disc with respect
to the complete constant curvature conformal metric of the Riemann surface (U
union B). Moreover the pair (U,S) is unique up to conformal isomorphisms. We
give a generalization to countably infinite connectivity. Finally we show how
one can compute numerical approximations to intrinsic circle domains using
circle packings and conformal welding.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:17:03 GMT""},{""version"":""v2"",""created"":""Mon, 4 Nov 2013 15:52:42 GMT""}]","2013-11-05"
"1303.6931","Berthold Stech","Berthold Stech","Degenerate states in the scalar boson spectrum. Is the Higgs Boson a
  Twin ?","6 pages",,,,"hep-ph hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The extension of the standard model to $SU(3)_L\times SU(3)_R \times SU(3)_C$
is considered. Spontaneous symmetry breaking requires two $(3^*, 3, 1)$ Higgs
field multiplets with a strong hierarchical structure of their vacuum
expectation values. An invariant potential is constructed to provide for these
vacuum expectation values. This potential gives masses to all scalar fields
apart from the 15 Goldstone bosons. In case there exists a one-to-one
correspondence between the vacuum expectation values of the two field
multiplets, the scalar boson spectrum contains degenerate eigenstates. The
lowest eigenstate has a mass near 123 GeV close to the Higgs-like particle
discovered at the LHC. In one class of solutions this lowest state is a nearly
degenerate twin state. Each member is a superposition of fields from both
multiplets with about equal strength. The twins are non identical twins, namely
different combinations of a conventional Higgs and a Higgs field which is not
coupled to fermions, only to gauge bosons. A second class of solutions leads
again to degenerate states but in this case the state near 123 GeV remains a
single state even for identical low scale vacuum expectation values in both
multiplets.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:17:33 GMT""}]","2013-03-28"
"1303.6933","Alan Huckleberry","Alan Huckleberry","Hans Grauert (1930-2011)","Written for publication in 2013 in the Jahresber. Deutsch.
  Math.-Vereinigung",,,,"math.HO math.AG math.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Hans Grauert died in September of 2011. This article reviews his life in
mathematics and recalls some detail his major accomplishments.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:23:57 GMT""}]","2013-03-28"
"1303.6934","Marta D'Elia","Marta D'Elia, Max Gunzburger","The fractional Laplacian operator on bounded domains as a special case
  of the nonlocal diffusion operator","27 pages, 5 figures",,,,"math.AP math.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We analyze a nonlocal diffusion operator having as special cases the
fractional Laplacian and fractional differential operators that arise in
several applications. In our analysis, a nonlocal vector calculus is exploited
to define a weak formulation of the nonlocal problem. We demonstrate that, when
sufficient conditions on certain kernel functions hold, the solution of the
nonlocal equation converges to the solution of the fractional Laplacian
equation on bounded domains as the nonlocal interactions become infinite. We
also introduce a continuous Galerkin finite element discretization of the
nonlocal weak formulation and we derive a priori error estimates. Through
several numerical examples we illustrate the theoretical results and we show
that by solving the nonlocal problem it is possible to obtain accurate
approximations of the solutions of fractional differential equations
circumventing the problem of treating infinite-volume constraints.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:31:58 GMT""}]","2013-03-28"
"1303.6935","Xiaocheng  Tang","Xiaocheng Tang and Katya Scheinberg","Efficiently Using Second Order Information in Large l1 Regularization
  Problems",,,,,"stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose a novel general algorithm LHAC that efficiently uses second-order
information to train a class of large-scale l1-regularized problems. Our method
executes cheap iterations while achieving fast local convergence rate by
exploiting the special structure of a low-rank matrix, constructed via
quasi-Newton approximation of the Hessian of the smooth loss function. A greedy
active-set strategy, based on the largest violations in the dual constraints,
is employed to maintain a working set that iteratively estimates the complement
of the optimal active set. This allows for smaller size of subproblems and
eventually identifies the optimal active set. Empirical comparisons confirm
that LHAC is highly competitive with several recently proposed state-of-the-art
specialized solvers for sparse logistic regression and sparse inverse
covariance matrix selection.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:34:05 GMT""}]","2013-03-28"
"1303.6936","Rui Mao","R. Mao, B. D. Kong, C. Gong, S. Xu, T. Jayasekera, K. Cho, K. W. Kim","First-Principles Calculation of Thermal Transport in the Metal/Graphene
  System",,,"10.1103/PhysRevB.87.165410",,"cond-mat.mes-hall cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Thermal properties in the metal/graphene (Gr) systems are analyzed by using
an atomistic phonon transport model based on Landauer formalism and
first-principles calculations. The specific structures under investigation
include chemisorbed Ni(111)/Gr, physisorbed Cu(111)/Gr and Au(111)/Gr, as well
as Pd(111)/Gr with intermediate characteristics. Calculated results illustrate
a strong dependence of thermal transfer on the details of interfacial
microstructures. In particular, it is shown that the chemisorbed case provides
a generally smaller interfacial thermal resistance than the physisorbed due to
the stronger bonding. However, our calculation also indicates that the weakly
chemisorbed interface of Pd/Gr may be an exception, with the largest thermal
resistance among the considered. Further examination of the electrostatic
potential and interatomic force constants reveal that the mixed bonding force
between the Pd and C atoms results in incomplete hybridization of Pd and
graphene orbital states at the junction, leading effectively to two phonon
interfaces and a larger than expected thermal resistance. Comparison with
available experimental data shows good agreement. The result clearly suggests
the feasibility of phonon engineering for thermal property optimization at the
interface.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:34:58 GMT""}]","2015-06-15"
"1303.6937","Botong Wang","Botong Wang","Cohomology jump loci of compact K\""ahler manifolds",,,,,"math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We apply the method of Dimca-Papadima to study the cohomology jump loci in
the representation variety and the moduli space of vector bundles with
vanishing chern classes for a compact K\""ahler manifold. We introduce modules
over differential graded Lie algebra to extend results of Dimca-Papadima to a
general point in the representation variety or the moduli space. We show that
locally the cohomology jump loci is isomorphic to the resonance variety via the
exponential map. This paper generalizes a previous result of the author.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:10 GMT""}]","2013-03-28"
"1303.6938","Pasi Jyl\""anki","Pasi Jyl\""anki, Aapo Nummenmaa and Aki Vehtari","Expectation Propagation for Neural Networks with Sparsity-promoting
  Priors",,"Journal of Machine Learning Research, 15(May): 1849-1901, 2014",,,"stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose a novel approach for nonlinear regression using a two-layer neural
network (NN) model structure with sparsity-favoring hierarchical priors on the
network weights. We present an expectation propagation (EP) approach for
approximate integration over the posterior distribution of the weights, the
hierarchical scale parameters of the priors, and the residual scale. Using a
factorized posterior approximation we derive a computationally efficient
algorithm, whose complexity scales similarly to an ensemble of independent
sparse linear models. The approach enables flexible definition of weight priors
with different sparseness properties such as independent Laplace priors with a
common scale parameter or Gaussian automatic relevance determination (ARD)
priors with different relevance parameters for all inputs. The approach can be
extended beyond standard activation functions and NN model structures to form
flexible nonlinear predictors from multiple sparse linear models. The effects
of the hierarchical priors and the predictive performance of the algorithm are
assessed using both simulated and real-world data. Comparisons are made to two
alternative models with ARD priors: a Gaussian process with a NN covariance
function and marginal maximum a posteriori estimates of the relevance
parameters, and a NN with Markov chain Monte Carlo integration over all the
unknown model parameters.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:26 GMT""}]","2015-01-23"
"1303.6939","Marco Stefano Bianchi","Marco S. Bianchi, Gaston Giribet, Matias Leoni and Silvia Penati","The 1/2 BPS Wilson loop in ABJM theory at two loops","5 pages",,"10.1103/PhysRevD.88.026009",,"hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We compute the expectation value of the 1/2 BPS circular Wilson loop in ABJM
theory at two loops in perturbation theory. The result shows perfect agreement
with the prediction from localization and the proposed framing factor.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:41 GMT""}]","2013-08-09"
"1303.6940","Angnis Schmidt-May","S. F. Hassan, Angnis Schmidt-May, Mikael von Strauss","Higher Derivative Gravity and Conformal Gravity From Bimetric and
  Partially Massless Bimetric Theory","Latex, 34 pages; minor comments and note added, matches published
  version","Universe 2015, 1(2), 92-122","10.3390/universe1020092",,"hep-th gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we establish the correspondence between ghost-free bimetric
theory and a class of higher derivative gravity actions, including conformal
gravity and New Massive Gravity. We also characterize the relation between the
respective equations of motion and classical solutions. We illustrate that, in
this framework, the spin-2 ghost of higher derivative gravity is an artifact of
the truncation to a 4-derivative theory. The analysis also gives a relation
between the proposed partially massless (PM) bimetric theory and conformal
gravity, showing, in particular, the equivalence of their equations of motion
at the 4-derivative level. For the PM bimetric theory this provides further
evidence for the existence of an extra gauge symmetry and the associated loss
of a propagating mode away from de Sitter backgrounds. The new symmetry is an
extension of Weyl symmetry which also suggests the PM bimetric theory as a
ghost-free completion of conformal gravity.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:51 GMT""},{""version"":""v2"",""created"":""Thu, 23 Jul 2015 12:24:53 GMT""}]","2015-07-24"
"1303.6941","Jim Hague","J.P. Hague","Enhancement of gaps in thin graphitic films for heterostructure
  formation","To appear in Phys. Rev. B","Phys. Rev. B 89, 155415 (2014)","10.1103/PhysRevB.89.155415",,"cond-mat.mes-hall cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There are a large number of atomically thin graphitic films with similar
structure to graphene. These films have a spread of bandgaps relating to their
ionicity, and also to the substrate on which they are grown. Such films could
have a range of applications in digital electronics where graphene is difficult
to use. I use the dynamical cluster approximation to show how electron-phonon
coupling between film and substrate can enhance these gaps in a way that
depends on the range and strength of the coupling. One of the driving factors
in this effect is the proximity to a charge density wave instability for
electrons on a honeycomb lattice. The enhancement at intermediate coupling is
sufficiently large that spatially varying substrates and superstrates could be
used to create heterostructures in thin graphitic films with position dependent
electron-phonon coupling and gaps, leading to advanced electronic components.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:05 GMT""},{""version"":""v2"",""created"":""Fri, 21 Jun 2013 13:48:10 GMT""},{""version"":""v3"",""created"":""Thu, 27 Mar 2014 10:42:05 GMT""}]","2014-04-28"
"1303.6942","Benjamin Hunt","B. Hunt, J. D. Sanchez-Yamagishi, A. F. Young, K. Watanabe, T.
  Taniguchi, P. Moon, M. Koshino, P. Jarillo-Herrero and R. C. Ashoori","Massive Dirac fermions and Hofstadter butterfly in a van der Waals
  heterostructure","6+11 pages, 4 figures main text, 15 figures supplementary text","Science Online, May 16 2013","10.1126/science.1237240",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Van der Waals heterostructures comprise a new class of artificial materials
formed by stacking atomically-thin planar crystals. Here, we demonstrate band
structure engineering of a van der Waals heterostructure composed of a
monolayer graphene flake coupled to a rotationally-aligned hexagonal boron
nitride substrate. The spatially-varying interlayer atomic registry results
both in a local breaking of the carbon sublattice symmetry and a long-range
moir\'e superlattice potential in the graphene. This interplay between short-
and long-wavelength effects results in a band structure described by isolated
superlattice minibands and an unexpectedly large band gap at charge neutrality,
both of which can be tuned by varying the interlayer alignment.
Magnetocapacitance measurements reveal previously unobserved fractional quantum
Hall states reflecting the massive Dirac dispersion that results from broken
sublattice symmetry. At ultra-high fields, integer conductance plateaus are
observed at non-integer filling factors due to the emergence of the Hofstadter
butterfly in a symmetry-broken Landau level.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:42 GMT""}]","2013-05-22"
"1303.6943","Wenqing Hu","Mark Freidlin, Wenqing Hu","Wave front propagation for a reaction-diffusion equation in narrow
  random channels","31 pages, 1 figure, revised version. arXiv admin note: text overlap
  with arXiv:1210.5226","Nonlinearity, 26, 8, 2013, pp. 2333--2356","10.1088/0951-7715/26/8/2333",,"math.PR math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider a reaction-diffusion equation in narrow random channels. We
approximate the generalized solution to this equation by the corresponding one
on a random graph. By making use of large deviation analysis we study the
asymptotic wave front propagation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:26 GMT""},{""version"":""v2"",""created"":""Thu, 20 Jun 2013 16:33:58 GMT""}]","2013-07-15"
"1303.6944","Pedro Miana","Valentin Keyantuo, Pedro J. Miana and Luis S\'anchez-Lajusticia","Sharp extensions for convoluted solutions of abstract Cauchy problems","24 pages",,,,"math.FA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we give sharp extension results for convoluted solutions of
abstract Cauchy problems in Banach spaces. The main technique is the use of
algebraic structure (for usual convolution product $\ast$) of these solutions
which are defined by a version of the Duhamel formula. We define algebra
homomorphisms from a new class of test-functions and apply our results to
concrete operators. Finally, we introduce the notion of $k$-distribution
semigroups to extend previous concepts of distribution semigroups.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:27 GMT""}]","2013-03-28"
"1303.6946","Oktay Mukhtarov","O. Sh. Mukhtarov and K. Aydemir","Asymptotic properties of boundary-value problem with transmission
  conditions",,,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this study by applying an own technique we investigate some asymptotic
approximation properties of new type discontinuous boundary-value problems,
which consists of a Sturm-Liouville equation together with
eigenparameter-dependent boundary and transmission conditions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 16:54:21 GMT""}]","2013-03-29"
"1303.6947","Oktay Mukhtarov","K. Aydemir and O. Sh. Mukhtarov","Boundary value problem with transmission conditions","arXiv admin note: substantial text overlap with arXiv:1303.6898",,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  One important innovation here is that for the Sturm-Liouville considered
equation together with eigenparameter dependent boundary conditions and two
supplementary transmission conditions at one interior point. We develop Green's
function method for spectral analysis of the considered problem in modified
Hilbert space.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:06:58 GMT""}]","2013-03-29"
"1303.6948","Oktay Mukhtarov","O. Sh. Mukhtarov and K. Aydemir","Asymptotic formulas for eigenvalues and eigenfunctions of boundary value
  problem","arXiv admin note: substantial text overlap with arXiv:1303.6888",,,,"math.CA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we are concerned with a new class of BVP' s consisting of
eigendependent boundary conditions and two supplementary transmission
conditions at one interior point. By modifying some techniques of classical
Sturm-Liouville theory and suggesting own approaches we find asymptotic
formulas for the eigenvalues and eigenfunction.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 17:11:28 GMT""}]","2013-03-29"
"1303.6949","Donghui Jeong","Liang Dai, Donghui Jeong, Marc Kamionkowski and Jens Chluba","The Pesky Power Asymmetry","5 pages, 4 figures",,"10.1103/PhysRevD.87.123005",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Physical models for the hemispherical power asymmetry in the cosmic microwave
background (CMB) reported by the Planck Collaboration must satisfy CMB
constraints to the homogeneity of the Universe and quasar constraints to power
asymmetries. We survey a variety of models for the power asymmetry and show
that consistent models include a modulated scale-dependent isocurvature
contribution to the matter power spectrum or a modulation of the reionization
optical depth, gravitational-wave amplitude, or scalar spectral index. We
propose further tests to distinguish between the different scenarios.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:01 GMT""}]","2013-06-26"
"1303.6950","Garrett Hickman","G. T. Hickman, Xin Wang, J. P. Kestner, and S. Das Sarma","Dynamically corrected gates for an exchange-only qubit","5 pages, 3 figures, + 5 pages supplemental information","Phys. Rev. B 88, 161303(R) (2013)","10.1103/PhysRevB.88.161303",,"cond-mat.mes-hall quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We provide analytical composite pulse sequences that perform dynamical
decoupling concurrently with arbitrary rotations for a qubit coded in the spin
state of a triple quantum dot. The sequences are designed to respect realistic
experimental constraints such as strictly nonnegative couplings. Logical errors
and leakage errors are simultaneously corrected. A short pulse sequence is
presented to compensate nuclear noise and a longer sequence is presented to
simultaneously compensate nuclear and charge noise. The capability developed in
this work provides a clear prescription for combatting the relevant sources of
noise that currently hinder exchange-only qubit experiments.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:01 GMT""},{""version"":""v2"",""created"":""Wed, 23 Oct 2013 15:55:48 GMT""}]","2013-10-24"
"1303.6951","Tullia Sbarrato","T. Sbarrato, G. Ghisellini, M. Nardini, G. Tagliaferri, J. Greiner, A.
  Rau, P. Schady","Blazar candidates beyond redshift 4 observed with GROND","12 pages, 10 figures, 4 tables. Accepted for publication in MNRAS",,"10.1093/mnras/stt882",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The search for extremely massive high redshift blazars is essential to put
strong constraints on the supermassive black holes formation theories. Up to
now, the few blazars known to have a redshift larger than 4 have been
discovered serendipitously. We try a more systematic approach. Assuming
radio-loudness as a proxy for the jet orientation, we select a sample of
extremely radio-loud quasars. We measure their black hole masses with a method
based on fitting the thermal emission from the accretion disc. We achieve a
precision of a factor of two for our measures, thanks to the observations
performed with the Gamma-Ray Burst Optical Near-Infrared Detector (GROND). The
infrared to optical GROND data allow us to observe directly the peak of the
disc emission, thus constraining the overall disc luminosity. We obtain a small
range of masses, that peaks at 10^{9.3}Msun. If some of our candidates will be
confirmed as blazars, these results would introduce interesting constraints on
the mass function of extremely massive black holes at very high redshift.
Moreover, all our blazar candidates have high accretion rates. This result,
along with the high masses, opens an interesting view on the need of a fast
growth of the heaviest black holes at very high redshift.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:01 GMT""},{""version"":""v2"",""created"":""Thu, 16 May 2013 10:34:42 GMT""}]","2015-06-15"
"1303.6952","Daekyoung Kang","Daekyoung Kang, Christopher Lee, and Iain W. Stewart","Using 1-Jettiness to Measure 2 Jets in DIS 3 Ways","53 pages, 13 figures, 3 tables. Version published in Physical Review
  D","Phys.Rev.D88:054004,2013","10.1103/PhysRevD.88.054004","MIT-CTP 4375, LA-UR-13-20960","hep-ph hep-ex nucl-ex nucl-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We predict cross sections in deep inelastic scattering (DIS) for the
production of two jets---one along the proton beam direction created by initial
state radiation (ISR) and another created by final state radiation after the
hard collision. Our results include fixed order corrections and a summation of
large logarithms up to next-to-next-to-leading logarithmic (NNLL) accuracy in
resummed perturbation theory. We make predictions for three versions of a DIS
event shape 1-jettiness, each of which constrains hadronic final states to be
well collimated into two jets along the beam and final-state jet directions,
but which differ in their sensitivity to the transverse momentum of the ISR
from the proton beam. We use the tools of soft collinear effective theory
(SCET) to derive factorization theorems for these three versions of
1-jettiness. The sensitivity to the ISR gives rise to significantly different
structures in the corresponding factorization theorems---for example,
dependence on either the ordinary or the generalized kperp-dependent beam
function. Despite the differences among 1-jettiness definitions, we show that
the leading nonperturbative correction that shifts the tail region of their
distributions is given by a single universal nonperturbative parameter Omega1,
even accounting for hadron mass effects. Finally, we give numerical results for
Q^2 and x values explored at the HERA collider, emphasizing that the target of
our factorization-based analyses is to open the door for higher-precision jet
phenomenology in DIS.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:02 GMT""},{""version"":""v2"",""created"":""Wed, 11 Sep 2013 18:09:44 GMT""}]","2013-09-12"
"1303.6953","Vincenzo Cirigliano","Vincenzo Cirigliano, Susan Gardner, Barry Holstein","Beta Decays and Non-Standard Interactions in the LHC Era","To appear in Prog. Part. Nucl. Phys",,"10.1016/j.ppnp.2013.03.005",,"hep-ph nucl-ex nucl-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider the role of precision measurements of beta decays and light meson
semi-leptonic decays in probing physics beyond the Standard Model in the LHC
era. We describe all low-energy charged-current processes within and beyond the
Standard Model using an effective field theory framework. We first discuss the
theoretical hadronic input which in these precision tests plays a crucial role
in setting the baseline for new physics searches. We then review the current
and upcoming constraints on the various non-standard operators from the study
of decay rates, spectra, and correlations in a broad array of light-quark
systems. We finally discuss the interplay with LHC searches, both within models
and in an effective theory approach. Our discussion illustrates the independent
yet complementary nature of precision beta decay measurements as probes of new
physics, showing them to be of continuing importance throughout the LHC era.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:02 GMT""}]","2015-06-15"
"1303.6954","Adrian Del Maestro","B. Kulchytskyy, G. Gervais, and A. Del Maestro","Local Superfluidity at the Nanoscale","Added a figure and extended discussion","Physical Review B 88, 064512 (2013)","10.1103/PhysRevB.88.064512",,"cond-mat.mes-hall cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We have performed quantum Monte Carlo simulations measuring the finite size
and temperature superfluid response of helium-4 to the linear and rotational
motion of the walls of a nanopore. Within the two-fluid model, the portion of
the normal liquid dragged along with the boundaries is dependent on the type of
motion and the resulting anisotropic superfluid density saturates far below
unity at T=0.5 K. The origin of the saturation is uncovered by computing the
spatial distribution of superfluidity, with only the core of the nanopore
exhibiting any evidence of phase coherence. The superfluid core displays
scaling behavior consistent with Luttinger liquid theory, thereby providing an
experimental test for the emergence of a one dimensional quantum liquid.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:04 GMT""},{""version"":""v2"",""created"":""Tue, 3 Sep 2013 01:06:36 GMT""}]","2013-09-04"
"1303.6955","Thomas Hartman","Thomas Hartman","Entanglement Entropy at Large Central Charge","28 pages",,,,"hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Two-dimensional conformal field theories with a large central charge and a
small number of low-dimension operators are studied using the conformal block
expansion. A universal formula is derived for the Renyi entropies of N disjoint
intervals in the ground state, valid to all orders in a series expansion. This
is possible because the full perturbative answer in this regime comes from the
exchange of the stress tensor and other descendants of the vacuum state.
Therefore, the Renyi entropy is related to the Virasoro vacuum block at large
central charge. The entanglement entropy, computed from the Renyi entropy by an
analytic continuation, decouples into a sum of single-interval entanglements.
This field theory result agrees with the Ryu-Takayanagi formula for the
holographic entanglement entropy of a 2d CFT, applied to any number of
intervals, and thus can be interpreted as a microscopic calculation of the area
of minimal surfaces in 3d gravity.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:05 GMT""}]","2013-03-29"
"1303.6956","Andrew Potter","Andrew C. Potter and Patrick A. Lee","Edge-Ferromagnetism from Majorana Flat-Bands: Application to Split
  Tunneling-Conductance Peaks in the High-Tc Cuprates","10 pages, 5 figures","Phys. Rev. Lett. 112, 117002 (2014)","10.1103/PhysRevLett.112.117002",,"cond-mat.supr-con cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In mean-field descriptions of nodal d-wave superconductors, generic edges
exhibit dispersionless Majorana fermion bands at zero-energy. These states give
rise to an extensive ground-state degeneracy, and are protected by
time-reversal (TR) symmetry. We argue that the infinite density of states of
these flat-bands make them inherently unstable to interactions, and show that
repulsive interactions lead to edge FM which splits the flat bands. This edge
FM offers an explanation for the observation of splitting of zero-bias peaks in
edge tunneling in High-Tc cuprate superconductors. We argue that this mechanism
for splitting is more likely than previously proposed scenarios, and describe
its experimental consequences.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:06 GMT""}]","2014-03-26"
"1303.6957","Jennifer Yee","J.C. Yee (The Ohio State University)","WFIRST Planet Masses from Microlens Parallax","11 pages, 2 figures. Replaced 10/10/13 to reflect the version
  published in ApJL",,"10.1088/2041-8205/770/2/L31",,"astro-ph.EP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  I present a method using only a few ground-based observations of magnified
microlensing events to routinely measure the parallaxes of WFIRST events if
WFIRST is in an L2 orbit. This could be achieved for all events with Amax > 30
using target-ofopportunity observations of select WFIRST events, or with a
complementary, ground-based survey of the WFIRST field, which can push beyond
this magnification limit. When combined with a measurement of the angular size
of the Einstein ring, which is almost always measured in planetary events,
these parallax measurements will routinely give measurements of the lens masses
and hence, the absolute masses of the planets. They can also lead to mass
measurements for dark, isolated objects such as brown dwarfs, free-floating
planets, and stellar remnants if the size of the Einstein ring is measured.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:06 GMT""},{""version"":""v2"",""created"":""Thu, 10 Oct 2013 16:14:04 GMT""}]","2015-06-15"
"1303.6958","Tony Pan","Tony Pan, Daniel J. Patnaude, Abraham Loeb","Super-luminous X-ray Emission from the Interaction of Supernova Ejecta
  with Dense Circumstellar Shells","Submitted to MNRAS. 12 pages, 4 figures",,"10.1093/mnras/stt780",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  For supernova powered by the conversion of kinetic energy into radiation due
to the interactions of the ejecta with a dense circumstellar shell, we show
that there could be X-ray analogues of optically super-luminous SNe with
comparable luminosities and energetics. We consider X-ray emission from the
forward shock of SNe ejecta colliding into an optically-thin CSM shell, derive
simple expressions for the X-ray luminosity as a function of the circumstellar
shell characteristics, and discuss the different regimes in which the shock
will be radiative or adiabatic, and whether the emission will be dominated by
free-free radiation or line-cooling. We find that even with normal supernova
explosion energies of 10^51 erg, there exists CSM shell configurations that can
liberate a large fraction of the explosion energy in X-rays, producing
unabsorbed X-ray luminosities approaching 10^44 erg/s events lasting a few
months, or even 10^45 erg/s flashes lasting days. Although the large column
density of the circumstellar shell can absorb most of the flux from the initial
shock, the most luminous events produce hard X-rays that are less susceptible
to photoelectric absorption, and can counteract such losses by completely
ionizing the intervening material. Regardless, once the shock traverses the
entire circumstellar shell, the full luminosity could be available to
observers.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:08 GMT""}]","2015-06-15"
"1303.6959","Daniel Angl\'es-Alc\'azar","Daniel Angl\'es-Alc\'azar (1), Romeel Dav\'e (1 and 2), Feryal \""Ozel
  (1), Benjamin D. Oppenheimer (3) ((1) Arizona, (2) Cape Town, (3) Leiden)","Cosmological Zoom Simulations of z = 2 Galaxies: The Impact of Galactic
  Outflows","22 pages, 13 figures, ApJ accepted",,"10.1088/0004-637X/782/2/84",,"astro-ph.CO astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We use high-resolution cosmological zoom simulations with ~200 pc resolution
at z = 2 and various prescriptions for galactic outflows in order to explore
the impact of winds on the morphological, dynamical, and structural properties
of eight individual galaxies with halo masses ~ 10^11--2x10^12 Msun at z = 2.
We present a detailed comparison to spatially and spectrally resolved H{\alpha}
and other observations of z ~ 2 galaxies. We find that simulations without
winds produce massive, compact galaxies with low gas fractions, super-solar
metallicities, high bulge fractions, and much of the star formation
concentrated within the inner kpc. Strong winds are required to maintain high
gas fractions, redistribute star-forming gas over larger scales, and increase
the velocity dispersion of simulated galaxies, more in agreement with the
large, extended, turbulent disks typical of high-redshift star-forming
galaxies. Winds also suppress early star formation to produce high-redshift
cosmic star formation efficiencies in better agreement with observations.
Sizes, rotation velocities, and velocity dispersions all scale with stellar
mass in accord with observations. Our simulations produce a diversity of
morphological characteristics - among our three most massive galaxies, we find
a quiescent grand-design spiral, a very compact star-forming galaxy, and a
clumpy disk undergoing a minor merger; the clumps are evident in H{\alpha} but
not in the stars. Rotation curves are generally slowly rising, particularly
when calculated using azimuthal velocities rather than enclosed mass. Our
results are broadly resolution-converged. These results show that cosmological
simulations including outflows can produce disk galaxies similar to those
observed during the peak epoch of cosmic galaxy growth.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:08 GMT""},{""version"":""v2"",""created"":""Fri, 3 Jan 2014 22:00:03 GMT""}]","2015-06-15"
"1303.6960","Tony Pan","Tony Pan, Abraham Loeb","Finding Core Collapse Supernova from the Epoch of Reionization Behind
  Cluster Lenses","Submitted to MNRAS Letters. 5 pages, 5 figures",,"10.1093/mnrasl/slt089",,"astro-ph.HE astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Current surveys are underway to utilize gravitational lensing by galaxy
clusters with Einstein radii >35"" in the search for the highest redshift
galaxies. Associated supernova from the epoch of reionization would have their
fluxes boosted above the detection threshold, extending their duration of
visibility. We predict that the James Webb Space Telescope (JWST) will be able
to discover lensed core-collapse supernovae at redshifts exceeding z=7-8.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:11 GMT""}]","2015-06-15"
"1303.6961","Simon Portegies Zwart","Edward P. J. van den Heuvel (Amsterdam), Simon Portegies Zwart
  (Sterrewacht Leiden)","Are Super-Luminous supernovae and Long GRBs produced exclusively in
  young dense star clusters?","ApJ (23 pages, in press)",,"10.1088/0004-637X/779/2/114",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Super Luminous supernovae (SLSN) occur almost exclusively in small galaxies
(SMC/LMC-like or smaller), and the few SLSN observed in larger star-forming
galaxies always occur close to the nuclei of their hosts. Another type of
peculiar and highly energetic supernovae are the broad-line type Ic SNe (SN
Ic-BL) that are associated with long-duration gamma-ray bursts (LGRBs). Also
these have a strong preference for occurring in small (SMC/LMC-like or smaller)
star-forming galaxies, and in these galaxies LGRBs always occur in the
brightest spots. Studies of nearby star-forming galaxies that are similar to
the hosts of LGRBs show that these brightest spots are giant HII regions
produced by massive dense young star clusters with many hundreds of O- and
Wolf-Rayet-type stars. Such dense young clusters are also found in abundance
within a few hundred parsecs from the nucleus of larger galaxies like our own.
We argue that the SLSN and the SN Ic-BL/LGRBs are exclusive products of two
types of dynamical interactions in dense young star clusters. In our model the
high angular momentum of the collapsing stellar cores required for the
""engines"" of a SN Ic-BL results from the post-main sequence mergers of
dynamically produced cluster binaries with almost equal-mass components. The
merger produces a critically rotating single helium star with sufficent angular
momentum to produce a LGRB; the observed ""metal aversio"" of LGRBs is a natural
consequence of the model. We argue that, on the other hand, SLSN could be the
products of runaway multiple collisions in dense clusters, and we present (and
quantize) plausible scenarios of how the different types of SLSNs can be
produced.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:12 GMT""},{""version"":""v2"",""created"":""Mon, 28 Oct 2013 03:07:08 GMT""}]","2015-06-15"
"1303.6962","Lawrence Hall","L. J. Hall and G. G. Ross","Discrete Symmetries and Neutrino Mass Perturbations for \theta_{13}","20 pages",,"10.1007/JHEP11(2013)091","CERN-PH-TH/2012-175","hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The recent measurement of the third lepton mixing angle, \theta_{13}, has
shown that, although small compared to \theta_{12} and \theta_{23}, it is much
larger than anticipated in schemes that generate Tri-Bi-Maximal (TBM) or Golden
Ratio (GR) mixing. We develop a model-independent formalism for perturbations
away from exact TBM or GR mixing in the neutrino sector. Each resulting
perturbation scheme reflects an underlying symmetry structure and involves a
single complex parameter. We show that such perturbations can readily fit the
observed value of \theta_{13}, which is then correlated with a change in the
other mixing angles. We also determine the implication for the lepton CP
violating phases. For comparison we determine the predictions for Bi-Maximal
mixing corrected by charged lepton mixing and we discuss the accuracy that will
be needed to distinguish between the various schemes.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:12 GMT""},{""version"":""v2"",""created"":""Sat, 2 Nov 2013 01:56:11 GMT""}]","2015-06-15"
"1303.6963","Bela Bauer","Bela Bauer and Brendan P. Keller and Michele Dolfi and Simon Trebst
  and Andreas W. W. Ludwig","Gapped and gapless spin liquid phases on the Kagome lattice from chiral
  three-spin interactions","5+5 pages, 6+6 figures. Manuscript partially superseded by
  arXiv:1401.3017",,,,"cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We argue that a relatively simple model containing only SU(2)-invariant
chiral three-spin interactions on a Kagome lattice of S=1/2 spins can give rise
to both a gapped and a gapless quantum spin liquid. Our arguments are rooted in
a formulation in terms of network models of edge states and are backed up by a
careful numerical analysis. For a uniform choice of chirality on the lattice,
we realize the Kalmeyer-Laughlin state, i.e. a gapped spin liquid which is
identified as the nu=1/2 bosonic Laughlin state. For staggered chiralities, a
gapless spin liquid emerges which exhibits gapless spin excitations along lines
in momentum space, a feature that we probe by studying quasi-two-dimensional
systems of finite width. We thus provide a single, appealingly simple spin
model (i) for what is probably the simplest realization of the
Kalmeyer-Laughlin state to date, as well as (ii) for a non-Fermi liquid state
with lines of gapless SU(2) spin excitations.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:00:12 GMT""},{""version"":""v2"",""created"":""Sun, 7 Sep 2014 18:15:38 GMT""}]","2014-09-09"
"1303.6964","Muhammad Adeel Ajaib","M. Adeel Ajaib, Ilia Gogoladze, Qaisar Shafi and Cem Salih Un","A Predictive Yukawa Unified SO(10) Model: Higgs and Sparticle Masses","27 pages, 9 figures, 1 table. arXiv admin note: text overlap with
  arXiv:1112.2206",,,,"hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We revisit a class of supersymmetric SO(10) models with t-b-tau Yukawa
coupling unification condition, with emphasis on the prediction of the Higgs
mass. We discuss qualitative features in this model that lead to a Higgs mass
prediction close to 125 GeV. We show this with two distinct computing packages,
Isajet and SuSpect, and also show that they yield similar global features in
the parameter space of this model. We find that t-b-tau Yukawa coupling
unification prefers values of the CP-odd Higgs mass m_{A} to be around 600 GeV,
with all colored sparticle masses above 3 TeV. We also briefly discuss
prospects for testing this scenario with the ongoing and planned direct dark
matter detection experiments. In this class of models with t-b-tau Yukawa
unification, the neutralino dark matter particle is heavy
(m_{\tilde{\chi}_1^{0}} \gtrsim 400 \rm \ GeV), which coannihilates with a stau
to yield the correct relic abundance.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:01:41 GMT""}]","2013-03-29"
"1303.6965","Canran Xu","Canran Xu and Maxim G. Vavilov","Full Counting Statistics of Photons Emitted by Double Quantum Dot","9 pages, 7 figures","Phys. Rev. B 88, 195307 (2013)","10.1103/PhysRevB.88.195307",,"cond-mat.mes-hall quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We analyze the full counting statistics of photons emitted by a double
quantum dot (DQD) coupled to a high-quality microwave resonator by electric
dipole interaction. We show that at the resonant condition between the energy
splitting of the DQD and the photon energy in the resonator, photon statistics
exhibits both a sub-Poissonian distribution and antibunching. In the ideal
case, when the system decoherence stems only from photodetection, the photon
noise is reduced below one-half of the noise for the Poisson distribution and
is consistent with current noise. The photon distribution remains
sub-Poissonian even at moderate decoherence in the DQD. We demonstrate that
Josephson junction based photomultipliers can be used to experimentally assess
statistics of emitted photons.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:02:08 GMT""},{""version"":""v2"",""created"":""Thu, 21 Nov 2013 22:43:28 GMT""}]","2013-11-25"
"1303.6966","John O. Dabiri","John O. Dabiri, Sanjeeb Bose, Brad J. Gemmell, Sean P. Colin, and John
  H. Costello","An algorithm to estimate unsteady and quasi-steady pressure fields from
  velocity field measurements","A free MATLAB implementation of this algorithm is available at
  http://dabiri.caltech.edu/software.html","Journal of Experimental Biology (2014) 217: 331-336","10.1242/jeb.092767",,"physics.flu-dyn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We describe and characterize a method for estimating the pressure field
corresponding to velocity field measurements, such as those obtained by using
particle image velocimetry. The pressure gradient is estimated from a time
series of velocity fields for unsteady calculations or from a single velocity
field for quasi-steady calculations. The corresponding pressure field is
determined based on median polling of several integration paths through the
pressure gradient field in order to reduce the effect of measurement errors
that accumulate along individual integration paths. Integration paths are
restricted to the nodes of the measured velocity field, thereby eliminating the
need for measurement interpolation during this step and significantly reducing
the computational cost of the algorithm relative to previous approaches. The
method is validated by using numerically-simulated flow past a stationary,
two-dimensional bluff body and a computational model of a three-dimensional,
self-propelled anguilliform swimmer to study the effects of spatial and
temporal resolution, domain size, signal-to-noise ratio, and out-of-plane
effects. Particle image velocimetry measurements of a freely-swimming jellyfish
medusa and a freely-swimming lamprey are analyzed using the method to
demonstrate the efficacy of the approach when applied to empirical data.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:04:46 GMT""},{""version"":""v2"",""created"":""Mon, 20 Jan 2014 23:58:13 GMT""}]","2014-01-22"
"1303.6967","Elizabeth Adams","Elizabeth A. K. Adams, Riccardo Giovanelli, Martha P. Haynes","A Catalog of Ultra-compact High Velocity Clouds from the ALFALFA Survey:
  Local Group Galaxy Candidates?","24 pages, 16 figures, published in ApJ, article updated for
  corrections to published version","Elizabeth A. K. Adams et al. 2013 ApJ, 768, 77","10.1088/0004-637X/768/1/77",,"astro-ph.CO astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a catalog of 59 ultra-compact high velocity clouds (UCHVCs)
extracted from the 40% complete ALFALFA HI-line survey. The ALFALFA UCHVCs have
median flux densities of 1.34 Jy km/s, median angular diameters of 10', and
median velocity widths of 23 km/s. We show that the full UCHVC population
cannot easily be associated with known populations of high velocity clouds. Of
the 59 clouds presented here, only 11 are also present in the compact cloud
catalog extracted from the commensal GALFA-HI survey, demonstrating the utility
of this separate dataset and analysis. Based on their sky distribution and
observed properties, we infer that the ALFALFA UCHVCs are consistent with the
hypothesis that they may be very low mass galaxies within the Local Volume. In
that case, most of their baryons would be in the form of gas, and because of
their low stellar content, they remain unidentified by extant optical surveys.
At distances of ~1 Mpc, the UCHVCs have neutral hydrogen (HI) masses of ~10^5
-10^6 M_sun, HI diameters of ~2-3 kpc, and indicative dynamical masses within
the HI extent of ~10^7 - 10^8 M_sun, similar to the Local Group ultra-faint
dwarf Leo T. The recent ALFALFA discovery of the star-forming, metal-poor, low
mass galaxy Leo P demonstrates that this hypothesis is true in at least one
case. In the case of the individual UCHVCs presented here, confirmation of
their extragalactic nature will require further work, such as the
identification of an optical counterpart to constrain their distance.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:05:06 GMT""},{""version"":""v2"",""created"":""Wed, 17 Apr 2013 14:45:42 GMT""}]","2013-04-18"
"1303.6968","Deatrick Foster","D. L. Foster (1 and 2), P. A. Charles (3), D. A. Swartz (4), R. Misra
  (5), and K. G. Stassun (2 and 6) ((1) SAAO, (2) Vanderbilt U., (3) U. of
  Southampton, (4) USRA, NASA MSFC, (5) IUCAA, (6) Fisk U.)","Monitoring the Very-Long-Term Variability of X-ray Sources in the Giant
  Elliptical Galaxy M87","19 pages, 19 figures, 1 table, Accepted for publication in MNRAS.
  Updated to correct typos in previous version",,"10.1093/mnras/stt557",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We report on our search for very-long-term variability (weeks to years) in
X-ray binaries (XRBs) in the giant elliptical galaxy M87. We have used archival
Chandra imaging observations to characterise the long-term variability of 8 of
the brightest members of the XRB population in M87. The peak brightness of some
of the sources exceeded the ultra luminous X-ray source (ULX) threshold
luminosity of ~ 10^{39} erg/s, and one source could exhibit dips or eclipses.
We show that for one source, if it has similar modulation amplitude as in
SS433, then period recoverability analysis on the current data would detect
periodic modulations, but only for a narrow range of periods less than 120
days. We conclude that a dedicated monitoring campaign, with appropriately
defined sampling, is essential if we are to investigate properly the nature of
the long-term modulations such as those seen in Galactic sources.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:07:36 GMT""},{""version"":""v2"",""created"":""Mon, 6 May 2013 19:56:44 GMT""}]","2013-05-07"
"1303.6969","Jeffrey Winicour","J. Winicour","The affine-null metric formulation of Einstein's equations","Version to appear in Physical Review D",,"10.1103/PhysRevD.87.124027",,"gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The details are presented of a new evolution algorithm for the characteristic
initial-boundary value problem based upon an affine parameter rather than the
areal radial coordinate used in the Bondi-Sachs formulation. The advantages
over the Bondi-Sachs version are discussed, with particular emphasis on the
application to the characteristic extraction of the gravitational waveform from
Cauchy simulations of general relativistic astrophysical systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:08:46 GMT""},{""version"":""v2"",""created"":""Fri, 14 Jun 2013 17:00:26 GMT""}]","2015-06-15"
"1303.6970","Denis Klevers","Mirjam Cveti\v{c} and Denis Klevers and Hernan Piragua","F-Theory Compactifications with Multiple U(1)-Factors: Constructing
  Elliptic Fibrations with Rational Sections","68 pages, 5 figures, 3 tables; v2: minor changes, 1 figure added; v3:
  typos corrected, references added",,"10.1007/JHEP06(2013)067","UPR-1249-T","hep-th hep-ph math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study F-theory compactifications with U(1)xU(1) gauge symmetry on
elliptically fibered Calabi-Yau manifolds with a rank two Mordell-Weil group.
We find that the natural presentation of an elliptic curve E with two rational
points and a zero point is the generic Calabi-Yau onefold in dP_2. We determine
the birational map to its Tate and Weierstrass form and the coordinates of the
two rational points in Weierstrass form. We discuss its resolved elliptic
fibrations over a general base B and classify them in the case of B=P^2. A
thorough analysis of the generic codimension two singularities of these
elliptic Calabi-Yau manifolds is presented. This determines the general
U(1)xU(1)-charges of matter in corresponding F-theory compactifications. The
matter multiplicities for the fibration over P^2 are determined explicitly and
shown to be consistent with anomaly cancellation. Explicit toric examples are
constructed, both with U(1)xU(1) and SU(5)xU(1)xU(1) gauge symmetry. As a
by-product, we prove the birational equivalence of the two elliptic fibrations
with elliptic fibers in the two blow-ups Bl_(1,0,0)P^2(1,2,3) and
Bl_(0,1,0)P^2(1,1,2) employing birational maps and extremal transitions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:09:50 GMT""},{""version"":""v2"",""created"":""Tue, 9 Apr 2013 22:01:56 GMT""},{""version"":""v3"",""created"":""Wed, 3 Jul 2013 00:37:31 GMT""}]","2015-06-15"
"1303.6971","Cody Jones","Cody Jones","Composite Toffoli gate with two-round error detection","8 pages, 8 figures","Phys. Rev. A 87, 052334 (2013)","10.1103/PhysRevA.87.052334",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce a fault-tolerant construction to implement a composite quantum
operation of four overlapping Toffoli gates. The same construction can produce
two independent Toffoli gates. This result lowers resource overheads in designs
for quantum computers by more than an order of magnitude. The procedure uses
Clifford operations and 64 copies of the non-Clifford gate $T = \exp[i \pi (I -
\sigma^z) /8]$. Quantum codes detect errors in the circuit. When the dominant
source of error is $T$-gate failure with probability $p$, then the composite
Toffoli circuit has postselected failure rate of $3072p^4$ to lowest order.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:09:55 GMT""}]","2013-08-06"
"1303.6972","Peng Zhao","Peter Sarnak and Peng Zhao, Appendix by Michael Woodbury","The Quantum Variance of the Modular Surface",,,,,"math.NT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The variance of observables of quantum states of the Laplacian on the modular
surface is calculated in the semiclassical limit. It is shown that this
hermitian form is diagonalized by the irreducible representations of the
modular quotient and on each of these it is equal to the classical variance of
the geodesic flow after the insertion of a subtle arithmetical special value of
the corresponding $L$-function.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:10:32 GMT""},{""version"":""v2"",""created"":""Tue, 7 May 2013 15:10:19 GMT""},{""version"":""v3"",""created"":""Tue, 13 Feb 2018 15:41:55 GMT""}]","2018-02-14"
"1303.6973","Elizabeth  Jurisich","Ben L. Cox and Elizabeth G. Jurisich","Realizations of the three point algebra $\mathfrak{sl}(2, \mathcal R)
  \oplus\left(\Omega_{\mathcal R}/d{\mathcal R}\right)$","arXiv admin note: text overlap with arXiv:0902.1273. This version has
  a correction to a scalar in Cor 2.3. The representations of the algebra are
  unaffected","Pacific Journal of Mathematics vol. 270, No. 1, 2014","10.2140/pjm.2014.270.27",,"math.RT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We describe the universal central extension of the three point current
algebra $\mathfrak{sl}(2,\mathcal R)$ where $\mathcal R=\mathbb
C[t,t^{-1},u\,|\,u^2=t^2+4t ]$ and construct realizations of it in terms of
sums of partial differential operators.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:28:30 GMT""},{""version"":""v2"",""created"":""Mon, 26 Aug 2013 00:49:26 GMT""},{""version"":""v3"",""created"":""Sun, 22 Feb 2015 23:10:21 GMT""}]","2015-02-24"
"1303.6974","Jan Korger","Jan Korger, Andrea Aiello, Vanessa Chille, Peter Banzer, Christoffer
  Wittmann, Norbert Lindlein, Christoph Marquardt, Gerd Leuchs","Observation of the geometric spin Hall effect of light","9 pages, 6 figures","Phys. Rev. Lett. 112, 113902 (2014)","10.1103/PhysRevLett.112.113902",,"physics.optics physics.class-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The spin Hall effect of light (SHEL) is the photonic analogue of spin Hall
effects occurring for charge carriers in solid-state systems. Typical examples
of this intriguing phenomenon occur when a light beam refracts at an air-glass
interface, or when it is projected onto an oblique plane, the latter effect
being known as geometric SHEL. It amounts to a polarization-dependent
displacement perpendicular to the plane of incidence. Here, we experimentally
demonstrate the geometric SHEL for a light beam transmitted across an oblique
polarizer. We find that the spatial intensity distribution of the transmitted
beam depends on the incident state of polarization and its centroid undergoes a
positional displacement exceeding one wavelength. This novel phenomenon is
virtually independent from the material properties of the polarizer and, thus,
reveals universal features of spin-orbit coupling.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:28:51 GMT""},{""version"":""v2"",""created"":""Wed, 3 Apr 2013 09:55:07 GMT""},{""version"":""v3"",""created"":""Fri, 21 Jun 2013 13:40:02 GMT""},{""version"":""v4"",""created"":""Tue, 22 Oct 2013 09:28:36 GMT""}]","2014-03-26"
"1303.6975","Krzysztof Urbanowski","K. Urbanowski, K. Raczynska","Possible Emission of Cosmic $X$-- and $\gamma$--rays by Unstable
  Particles at Late Times","15 pages, 5 figures","Physics Letters B731(2014)236","10.1016/j.physletb.2014.02.043",,"astro-ph.HE hep-ph hep-th quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Not all astrophysical mechanisms of the emission of electromagnetic radiation
including $X$-- and $\gamma$-- rays coming from the space are clear. We find
that charged unstable particles as well as neutral unstable particles with
non--zero magnetic moment which live sufficiently long may emit electromagnetic
radiation. This new mechanism is connected with the properties of unstable
particles at the post exponential time region. Analyzing the transition time
region between exponential and non-exponential form of the survival amplitude
it is found that the instantaneous energy of the unstable particle can take
very large values, much larger than the energy of this state for times from the
exponential time region. Basing on the results obtained for the model
considered, it is shown that this purely quantum mechanical effect may be
responsible for causing unstable particles to emit electromagnetic--, $X$-- or
$\gamma$--rays at some time intervals from the transition time regions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:34:52 GMT""},{""version"":""v2"",""created"":""Fri, 29 Mar 2013 16:53:58 GMT""},{""version"":""v3"",""created"":""Sat, 15 Mar 2014 22:33:36 GMT""}]","2014-03-18"
"1303.6976","Monica Patriche","Monica Patriche","The reduction of qualitative games","21 pages",,,,"cs.GT math.OC","http://creativecommons.org/licenses/by/3.0/","  We extend the study of the iterated elimination of strictly dominated
strategies (IESDS) from Nash strategic games to a class of qualitative games.
Also in this case, the IESDS process leads us to a kind of 'rationalizable'
result. We define several types of dominance relation and game reduction and
establish conditions under which a unique and nonempty maximal reduction
exists. We generalize, in this way, some results due to Dufwenberg and Stegeman
(2002) and Apt (2007).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:48:10 GMT""}]","2013-03-29"
"1303.6977","Christos Dimitrakakis","Christos Dimitrakakis, Nikolaos Tziortziotis","ABC Reinforcement Learning","Corrected version of paper appearing in ICML 2013",,,,"stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper introduces a simple, general framework for likelihood-free
Bayesian reinforcement learning, through Approximate Bayesian Computation
(ABC). The main advantage is that we only require a prior distribution on a
class of simulators (generative models). This is useful in domains where an
analytical probabilistic model of the underlying process is too complex to
formulate, but where detailed simulation models are available. ABC-RL allows
the use of any Bayesian reinforcement learning technique, even in this case. In
addition, it can be seen as an extension of rollout algorithms to the case
where we do not know what the correct model to draw rollouts from is. We
experimentally demonstrate the potential of this approach in a comparison with
LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology
in principle, even when non-sufficient statistics are used.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:51:33 GMT""},{""version"":""v2"",""created"":""Wed, 8 May 2013 12:54:53 GMT""},{""version"":""v3"",""created"":""Tue, 18 Jun 2013 09:42:59 GMT""},{""version"":""v4"",""created"":""Fri, 28 Jun 2013 11:18:26 GMT""}]","2013-07-01"
"1303.6978","German Lugones","V. R. C. Mour\~ao Roque and G. Lugones","Unveiling the cosmological QCD phase transition through the eLISA/NGO
  detector","to appear in Physical Review D","Phys. Rev. D 87, 083516 (2013)","10.1103/PhysRevD.87.083516",,"astro-ph.CO hep-lat hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the evolution of turbulence in the early universe at the QCD epoch
using a state-of-the-art equation of state derived from lattice QCD
simulations. Since the transition is a crossover we assume that temperature and
velocity fluctuations were generated by some event in the previous history of
the Universe and survive until the QCD epoch due to the extremely large
Reynolds number of the primordial fluid. The fluid at the QCD epoch is assumed
to be non-viscous, based on the fact that the viscosity per entropy density of
the quark gluon plasma obtained from heavy-ion collision experiments at the
RHIC and the LHC is extremely small.
  Our hydrodynamic simulations show that the velocity spectrum is very
different from the Kolmogorov power law considered in studies of primordial
turbulence that focus on first order phase transitions. This is due to the fact
that there is no continuous injection of energy in the system and the viscosity
of the fluid is negligible. Thus, as kinetic energy cascades from the larger to
the smaller scales, a large amount of kinetic energy is accumulated at the
smallest scales due to the lack of dissipation.
  We have obtained the spectrum of the gravitational radiation emitted by the
motion of the fluid finding that, if typical velocity and temperature
fluctuations have an amplitude $(\Delta v) /c \gtrsim 10^{-2}$ and/or $\Delta
T/T_c \gtrsim 10^{-3}$, they would be detected by eLISA at frequencies larger
than $\sim 10^{-4}$ Hz.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:57:51 GMT""}]","2013-08-21"
"1303.6979","Monica Patriche","Monica Patriche","Equilibrium existence results for a class of discontinuous games","28 pages",,,,"math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce the notions of w-lower semicontinuous and almost w-lower
semicontinuous correspondence with respect to a given set and prove a new
fixed-point theorem. We also introduce the notion of correspondence with
e-LSCS-property. As applications we obtain some new equilibrium theorems for
abstract economies and for generalized multiobjective games.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:04:50 GMT""}]","2013-03-29"
"1303.6980","Alexander Philippov","Alexander A. Philippov and Roman R. Rafikov","Analysis of Spin-Orbit Misalignment in Eclipsing Binary DI Herculis","Accepted by ApJ, 12 pages, 10 figures",,"10.1088/0004-637X/768/2/112",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Eclipsing binary DI Herculis (DI Her) is known to exhibit anomalously slow
apsidal precession, below the rate predicted by the general relativity. Recent
measurements of the Rossiter-McLauglin effect indicate that stellar spins in DI
Her are almost orthogonal to the orbital angular momentum, which explains the
anomalous precession in agreement with the earlier theoretical suggestion by
Shakura. However, these measurements yield only the projections of the
spin-orbit angles onto the sky plane, leaving the spin projection onto our line
of sight unconstrained. Here we describe a method of determining the full
three-dimensional spin orientation of the binary components relying on the use
of the gravity darkening effect, which is significant for the rapidly rotating
stars in DI Her. Gravity darkening gives rise to nonuniform brightness
distribution over the stellar surface, the pattern of which depends on the
stellar spin orientation. Using archival photometric data obtained during
multiple eclipses spread over several decades we are able to constrain the
unknown spin angles in DI Her with this method, finding that spin axes of both
stars lie close to the plane of the sky. Our procedure fully accounts for the
precession of stellar spins over the long time span of observations.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:20:00 GMT""}]","2015-06-15"
"1303.6981","Rafael Stern","Rafael Bassi Stern, Joseph Born Kadane","Coherence of countably many bets","15 pages",,"10.1007/s10959-013-0489-9",,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  De Finetti's betting argument is used to justify finitely additive
probabilities when only finitely many bets are considered. Under what
circumstances can countably many bets be used to justify countable additivity?
In this framework, one faces issues such as the convergence of the returns of
the bet. Generalizations of de Finetti's argument depend on what type of
conditions on convergence are required of the bets under consideration. Two new
such conditions are compared with others presented in the literature.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:20:20 GMT""}]","2013-05-08"
"1303.6982","Monica Patriche","Monica Patriche","Fixed Point Theorems and applications in Theory of Games","20 pages Accepted for publication in Fixed Point Theory",,,,"math.OC","http://creativecommons.org/licenses/by/3.0/","  We introduce the notions of weakly *-concave and weakly naturally
quasi-concave correspondence and prove fixed point theorems and continuous
selection theorems for these kind of correspondences. As applications in the
game theory, by using a tehnique based on a continuous selection, we establish
new existence results for the equilibrium of the abstract economies. The
constraint correspondences are weakly naturally quasi-concave. We show that the
equilibrium exists without continuity assumptions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:24:54 GMT""}]","2013-03-29"
"1303.6983","Philip Richerme","Philip Richerme, Crystal Senko, Simcha Korenblit, Jacob Smith, Aaron
  Lee, Rajibul Islam, Wesley C. Campbell, Christopher Monroe","Quantum Catalysis of Magnetic Phase Transitions in a Quantum Simulator","New data in Fig. 3, and much of the paper rewritten","Phys. Rev. Lett. 111, 100506 (2013)","10.1103/PhysRevLett.111.100506",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We control quantum fluctuations to create the ground state magnetic phases of
a classical Ising model with a tunable longitudinal magnetic field using a
system of 6 to 10 atomic ion spins. Due to the long-range Ising interactions,
the various ground state spin configurations are separated by multiple
first-order phase transitions, which in our zero temperature system cannot be
driven by thermal fluctuations. We instead use a transverse magnetic field as a
quantum catalyst to observe the first steps of the complete fractal devil's
staircase, which emerges in the thermodynamic limit and can be mapped to a
large number of many-body and energy-optimization problems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:25:12 GMT""},{""version"":""v2"",""created"":""Tue, 25 Jun 2013 19:04:42 GMT""}]","2013-09-06"
"1303.6984","Ruslan Vaulin","Rahul Biswas, Lindy Blackburn, Junwei Cao, Reed Essick, Kari Alison
  Hodge, Erotokritos Katsavounidis, Kyungmin Kim, Young-Min Kim, Eric-Olivier
  Le Bigot, Chang-Hwan Lee, John J. Oh, Sang Hoon Oh, Edwin J. Son, Ruslan
  Vaulin, Xiaoge Wang and Tao Ye","Application of machine learning algorithms to the study of noise
  artifacts in gravitational-wave data","21 pages, 8 figures",,"10.1103/PhysRevD.88.062003",,"astro-ph.IM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The sensitivity of searches for astrophysical transients in data from the
LIGO is generally limited by the presence of transient, non-Gaussian noise
artifacts, which occur at a high-enough rate such that accidental coincidence
across multiple detectors is non-negligible. Furthermore, non-Gaussian noise
artifacts typically dominate over the background contributed from stationary
noise. These ""glitches"" can easily be confused for transient gravitational-wave
signals, and their robust identification and removal will help any search for
astrophysical gravitational-waves. We apply Machine Learning Algorithms (MLAs)
to the problem, using data from auxiliary channels within the LIGO detectors
that monitor degrees of freedom unaffected by astrophysical signals. The number
of auxiliary-channel parameters describing these disturbances may also be
extremely large; an area where MLAs are particularly well-suited. We
demonstrate the feasibility and applicability of three very different MLAs:
Artificial Neural Networks, Support Vector Machines, and Random Forests. These
classifiers identify and remove a substantial fraction of the glitches present
in two very different data sets: four weeks of LIGO's fourth science run and
one week of LIGO's sixth science run. We observe that all three algorithms
agree on which events are glitches to within 10% for the sixth science run
data, and support this by showing that the different optimization criteria used
by each classifier generate the same decision surface, based on a
likelihood-ratio statistic. Furthermore, we find that all classifiers obtain
similar limiting performance, suggesting that most of the useful information
currently contained in the auxiliary channel parameters we extract is already
being used.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:25:27 GMT""}]","2013-10-02"
"1303.6985","Irfan Siap","Irfan Siap and Ismail Aydogdu","Counting The Generator Matrices of $\mathbb{Z}_{2}\mathbb{Z}_{8}$-Codes",,,,,"math.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we count the number of matrices whose rows generate different
$\mathbb{Z}_2\mathbb{Z}_8$ additive codes. This is a natural generalization of
the well known Gaussian numbers that count the number of matrices whose rows
generate vector spaces with particular dimension over finite fields. Due to
this similarity we name this numbers as Mixed Generalized Gaussian Numbers
(MGN). The MGN formula by specialization leads to the well known formula for
the number of binary codes and the number of codes over $\mathbb{Z}_8,$ and for
additive $\mathbb{Z}_2\mathbb{Z}_4$ codes. Also, we conclude by some properties
and examples of the MGN numbers that provide a good source for new number
sequences that are not listed in The On-Line Encyclopedia of Integer Sequences.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:29:16 GMT""}]","2013-03-29"
"1303.6986","John Sheckelton","John P. Sheckelton, James R. Neilson, Daniel G. Soltan, and Tyrel M.
  McQueen","Possible valence-bond condensation in the frustrated cluster magnet
  LiZn2Mo3O8","30 pages, 10 figures","Nature Materials 11 (2012) 493-496","10.1038/nmat3329",,"cond-mat.mtrl-sci cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The emergence of complex electronic behaviour from simple ingredients has
resulted in the discovery of numerous states of matter. Many examples are found
in systems exhibiting geometric magnetic frustration, which prevents
simultaneous satisfaction of all magnetic interactions. This frustration gives
rise to complex magnetic properties such as chiral spin structures
orbitally-driven magnetism, spin-ice behavior exhibiting Dirac strings with
magnetic monopoles, valence bond solids, and spin liquids. Here we report the
synthesis and characterization of LiZn2Mo3O8, a geometrically frustrated
antiferromagnet in which the magnetic moments are localized on small transition
metal clusters rather than individual ions. By doing so, first order
Jahn-Teller instabilities and orbital ordering are prevented, allowing the
strongly interacting magnetic clusters in LiZn2Mo3O8 to probably give rise to
an exotic condensed valence-bond ground state reminiscent of the proposed
resonating valence bond state. Our results also link magnetism on clusters to
geometric magnetic frustration in extended solids, demonstrating a new approach
for unparalleled chemical control and tunability in the search for collective,
emergent electronic states of matter.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:34:31 GMT""}]","2013-03-29"
"1303.6987","Roger Johnson","R. D. Johnson, P. Barone, A. Bombardi, R. J. Bean, S. Picozzi, P. G.
  Radaelli, Y. S. Oh, S-W. Cheong, and L. C. Chapon","X-ray imaging and multiferroic coupling of cycloidal magnetic domains in
  ferroelectric monodomain BiFeO3","Published in Physical Review Letters","R. D. Johnson et al., PRL 110, 217206 (2013)","10.1103/PhysRevLett.110.217206",,"cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Magnetic domains at the surface of a ferroelectric monodomain BiFeO3 single
crystal have been imaged by hard X-ray magnetic scattering. Magnetic domains up
to several hundred microns in size have been observed, corresponding to
cycloidal modulations of the magnetization along the wave-vector
k=2\pi(\delta,\delta,0) and symmetry equivalent directions. The rotation
direction of the magnetization in all magnetic domains, determined by
diffraction of circularly polarized light, was found to be unique and in
agreement with predictions of a combined approach based on a spin-model
complemented by relativistic density-functional simulations. Imaging of the
surface shows that the largest adjacent domains display a 120 degree vortex
structure.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:39:37 GMT""},{""version"":""v2"",""created"":""Fri, 24 May 2013 09:58:30 GMT""}]","2013-05-27"
"1303.6988","Dongwook Lee","Dongwook Lee","A Solution Accurate, Efficient and Stable Unsplit Staggered Mesh Scheme
  for Three Dimensional Magnetohydrodynamics","31 pages, 21 figures",,"10.1016/j.jcp.2013.02.049",,"physics.comp-ph astro-ph.HE math.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we extend the unsplit staggered mesh scheme (USM) for 2D
magnetohydrodynamics (MHD) (Lee and Deane, 2009) to a full 3D MHD scheme. The
scheme is a finite-volume Godunov method consisting of a constrained transport
(CT) method and an efficient and accurate single-step, directionally unsplit
multidimensional data reconstruction-evolution algorithm, which extends the
original 2D corner transport upwind (CTU) method (Colella, 1990). We present
two types of data reconstruction-evolution algorithms for 3D: (1) a reduced CTU
scheme and (2) a full CTU scheme. The reduced 3D CTU scheme is a variant of a
simple 3D extension of the 2D CTU method by Colella (1990) and is considered as
a direct extension from the 2D USM scheme. The full 3D CTU scheme is our
primary 3D solver which includes all multidimensional cross-derivative terms
for stability. The latter method is logically analogous to the 3D unsplit CTU
method by Saltzman. The major novelties in our algorithms are twofold. First,
we extend the reduced CTU scheme to the full CTU scheme which is able to run
with CFL numbers close to unity. Both methods utilize the transverse update
technique developed in the 2D USM algorithm to account for transverse fluxes
without solving intermediate Riemann problems, which in turn gives
cost-effective 3D methods by reducing the total number of Riemann solves. The
proposed algorithms are simple and efficient especially when including
multidimensional MHD terms that maintain in-plane magnetic field dynamics.
Second, we introduce a new CT scheme that makes use of proper upwind
information in taking averages of electric fields.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:48:52 GMT""}]","2013-03-29"
"1303.6989","Wojciech Dorabiala","Bernard Badzioch, David Blanc, and Wojciech Dorabiala","Recognizing mapping spaces","24 pages, incorporated corrections based on referee's report, to
  appear in Journal of Pure and Applied Algebra",,,,"math.AT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Given a fixed object $A$ in a suitable pointed simplicial model category
$\C$, we study the problem of recovering the target $Y$ from the pointed
mapping space \w{\mapa(A,Y)} (up to $A$-equivalence). We describe a recognition
principle, modelled on the classical ones for loop spaces, but using the more
general notion of an \emph{\Ama[.]} It has an associated transfinite procedure
for recovering \w{\CWA Y} from \w[,]{\mapa(A,Y)} inspired by Dror-Farjoun's
construction of \ww{\CWA{}}-approximations.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:49:57 GMT""},{""version"":""v2"",""created"":""Thu, 23 May 2013 18:38:29 GMT""}]","2013-05-24"
"1303.6990","Omar Gamel","Omar Gamel and Daniel F. V. James","The Complete Positivity of Classical Polarization Maps","6 pages","Optics Letters, Vol. 36, Issue 15, pp. 2821-2823 (2011)","10.1364/OL.36.002821",,"physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Mueller and Jones matrices have been thoroughly studied as mathematical tools
to describe the manipulation of the polarization state of classical light. In
particular, the most general physical transformation on the polarization state
has been represented as an ensemble of Jones matrices, as $\sum_i V_i \Phi
V^{\dagger}_i$. But this has generally been directly assumed without proof by
most authors. In this Letter, we derive this expression from simple physical
principles and the matrix theory of positive maps.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:56:18 GMT""},{""version"":""v2"",""created"":""Sun, 6 Oct 2013 16:10:38 GMT""},{""version"":""v3"",""created"":""Tue, 29 Sep 2015 18:34:49 GMT""}]","2015-09-30"
"1303.6991","Monica Patriche","Monica Patriche","On the maximal reduction of games","20 pages",,,,"math.OC","http://creativecommons.org/licenses/by/3.0/","  We study the conditions under which the iterated elimination of strictly
dominated strategies is order independent and we identify a class of
discontinuous games for which order does not matter. In this way, we answer the
open problem raised by M. Dufwenberg and M. Stegeman (2002) and generalize
their main results. We also establish new theorems concerning the existence and
uniqueness of the maximal game reduction when the pure strategies are dominated
by mixed strategies.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:03:14 GMT""}]","2013-03-29"
"1303.6992","William Kleiber","William Kleiber, Stephan R. Sain, Matthew J. Heaton, Michael
  Wiltberger, C. Shane Reese, Derek Bingham","Parameter tuning for a multi-fidelity dynamical model of the
  magnetosphere","Published in at http://dx.doi.org/10.1214/13-AOAS651 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2013, Vol. 7, No. 3, 1286-1310","10.1214/13-AOAS651","IMS-AOAS-AOAS651","stat.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Geomagnetic storms play a critical role in space weather physics with the
potential for far reaching economic impacts including power grid outages, air
traffic rerouting, satellite damage and GPS disruption. The LFM-MIX is a
state-of-the-art coupled magnetospheric-ionospheric model capable of simulating
geomagnetic storms. Imbedded in this model are physical equations for turning
the magnetohydrodynamic state parameters into energy and flux of electrons
entering the ionosphere, involving a set of input parameters. The exact values
of these input parameters in the model are unknown, and we seek to quantify the
uncertainty about these parameters when model output is compared to
observations. The model is available at different fidelities: a lower fidelity
which is faster to run, and a higher fidelity but more computationally intense
version. Model output and observational data are large spatiotemporal systems;
the traditional design and analysis of computer experiments is unable to cope
with such large data sets that involve multiple fidelities of model output. We
develop an approach to this inverse problem for large spatiotemporal data sets
that incorporates two different versions of the physical model. After an
initial design, we propose a sequential design based on expected improvement.
For the LFM-MIX, the additional run suggested by expected improvement
diminishes posterior uncertainty by ruling out a posterior mode and shrinking
the width of the posterior distribution. We also illustrate our approach using
the Lorenz `96 system of equations for a simplified atmosphere, using known
input parameters. For the Lorenz `96 system, after performing sequential runs
based on expected improvement, the posterior mode converges to the true value
and the posterior variability is reduced.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:09:56 GMT""},{""version"":""v2"",""created"":""Thu, 5 Dec 2013 09:17:02 GMT""}]","2013-12-06"
"1303.6993","Alfred Bennun","Alfred Bennun","The coupling of thermodynamics with the organizational water-protein
  intra-dynamics driven by the H-bonds dissipative potential of cluster water","9 pages, 2 figures",,,,"q-bio.MN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Red cell-Hb-CSF functions as a sensor adapting response to Hb
heterotropic equilibriums. At the lungs O2 and Mg2+, each one increasing
affinity for the other stabilize the relax (R) form [(O2)4Hb(Mg)2].(H2O)R. At
tissue level, the inclusion of H+ and 2,3-DPG excludes O2 and Mg2+ to stabilize
the tense (T) form 2,3-DPG-deoxyHb-(H2O)T. Both senses are integrated into a
cycle T into R and R into T, without involving a direct reversal. The
dissipative potential of water cluster (H2O)n interacts with the hydrophilic
asymmetries of Hb, to restrict randomness of the kinetic sense implicated in a
single peak for activation energy (Ea). The hydration shells could sequence an
enhanced Ea into several peaks, to sequentially activate transitions states.
Hence, changes in dipole state, sliding, pKa, n-H-bonds, etc., could became
concatenated for vectoriality. (H2O)n by the loss of H-bonds couple with to the
hydration turnover of proteins and ions to result in incomplete water cluster
(H2O)n*, with a lower-n. (H2O)n* became a carrier of heat/entropy into the
cerebrospinal fluid (CSF) which has to be replaced 3.7 times per day. OxyHb
formation involves sliding-down of alpha vs beta chains, to shift alpha1 and
alpha2 Pro 44 into allowing the entrance of a fully hydrated
[Mg.(H2O)6](H2O)12-14(2+) (or Zn2+) into the hydrophilic beta2-alpha1 and
beta1-alpha2 interfaces. OxyHb pKa of 6.4 leads to H+-dissociation increasing
negative charge of R-groups. This at beta2-alpha1 sequence two tetradentate
chelates, first an Mg2+, bonding with beta2 His 92 and a second Mg2+ with
alpha1 His 87, to cooperatively release hindrance. The interconversion of
oxy-to-deoxyHb, pKa=8, leads to the amphoteric imidazole to became positively
charged and proximal histidines return into hindrance position, releasing the
incomplete hydrated Mg.(H2O)inc(2+) and O2 into CSF.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:14:45 GMT""}]","2013-03-29"
"1303.6994","Tobias Meng","Tobias Meng and Daniel Loss","Strongly anisotropic spin response as a signature of the helical regime
  in Rashba nanowires","7 pages, 1 figure, final version","Phys. Rev. B 88, 035437 (2013)","10.1103/PhysRevB.88.035437",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Rashba nanowires in a magnetic field exhibit a helical regime when the
spin-orbit momentum is close to the Fermi momentum, k_F \approx k_{SO}. We show
that this regime is characterized by a strongly anisotropic electron spin
susceptibility, with an exponentially suppressed signal along one direction in
spin space, and that there are no low frequency spin fluctuations along this
direction. Since the spin response in the gapless regime k_F \not \approx
k_{SO} has a power law behavior in all three directions, spin measurements
provide a signature of the helical regime that complements spin-insensitive
conductance measurements.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:18:00 GMT""},{""version"":""v2"",""created"":""Mon, 5 Aug 2013 08:24:11 GMT""}]","2013-08-06"
"1303.6995","Norimi Yokozaki","Masahiro Ibe and Tsutomu T. Yanagida and Norimi Yokozaki","Muon g-2 and 125 GeV Higgs in Split-Family Supersymmetry","20 pages, 5 figures",,"10.1007/JHEP08(2013)067",,"hep-ph hep-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We discuss the minimal supersymmetric standard model with ""split-family""
spectrum where the sfermions in the first two generations are in the hundreds
GeV to a TeV range while the sfermions in the third generation are in the range
of tens TeV. With the split-family spectrum, the deviation of the muon g-2 and
the observed Higgs boson mass are explained simultaneously. It is predicted
that the gluino and the squarks in the first two generations are within the
reach of the LHC experiments in most favored parameter space for the universal
gaugino mass, which can be tested by searching for events with missing
transverse energy or events with stable charged massive particles. We also
point out that the split-family scenario can be consistent with the focus point
scenario for the non-universal gaugino masses where the required mu-term is in
the hundreds GeV range.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:18:14 GMT""}]","2015-06-15"
"1303.6996","Victor Acosta","Victor M. Acosta, Kasper Jensen, Charles Santori, Dmitry Budker, and
  Rymond G. Beausoleil","Electromagnetically-induced transparency in a diamond spin ensemble
  enables all-optical electromagnetic field sensing","12 pages, 12 figures","Phys. Rev. Lett. 110, 213605 (2013)","10.1103/PhysRevLett.110.213605",,"physics.optics cond-mat.mtrl-sci physics.atom-ph physics.ins-det quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We use electromagnetically-induced transparency (EIT) to probe the narrow
electron-spin resonance of nitrogen-vacancy centers in diamond. Working with a
multi-pass diamond chip at temperatures 6-30 K, the zero-phonon absorption line
(637 nm) exhibits an optical depth of 6 and inhomogenous linewidth of ~30 GHz
full-width-at-half-maximum (FWHM). Simultaneous optical excitation at two
frequencies separated by the ground-state zero-field splitting (2.88 GHz),
reveals EIT resonances with a contrast exceeding 6% and FWHM down to 0.4 MHz.
The resonances provide an all-optical probe of external electric and magnetic
fields with a projected photon-shot-noise-limited sensitivity of 0.2
V/cm/sqrt(Hz) and 0.1 nT/sqrt(Hz), respectively. Operation of a prototype
diamond-EIT magnetometer measures a noise floor of less than 1 nT/sqrt(Hz) for
frequencies above 10 Hz and Allan deviation of 1.3 +/- 1.1 nT for 100 s
intervals. The results demonstrate the potential of diamond-EIT devices for
applications ranging from quantum-optical memory to few-photon nonlinear
optics, precision measurement, and tests of fundamental physics.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:32:19 GMT""},{""version"":""v2"",""created"":""Fri, 24 May 2013 20:44:21 GMT""}]","2015-06-15"
"1303.6997","Yuancheng Fan","Yuancheng Fan, Jin Han, Hongqiang Li","Gaussian Beam Collimation via Transformation Media","6 pages, 3 figures",,,,"physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Conventional method for Gaussian beam collimation is to expand the beam waist
to achieve a smaller divergence angle. Recent transformation optics offers an
unconventional path to control the electromagnetic field. In this paper we show
that a Gaussian beam can be converted into a plane wave-like beam by
transformation media within the scale of several wavelengths. Design details
and full-wave simulation results via finite element method are provided.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:40:17 GMT""}]","2013-03-29"
"1303.6998","Paolo Stellari","Mart\'i Lahoz, Emanuele Macr\`i, Paolo Stellari","ACM bundles on cubic fourfolds containing a plane","18 pages. This is the last section of the previous version of
  arXiv:1303.6998 which is now spit into two parts. Explanations added","In: Brauer groups and obstruction problems: Moduli spaces and
  arithmetic, 155-175, Progr. Math. 320, Birkhauser/Springer (2017)",,,"math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study ACM bundles on cubic fourfolds containing a plane exploiting the
geometry of the associated quadric fibration and Kuznetsov's treatment of their
bounded derived categories of coherent sheaves. More precisely, we recover the
K3 surface naturally associated to the fourfold as a moduli space of Gieseker
stable ACM bundles of rank four.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:47:52 GMT""},{""version"":""v2"",""created"":""Sat, 15 Feb 2014 19:32:28 GMT""},{""version"":""v3"",""created"":""Sun, 8 Feb 2015 14:51:05 GMT""},{""version"":""v4"",""created"":""Mon, 20 Nov 2017 20:42:22 GMT""}]","2017-11-22"
"1303.6999","Bertrand Cloez","Bertrand Cloez, Martin Hairer","Exponential ergodicity for Markov processes with random switching","Published at http://dx.doi.org/10.3150/13-BEJ577 in the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)","Bernoulli 2015, Vol. 21, No. 1, 505-536","10.3150/13-BEJ577","IMS-BEJ-BEJ577","math.PR math.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study a Markov process with two components: the first component evolves
according to one of finitely many underlying Markovian dynamics, with a choice
of dynamics that changes at the jump times of the second component. The second
component is discrete and its jump rates may depend on the position of the
whole process. Under regularity assumptions on the jump rates and Wasserstein
contraction conditions for the underlying dynamics, we provide a concrete
criterion for the convergence to equilibrium in terms of Wasserstein distance.
The proof is based on a coupling argument and a weak form of the Harris
theorem. In particular, we obtain exponential ergodicity in situations which do
not verify any hypoellipticity assumption, but are not uniformly contracting
either. We also obtain a bound in total variation distance under a suitable
regularising assumption. Some examples are given to illustrate our result,
including a class of piecewise deterministic Markov processes.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:52:35 GMT""},{""version"":""v2"",""created"":""Mon, 13 Apr 2015 12:33:30 GMT""}]","2015-04-14"
"1303.7000","Hua Sun","Hua Sun and Syed A. Jafar","Index Coding Capacity: How far can one go with only Shannon
  Inequalities?",,,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An interference alignment perspective is used to identify the simplest
instances (minimum possible number of edges in the alignment graph, no more
than 2 interfering messages at any destination) of index coding problems where
non-Shannon information inequalities are necessary for capacity
characterization. In particular, this includes the first known example of a
multiple unicast (one destination per message) index coding problem where
non-Shannon information inequalities are shown to be necessary. The simplest
multiple unicast example has 7 edges in the alignment graph and 11 messages.
The simplest multiple groupcast (multiple destinations per message) example has
6 edges in the alignment graph, 6 messages, and 10 receivers. For both the
simplest multiple unicast and multiple groupcast instances, the best outer
bound based on only Shannon inequalities is $\frac{2}{5}$, which is tightened
to $\frac{11}{28}$ by the use of the Zhang-Yeung non-Shannon type information
inequality, and the linear capacity is shown to be $\frac{5}{13}$ using the
Ingleton inequality. Conversely, identifying the minimal challenging aspects of
the index coding problem allows an expansion of the class of solved index
coding problems up to (but not including) these instances.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:53:13 GMT""}]","2013-03-29"
"1303.7001","Alfio Bonanno","Alfio Bonanno, Vadim Urpin","Rotational suppression of the Tayler instability in stellar radiation
  zones","7 pages, 5 figures, to appear on MNRAS. arXiv admin note: text
  overlap with arXiv:1302.2523",,"10.1093/mnras/stt451",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The study of the magnetic field in stellar radiation zones is an important
topic in modern astrophysics because the magnetic field can play an important
role in several transport phenomena such as mixing and angular momentum
transport. We consider the influence of rotation on stability of a
predominantly toroidal magnetic field in the radiation zone. We find that the
effect of rotation on the stability depends on the magnetic configuration of
the basic state. If the toroidal field increases sufficiently rapidly with the
spherical radius, the instability cannot be suppressed entirely even by a very
fast rotation although the strength of the instability can be significantly
reduced. On the other hand, if the field increases slowly enough with the
radius or decreases, the instability has a threshold and can be completely
suppressed in rapidly rotating stars. We find that in the regions where the
instability is entirely suppressed a particular type of magnetohydrodynamic
waves may exist which are marginally stable.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:56:47 GMT""}]","2015-06-15"
"1303.7002","Giovanni Montana","Christopher Minas, Edward Curry and Giovanni Montana","A Distance-Based Test of Association Between Paired Heterogeneous
  Genomic Data",,,,,"stat.ME stat.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Due to rapid technological advances, a wide range of different measurements
can be obtained from a given biological sample including single nucleotide
polymorphisms, copy number variation, gene expression levels, DNA methylation
and proteomic profiles. Each of these distinct measurements provides the means
to characterize a certain aspect of biological diversity, and a fundamental
problem of broad interest concerns the discovery of shared patterns of
variation across different data types. Such data types are heterogeneous in the
sense that they represent measurements taken at very different scales or
described by very different data structures. We propose a distance-based
statistical test, the generalized RV (GRV) test, to assess whether there is a
common and non-random pattern of variability between paired biological
measurements obtained from the same random sample. The measurements enter the
test through distance measures which can be chosen to capture particular
aspects of the data. An approximate null distribution is proposed to compute
p-values in closed-form and without the need to perform costly Monte Carlo
permutation procedures. Compared to the classical Mantel test for association
between distance matrices, the GRV test has been found to be more powerful in a
number of simulation settings. We also report on an application of the GRV test
to detect biological pathways in which genetic variability is associated to
variation in gene expression levels in ovarian cancer samples, and present
results obtained from two independent cohorts.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:00:49 GMT""}]","2013-03-29"
"1303.7003","Mario Silveirinha G.","Mario Silveirinha","Effective Medium Response of Metallic Nanowire Arrays with a Kerr-type
  Dielectric Host","43 pages in press (to appear in Phys. Rev. B)","Phys. Rev. B, 87, 165127, 2013","10.1103/PhysRevB.87.165127",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We derive an effective medium model to characterize the macroscopic
electromagnetic response of metallic nanowire arrays embedded in a host
dielectric with a Kerr-type nonlinear permittivity function. It is shown that
the macroscopic electromagnetic fields are coupled to the conduction current in
the nanowires and to an additional quasi-static potential through a system of
nonlinear equations. We prove that a weak nonlinearity leads to an
electromagnetic response closer to that of an indefinite medium, and to
isofrequency contours with increased hyperbolicity. For high-field intensities
the negative refraction of electromagnetic waves at an air nanowire material
interface is enhanced when the nanowires are embedded in a self-focusing Kerr
medium.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:06:46 GMT""},{""version"":""v2"",""created"":""Sat, 30 Mar 2013 14:01:31 GMT""}]","2015-06-15"
"1303.7004","Wayne Hu","Peter Adshead, Wayne Hu, and V Miranda","Bispectrum in Single-Field Inflation Beyond Slow-Roll","20 pages, 6 figures (typo in A22 corrected)","Phys. Rev. D88 023507 (2013)","10.1103/PhysRevD.88.023507",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We develop an integral form for the bispectrum in general single-field
inflation whose domain of validity includes models of inflation where the
background evolution is not constrained to be slowly varying everywhere. Our
integral form preserves the squeezed-limit consistency relation, allows for
fast evaluation of the bispectrum for all triangle configurations expediting
the efficient comparison of slow-roll violating models with data, and provides
complete and compact slow-roll expressions correct to first order in slow-roll
parameters. Motivated by the recent Planck results, we consider as an example a
sharp step in the warped-brane tension of DBI inflation and provide analytic
solutions for the peak of the resulting bispectrum. For the step in the warp
that reproduces the oscillations in the power spectrum favored by the Planck
data, the corresponding equilateral bispectrum is both extremely large and
highly scale dependent. The bispectrum serves as a means of distinguishing such
a model from alternative scenarios that generate otherwise indistinguishable
power spectra, such as a step in the potential in canonical single-field
inflation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:10:09 GMT""},{""version"":""v2"",""created"":""Fri, 20 Apr 2018 19:03:29 GMT""}]","2018-09-18"
"1303.7005","Abner Salgado","Ricardo H. Nochetto, Abner J. Salgado and Ignacio Tomas","The micropolar Navier-Stokes equations: A priori error analysis",,,,,"math.NA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The unsteady Micropolar Navier-Stokes Equations (MNSE) are a system of
parabolic partial differential equations coupling linear velocity and pressure
with angular velocity: material particles have both translational and
rotational degrees of freedom. We propose and analyze a first order
semi-implicit fully-discrete scheme for the MNSE, which decouples the
computation of the linear and angular velocities, is unconditionally stable and
delivers optimal convergence rates under assumptions analogous to those used
for the Navier-Stokes equations. With the help of our scheme we explore some
qualitative properties of the MNSE related to ferrofluid manipulation and
pumping. Finally, we propose a second order scheme and show that it is almost
unconditionally stable.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:19:43 GMT""}]","2013-03-29"
"1303.7006","Bhavin Khatri","Bhavin S. Khatri and Richard A. Goldstein","Evolutionary stochastic dynamics of speciation and a simple
  genotype-phenotype map for protein binding DNA","5 pages, 2 figures","Journal of Theoretical Biology, 378 (2015), p56-64","10.1016/j.jtbi.2015.04.027",,"q-bio.PE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Speciation is of fundamental importance to understanding the huge diversity
of life on Earth. In contrast to current phenomenological models, we develop a
biophysically motivated approach to study speciation involving the co-evolution
of protein binding DNA for two geographically isolated populations. Our results
predict that, despite neutral diffusion of hybrids in trait space, smaller
populations have a higher rate of speciation, due to sequence entropy poising
populations more closely to incompatible regions of phenotype space. A key
lesson of this work is that non-trivial contributions of sequence entropy give
rise to a strong population size dependence on speciation rates.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:32:59 GMT""},{""version"":""v2"",""created"":""Sun, 12 May 2013 10:27:34 GMT""}]","2015-05-18"
"1303.7007","Astrakharchik Grigori E","G.E. Astrakharchik and I. Brouzos","Trapped one-dimensional ideal Fermi gas with a single impurity","5 pages, 4 figures","Phys. Rev. A 88, 021602(R) (2013)","10.1103/PhysRevA.88.021602",,"cond-mat.quant-gas","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Properties of a single impurity in a one-dimensional Fermi gas are
investigated in homogeneous and trapped geometries. In a homogeneous system we
use McGuire's expression [J. B. McGuire, J. Math. Phys. 6, 432 (1965)] to
obtain interaction and kinetic energies, as well as the local pair correlation
function. The energy of a trapped system is obtained (i) by generalizing
McGuire expression (ii) within local density approximation (iii) using
perturbative approach in the case of a weakly interacting impurity and (iv)
diffusion Monte Carlo method. We demonstrate that a closed formula based on the
exact solution of the homogeneous case provides a precise estimation for the
energy of a trapped system for arbitrary coupling constant of the impurity even
for a small number of fermions. We analyze energy contributions from kinetic,
interaction and potential components, as well as spatial properties such as the
system size. Finally, we calculate the frequency of the breathing mode. Our
analysis is directly connected and applicable to the recent experiments in
microtraps.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:42:56 GMT""}]","2014-03-14"
"1303.7008","Fukano Hidenori S","Hidenori S. Fukano and Kimmo Tuominen","Top-seesaw assisted technicolor model with 126 GeV Higgs boson","6 pages, 4figures, contribution to SCGT12 ""KMI-GCOE Workshop on
  Strong Coupling Gauge Theories in the LHC Perspective"", 4-7 Dec. 2012, Nagoya
  University",,"10.1142/9789814566254_0027",,"hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We discuss a model which involves the top quark condensation and the walking
technicolor. We focus on the scalar boson in such a model from the viewpoint of
the observed scalar boson at the LHC.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:55:09 GMT""}]","2017-08-23"
"1303.7009","Benjamin Nachman","Benjamin Nachman and Christopher G. Lester","Significance Variables","21 pages, 7 figures","Phys. Rev. D 88, 075013 (2013)","10.1103/PhysRevD.88.075013",,"hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Many particle physics analyses which need to discriminate some background
process from a signal ignore event-by-event resolutions of kinematic variables.
Adding this information, as is done for missing momentum significance, can only
improve the power of existing techniques. We therefore propose the use of
significance variables which combine kinematic information with event-by-event
resolutions. We begin by giving some explicit examples of constructing optimal
significance variables. Then, we consider three applications: new heavy gauge
bosons, Higgs to $\tau\tau$, and direct stop squark pair production. We find
that significance variables can provide additional discriminating power over
the original kinematic variables: $\sim$ 20% improvement over $m_T$ in the case
of $H\rightarrow\tau\tau$ case, and $\sim$ 30% impovement over $m_{T2}$ in the
case of the direct stop search.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 23:57:11 GMT""}]","2013-10-30"
"1303.7010","Mikhail Shifman","M. Shifman, A. Yung","Abrikosov-Nielsen-Olesen string with Non-Abelian Moduli and ""Spin-Orbit""
  Interaction","5 pages, no figures; v.2 One reference added, one footnote added.
  Final version to appear in Phys. Rev. Letters; v3. Corrections in
  proofreading incorporated",,"10.1103/PhysRevLett.110.201602","FTPI-MINN-13/11, UMN-TH-3143/13","hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is generally believed that the spontaneous breaking of the Poincar\'e
group by flux tubes (strings) generate only two zero modes localized on the
string and associated with the spontaneous breaking of translational invariance
(the so-called Low-Manohar argument). Being perfectly true in many instances it
is nevertheless nonuniversal, and have to be amended in the case of order
parameters carrying spatial indices. We show that under certain circumstances
additional zero (or quasizero) modes can appear due to spin symmetry.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 00:00:53 GMT""},{""version"":""v2"",""created"":""Tue, 30 Apr 2013 19:58:08 GMT""},{""version"":""v3"",""created"":""Wed, 8 May 2013 16:56:01 GMT""}]","2013-05-22"
"1303.7011","Niels Gronbech Jensen","Niels Gr{\o}nbech-Jensen, Natha Robert Hayre, Oded Farago","Application of the G-JF Discrete-Time Thermostat for Fast and Accurate
  Molecular Simulations","Five pages, one figure. Version accepted for publication. Previous
  title: A new Langevin thermostat for fast and accurate molecular simulations","Computer Physics Communications Vol.185, p.524 (2014)","10.1016/j.cpc.2013.10.006",,"cond-mat.mtrl-sci cond-mat.soft physics.comp-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A new Langevin-Verlet thermostat that preserves the fluctuation-dissipation
relationship for discrete time steps, is applied to molecular modeling and
tested against several popular suites (AMBER, GROMACS, LAMMPS) using a small
molecule as an example that can be easily simulated by all three packages.
Contrary to existing methods, the new thermostat exhibits no detectable changes
in the sampling statistics as the time step is varied in the entire numerical
stability range. The simple form of the method, which we express in the three
common forms (Velocity-Explicit, Stormer-Verlet, and Leap-Frog), allows for
easy implementation within existing molecular simulation packages to achieve
faster and more accurate results with no cost in either computing time or
programming complexity.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 00:10:18 GMT""},{""version"":""v2"",""created"":""Tue, 21 May 2013 03:16:29 GMT""},{""version"":""v3"",""created"":""Tue, 22 Oct 2013 21:23:21 GMT""}]","2013-12-17"
"1303.7012","Abedelaziz  Mohaisen","Abedelaziz Mohaisen and Omar Alrawi","Unveiling Zeus","Accepted to SIMPLEX 2013 (a workshop held in conjunction with WWW
  2013)",,,,"cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Malware family classification is an age old problem that many Anti-Virus (AV)
companies have tackled. There are two common techniques used for
classification, signature based and behavior based. Signature based
classification uses a common sequence of bytes that appears in the binary code
to identify and detect a family of malware. Behavior based classification uses
artifacts created by malware during execution for identification. In this paper
we report on a unique dataset we obtained from our operations and classified
using several machine learning techniques using the behavior-based approach.
Our main class of malware we are interested in classifying is the popular Zeus
malware. For its classification we identify 65 features that are unique and
robust for identifying malware families. We show that artifacts like file
system, registry, and network features can be used to identify distinct malware
families with high accuracy---in some cases as high as 95%.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 00:11:54 GMT""}]","2013-03-29"
"1303.7013","C. M. Chandrashekar","C. M. Chandrashekar and Th. Busch","Quantum percolation and transition point of a directed discrete-time
  quantum walk","17 pages, 9 figures ; Published version","Scientific Reports 4, 6583 (2014)","10.1038/srep06583",,"quant-ph cond-mat.dis-nn cond-mat.other cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Quantum percolation describes the problem of a quantum particle moving
through a disordered system. While certain similarities to classical
percolation exist, the quantum case has additional complexity due to the
possibility of Anderson localisation. Here, we consider a directed
discrete-time quantum walk as a model to study quantum percolation of a
two-state particle on a two-dimensional lattice. Using numerical analysis we
determine the fraction of connected edges required (transition point) in the
lattice for the two-state particle to percolate with finite (non-zero)
probability for three fundamental lattice geometries, finite square lattice,
honeycomb lattice, and nanotube structure and show that it tends towards unity
for increasing lattice sizes. To support the numerical results we also use a
continuum approximation to analytically derive the expression for the
percolation probability for the case of the square lattice and show that it
agrees with the numerically obtained results for the discrete case. Beyond the
fundamental interest to understand the dynamics of a two-state particle on a
lattice (network) with disconnected vertices, our study has the potential to
shed light on the transport dynamics in various quantum condensed matter
systems and the construction of quantum information processing and
communication protocols.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 00:17:38 GMT""},{""version"":""v2"",""created"":""Mon, 2 Dec 2013 23:48:41 GMT""},{""version"":""v3"",""created"":""Thu, 2 Oct 2014 01:01:11 GMT""}]","2014-10-03"
"1303.7014","Junhua Zhang","Junhua Zhang, Tianqi Li, Jigang Wang, Joerg Schmalian","Post-transient relaxation in graphene after an intense laser pulse","10 pages, 4 figures, submitted as contribution of the IMPACT Special
  Topics series of the EPJ",,"10.1140/epjst/e2013-01920-2",,"cond-mat.mes-hall cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  High intensity laser pulses were recently shown to induce a population
inverted transient state in graphene [T. Li et al. Phys. Rev. Lett. 108, 167401
(2012)]. Using a combination of hydrodynamic arguments and a kinetic theory we
determine the post-transient state relaxation of hot, dense, population
inverted electrons towards equilibrium. The cooling rate and charge-imbalance
relaxation rate are determined from the Boltzmann-equation including
electron-phonon scattering. We show that the relaxation of the population
inversion, driven by inter-band scattering processes, is much slower than the
relaxation of the electron temperature, which is determined by intra-band
scattering processes. This insight may be of relevance for the application of
graphene as an optical gain medium.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 00:44:41 GMT""}]","2015-06-15"
"1303.7015","Xiaojun Zhou","Xiaojun Zhou","A Multiobjective State Transition Algorithm for Single Machine
  Scheduling","10 pages, 4 figures","Advances in Global Optimization, 2015, 95: 79-88","10.1007/978-3-319-08377-3_9",,"math.OC cs.IT math.CO math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, a discrete state transition algorithm is introduced to solve a
multiobjective single machine job shop scheduling problem. In the proposed
approach, a non-dominated sort technique is used to select the best from a
candidate state set, and a Pareto archived strategy is adopted to keep all the
non-dominated solutions. Compared with the enumeration and other heuristics,
experimental results have demonstrated the effectiveness of the multiobjective
state transition algorithm.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 00:47:17 GMT""}]","2015-09-22"
"1303.7016","Deepali Lodhia","Deepali Lodhia, Daniel Brown, Frank Brueckner, Ludovico Carbone, Paul
  Fulda, Keiko Kokeyama and Andreas Freise","Phase effects due to beam misalignment on diffraction gratings","14 pages, 8 figures, submitted to Optics Express",,"10.1364/OE.21.029578",,"physics.optics gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  All-reflective interferometer configurations have been proposed for the next
generation of gravitational wave detectors, with diffractive elements replacing
transmissive optics. However, an additional phase noise creates more stringent
conditions for alignment stability. A framework for alignment stability with
the use of diffractive elements was required using a Gaussian model. We
successfully create such a framework involving modal decomposition to replicate
small displacements of the beam (or grating) and show that the modal model does
not contain the phase changes seen in an otherwise geometric planewave
approach. The modal decomposition description is justified by verifying
experimentally that the phase of a diffracted Gaussian beam is independent of
the beam shape, achieved by comparing the phase change between a zero-order and
first-order mode beam. To interpret our findings we employ a rigorous
time-domain simulation to demonstrate that the phase changes resulting from a
modal decomposition are correct, provided that the coordinate system which
measures the phase is moved simultaneously with the effective beam
displacement. This indeed corresponds to the phase change observed in the
geometric planewave model. The change in the coordinate system does not
instinctively occur within the analytical framework, and therefore requires
either a manual change in the coordinate system or an addition of the geometric
planewave phase factor.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 01:04:52 GMT""}]","2017-07-26"
"1303.7017","Guang-Yu Guo","Guang-Yu Guo, Vasily Klimov, Shulin Sun, and Wei-Jin Zheng","Metamaterial slab-based super-absorbers and perfect nanodetectors for
  single dipole sources","Accepted for publication in Optics Express","Optics Express 21, 11348 (2013)","10.1364/OE.21.011338",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose to use double negative (DNG) metamaterial slabs to build effective
super-absorbers and perfect nanodetectors for single divergent sources. We
demonstrate by numerical simulations that an absorbing nanoparticle properly
placed inside a DNG slab back-covered with a perfect electric conductor or
perfect magnetic conductor mirror can absorb up to 100% radiation energy of a
single dipole source placed outside the slab. Furthermore, we also show that
even the simple DNG slab without any absorbing nanoparticle could be used as a
perfect absorber for both plane and divergent beams. The proposed systems may
focus the radiation in nanoscale and thus have applications in optical
nanodevices for a variety of different purposes.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 01:13:25 GMT""}]","2013-05-03"
"1303.7018","John Luecke","Ken Baker, Cameron Gordon, and John Luecke","Bridge number and integral Dehn surgery",,"Algebr. Geom. Topol. 16 (2016) 1-40","10.2140/agt.2016.16.1",,"math.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In a 3-manifold M, let K be a knot and R be an annulus which meets K
transversely. We define the notion of the pair (R,K) being caught by a surface
Q in the exterior of the link given by K and the boundary curves of R. For a
caught pair (R,K), we consider the knot K^n gotten by twisting K n times along
R and give a lower bound on the bridge number of K^n with respect to Heegaard
splittings of M -- as a function of n, the genus of the splitting, and the
catching surface Q. As a result, the bridge number of K^n tends to infinity
with n. In application, we look at a family of knots K^n found by Teragaito
that live in a small Seifert fiber space M and where each K^n admits a Dehn
surgery giving the 3-sphere. We show that the bridge number of K^n with respect
to any genus 2 Heegaard splitting of M tends to infinity with n. This contrasts
with other work of the authors as well as with the conjectured picture for
knots in lens spaces that admit Dehn surgeries giving the 3-sphere.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 01:23:15 GMT""}]","2016-03-09"
"1303.7019","Kicheon Kang","Kicheon Kang","Local Geometric Phase and Quantum State Tomography in a Superconducting
  Qubit","5 pages, 1 figure",,"10.3938/jkps.64.567",,"cond-mat.mes-hall quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate quantum state reconstruction of a superconducting qubit
threaded by an Aharonov-Bohm flux, with particular attention to the local
geometric phase. A state reconstruction scheme is introduced with a proper
account of the local geometric phase generated by Faraday's law of induction.
Our scheme is based on measurement of three complementary quantities, that is,
the extra charge and two local currents. Incorporating time-reversal symmetry
and the Faraday's law, we show that the full density matrix can be
reconstructed without ambiguity in the choice of gauge. This procedure clearly
demonstrates that the quantum Faraday effect plays an essential role in the
dynamics of a quantum system that involves Aharonov-Bohm flux.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 01:31:02 GMT""}]","2015-06-15"
"1303.7020","Markus Grassl","Salman Beigi, Jianxin Chen, Markus Grassl, Zhengfeng Ji, Qiang Wang
  and Bei Zeng","Symmetries of Codeword Stabilized Quantum Codes","15 pages, 1 figure. Accepted by TQC 2013. Version 2: Funding
  information added; typos corrected",,"10.4230/LIPIcs.TQC.2013.192",,"quant-ph cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Symmetry is at the heart of coding theory. Codes with symmetry, especially
cyclic codes, play an essential role in both theory and practical applications
of classical error-correcting codes. Here we examine symmetry properties for
codeword stabilized (CWS) quantum codes, which is the most general framework
for constructing quantum error-correcting codes known to date. A CWS code Q can
be represented by a self-dual additive code S and a classical code C, i.,e.,
Q=(S,C), however this representation is in general not unique. We show that for
any CWS code Q with certain permutation symmetry, one can always find a
self-dual additive code S with the same permutation symmetry as Q such that
Q=(S,C). As many good CWS codes have been found by starting from a chosen S,
this ensures that when trying to find CWS codes with certain permutation
symmetry, the choice of S with the same symmetry will suffice. A key step for
this result is a new canonical representation for CWS codes, which is given in
terms of a unique decomposition as union stabilizer codes. For CWS codes, so
far mainly the standard form (G,C) has been considered, where G is a graph
state. We analyze the symmetry of the corresponding graph of G, which in
general cannot possess the same permutation symmetry as Q. We show that it is
indeed the case for the toric code on a square lattice with translational
symmetry, even if its encoding graph can be chosen to be translational
invariant.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 01:48:26 GMT""},{""version"":""v2"",""created"":""Mon, 8 Apr 2013 13:17:24 GMT""}]","2013-12-30"
"1303.7021","Guoliang Lv","Guoliang Lu, Chunhua Zhu, Philipp Podsiadlowski","Dust Formation in the Ejecta of Common Envelope Systems","11pages, 7 figures, accepted for publication by ApJ",,"10.1088/0004-637X/768/2/193",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The material that is ejected in a common-envelope (CE) phase in a close
binary system provides an ideal environment for dust formation. By constructing
a simple toy model to describe the evolution of the density and the temperature
of CE ejecta and using the \emph{AGBDUST} code to model dust formation, we show
that dust can form efficiently in this environment. The actual dust masses
produced in the CE ejecta depend strongly on their temperature and density
evolution. We estimate the total dust masses produced by CE evolution by means
of a population synthesis code and show that, compared to dust production in
AGB stars, the dust produced in CE ejecta may be quite significant and could
even dominate under certain circumstances.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 02:04:15 GMT""}]","2015-06-15"
"1303.7022","Seung Hwan Hong","Seung Hwan Hong and Han-Yong Choi","Angle and frequency dependence of self-energy from spin fluctuations
  mediated d-wave pairing for high temperature superconductors","15 pages, 13 figures, Submitted to: J. Phys.: Condens. Matter",,"10.1088/0953-8984/25/36/365702",,"cond-mat.supr-con","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigated the characteristics of the spin fluctuations mediated
superconductivity employing the Eliashberg formalism. The effective interaction
between electrons was modeled in terms of the spin susceptibility measured by
the inelastic neutron scattering experiments on single crystal La2-xSrxCuO4
superconductors. The diagonal self-energy and off-diagonal self-energy were
calculated by solving the coupled Eliashberg equation self-consistently for
chosen spin susceptibility and tight-binding dispersion of electrons. The full
momentum and frequency dependence of the self-energy is presented for the
optimal, overdoped, and underdoped LSCO cuprates in superconductive state.
These results may be compared with the experimentally deduced self-energy from
ARPES experiments.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 02:15:07 GMT""}]","2015-06-15"
"1303.7023","Tomohiro Matsuda","Seishi Enomoto, Tomohiro Matsuda","Curvaton mechanism after multi-field inflation","10 pages, 2 figures, accepted for publication in PRD",,"10.1103/PhysRevD.87.083513",,"hep-ph astro-ph.CO hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The evolution of the curvature perturbation after multi-field inflation is
studied in the light of the curvaton mechanism. Past numerical studies show
that many-field inflation causes significant evolution of the curvature
perturbation after inflation, which generates significant non-Gaussianity at
the same time. We reveal the underlying mechanism of the evolution and show
that the evolution is possible in a typical two-field inflation model.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 02:28:48 GMT""}]","2015-06-15"
"1303.7024","Lei Zhang","Lei Zhang","$Sp_{2n}(F_{q^{2}})$-Invariants In Irreducible Unipotent Representations
  of $Sp_{4n}(F_{q})$","29 pages",,,,"math.RT math.NT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that for any irreducible representation of $Sp_{4n}(F_{q})$, the
subspace of all its $Sp_{2n}(F_{q^{2}})$-invariants is at most one-dimensional.
In terms of Lusztig symbols, we give a complete list of irreducible unipotent
representations of $Sp_{4n}(F_{q})$ which have a nonzero
$Sp_{2n}(F_{q^{2}})$-invariant and, in particular, we prove that every
irreducible unipotent cuspidal representation has a one-dimensional subspace of
$Sp_{2n}(F_{q^{2}})$-invariants. As an application, we give an elementary proof
of the fact that the unipotent cuspidal representation is defined over $Q$,
which was proved by Lusztig.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 02:43:40 GMT""}]","2013-03-29"
"1303.7025","Paco Talero Lopez","Paco Talero, Orlando Organista and Luis H. Barbosa","Velocities: mean, average and instantaneous in uniform accelerated
  motion, some pedagogical comments","4 pages,2 figures. Submitted to Revista Brasileira de Ensino de
  Fisica, in Portuguese",,,,"physics.ed-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The relation among instantaneous, mean and average velocities in
one-dimensional motion with constant acceleration is studied. It was shown that
the instant velocity evaluated in the time $t_{p}=\left(t_2+t_1\right)/2$ is
similar to the mean and average velocities evaluated between the times $t_1$
and $t_2$. The reason for relations illustrated before were shown in detail.
Also, the results obtained were used to propose a pedagogical strategy in order
to study the one-dimensional motion with constant acceleration as a natural
extension of one-dimensional motion with constant velocity.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 02:47:07 GMT""}]","2013-03-29"
"1303.7026","Farzad Hessar","Farzad Hessar, and Sumit Roy","Minimum Energy Source Coding for Asymmetric Modulation with Application
  to RFID",,,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Minimum energy (ME) source coding is an effective technique for efficient
communication with energy-constrained devices, such as sensor network nodes. In
this paper, the principles of generalized ME source coding is developed that is
broadly applicable. Two scenarios - fixed and variable length codewords - are
analyzed. The application of this technique to RFID systems where ME source
coding is particularly advantageous due to the asymmetric nature of data
communications is demonstrated, a first to the best of our knowledge.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:00:02 GMT""}]","2013-03-29"
"1303.7027","Hiroki Sako","Hiroki Sako","Property A for coarse spaces","11 pages. We give proofs for basic facts, which have been omitted in
  arXiv:1212.5900",,,,"math.MG math.OA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Property A introduced by Guoliang Yu is an amenability-type property for
metric spaces. In this article, we study property A for uniformly locally
finite coarse spaces. Main examples of coarse spaces are a metric space, a set
equipped with a discrete group action, and a sequence of finitely generated
groups. The purpose of this article is to give complete proofs to related basic
facts.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:11:47 GMT""}]","2013-03-29"
"1303.7028","Gabriel Menezes","G. Menezes, B. F. Svaiter and N. F. Svaiter","Riemann zeta zeros and prime number spectra in quantum field theory","Revised version, 18 pages","Int. J. Mod. Phys. A 28, 1350128 (2013)","10.1142/S0217751X13501285",,"math-ph hep-th math.MP math.NT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Riemann hypothesis states that all nontrivial zeros of the zeta function
lie in the critical line $\Re(s)=1/2$. Hilbert and P\'olya suggested that one
possible way to prove the Riemann hypothesis is to interpret the nontrivial
zeros in the light of spectral theory. Following this approach, we discuss a
necessary condition that such a sequence of numbers should obey in order to be
associated with the spectrum of a linear differential operator of a system with
countably infinite number of degrees of freedom described by quantum field
theory. The sequence of nontrivial zeros is zeta regularizable. Then,
functional integrals associated with hypothetical systems described by
self-adjoint operators whose spectra is given by this sequence can be
constructed. However, if one considers the same situation with primes numbers,
the associated functional integral cannot be constructed, due to the fact that
the sequence of prime numbers is not zeta regularizable. Finally, we extend
this result to sequences whose asymptotic distributions are not ""far away"" from
the asymptotic distribution of prime numbers.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:16:10 GMT""},{""version"":""v2"",""created"":""Fri, 2 Aug 2013 20:27:37 GMT""}]","2014-01-29"
"1303.7029","Chol-Rim Min Mr","Kang-Il Ri, Yun-Ho An and Chang-Il Rim","The Relation Between Diagrams of a Knot and Its Unknotting Number","Withdrawn because the theorem 4 is not correct","International Symposium in Commemoration of the 65th Anniversary
  of the Foundation of Kim Il Sung University (Mathematics)20-21. Sep.
  Juche100(2011), Pyongyang DPR Korea, 79-83, 2012",,"KISU-MATH-2011-E-C-010","math.GT math.DG math.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The unknotting number is the classical invariant of a knot. However, its
determination is difficult in general. To obtain the unknotting number from
definition one has to investigate all possible diagrams of the knot. We tried
to show the unknotting number can be obtained from any one diagram of the knot.
To do this we tried to prove the unknotting number is not changed under
Riedemiester moves, but such a proposition is not correct. Reidemeister II move
can change unknotting number. See Nakanishi-Bleiler example. So this article is
withdrawn.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:28:28 GMT""},{""version"":""v2"",""created"":""Mon, 24 Jun 2013 11:10:18 GMT""}]","2013-06-25"
"1303.7030","Stefano Rini","Stefano Rini, Ernest Kurniawany, Levan Ghaghanidze, and Andrea
  Goldsmithy","Energy Efficient Cooperative Strategies for Relay-Assisted Downlink
  Cellular Systems, Part I: Theoretical Framework",,,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The impact of cognition on the energy efficiency of a downlink cellular
system in which multiple relays assist the transmission of the base station is
considered. The problem is motivated by the practical importance of
relay-assisted solutions in mobile networks, such as LTE-A, in which
cooperation among relays holds the promise of greatly improving the energy
efficiency of the system. We study the fundamental tradeoff between the power
consumption at the base station and the level of cooperation and cognition at
the relay nodes. By distributing the same message to multiple relays, the base
station consumes more power but it enables cooperation among the relays, thus
making the transmission between relays to destination a multiuser cognitive
channel. Cooperation among the relays allows for a reduction of the power used
to transmit from the relays to the end users due to interference management and
the coherent combining gains. These gain are present even in the case of
partial or unidirectional transmitter cooperation, which is the case in
cognitive channels such as the cognitive interference channel and the
interference channel with a cognitive relay. We therefore address the problem
of determining the optimal level of cooperation at the relays which results in
the smallest total power consumption when accounting for the power reduction
due to cognition. A practical design examples and numerical simulation are
presented in a companion paper (part II).
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:32:20 GMT""},{""version"":""v2"",""created"":""Fri, 29 Mar 2013 17:03:12 GMT""}]","2013-04-01"
"1303.7031","Seng Ghee Tan","Ji Chen, Mansoor Bin Abdul Jalil, Seng Ghee Tan","Spin Torque on Magnetic Textures Coupled to the Surface of a
  Three-Dimensional Topological Insulator","17 pages, 2 figures",,,,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate theoretically the spin torque and magnetization dynamic in a
thin ferromagnetic (FM) layer with spatially varying magnetization. The FM
layer is deposited on the surface of a topological insulator (TI). In the limit
of the adiabatic relaxation of electron spin along the magnetization, the
interaction between the exchange interaction and the Rashba-like surface
texture of a TI yields a topological gauge field. Under the gauge field and an
applied current, spin torque is induced according to the direction of the
current. We derived the corresponding effective anisotropy field and hence the
modified Landau-Lifshitz-Gilbert equation, which describes the spin torque and
the magnetization dynamic. In addition, we study the effective field for
exemplary magnetic textures, such as domain wall, skyrmion, and vortex
configurations. The estimated strength of the effective field is comparable to
the switching fields of typical FM materials, and hence can significantly
influence the dynamics of the FM layer.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:34:09 GMT""}]","2013-03-29"
"1303.7032","Zhe Yao","Zhe Yao, Vincent Gripon and Michael G. Rabbat","A Massively Parallel Associative Memory Based on Sparse Neural Networks",,,,,"cs.AI cs.DC cs.NE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Associative memories store content in such a way that the content can be
later retrieved by presenting the memory with a small portion of the content,
rather than presenting the memory with an address as in more traditional
memories. Associative memories are used as building blocks for algorithms
within database engines, anomaly detection systems, compression algorithms, and
face recognition systems. A classical example of an associative memory is the
Hopfield neural network. Recently, Gripon and Berrou have introduced an
alternative construction which builds on ideas from the theory of error
correcting codes and which greatly outperforms the Hopfield network in
capacity, diversity, and efficiency. In this paper we implement a variation of
the Gripon-Berrou associative memory on a general purpose graphical processing
unit (GPU). The work of Gripon and Berrou proposes two retrieval rules,
sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector
multiplication and is easily implemented on the GPU. The sum-of-max rule is
much less straightforward to implement because it involves non-linear
operations. However, the sum-of-max rule gives significantly better retrieval
error rates. We propose a hybrid rule tailored for implementation on a GPU
which achieves a 880-fold speedup without sacrificing any accuracy.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:49:57 GMT""},{""version"":""v2"",""created"":""Sun, 21 Jul 2013 14:29:21 GMT""}]","2013-07-23"
"1303.7033","Manfred Bucher","Manfred Bucher","Coulomb-oscillator explanation of striped STM images of superconductive
  copper oxides","8 pages, 1 figure",,,,"physics.gen-ph","http://creativecommons.org/licenses/by-nc-sa/3.0/","  Asymmetric scanning tunneling microscopy (STM) of the CuO2 plane of
Ca2-xNaxCuO2Cl2, x = 0.125, shows a square domain structure with edge length
four times the compound's lattice constant a0 (Cu-O-Cu distance). The domain
structure is a direct consequence of the 4a0 by 4a0 superlattice formed by
vertical Na+ pairs (oriented parallel to the crystal's c axis) that substitute
Ca2+ ions. The surrounding O2- ions are displaced away from, and the Cu2+ ions
toward the Na+ pairs. Contrary to the fourfold symmetry of the CuO2 plane, the
stable displacement configuration has a twofold symmetry, dominated by large
and, respectively, small displacement of opposite O2- ions being nearest
neighbors to each vertical Na+ pair. The ion displacements give rise to
sufficient squeeze of certain O2- ions that, by the Coulomb-oscillator model of
superconductivity, prevents lateral overswing of their excited 3s electrons.
The axial 3s oscillations are predominantly oriented in the directions of O2-
ion displacements. The observed ladder pattern in the domains provides a direct
imaging of the 3s Coulomb oscillators. The 'sidepieces' of the ladders
correspond to long unidirectional pathways for 3s electrons in the CuO2 plane.
They account for superconductivity. The findings lend support to the validity
of the Coulomb-oscillator model of superconductivity.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:03:30 GMT""}]","2013-03-29"
"1303.7034","Stefano Rini","Stefano Riniy, Ernest Kurniawan, Levan Ghaghanidze, and Andrea
  Goldsmith","Energy Efficient Cooperative Strategies for Relay-Assisted Downlink
  Cellular Systems Part II: Practical Design",,,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In a companion paper [1], we present a general approach to evaluate the
impact of cognition in a downlink cellular system in which multiple relays
assist the transmission of the base station. This approach is based on a novel
theoretical tool which produces transmission schemes involving rate-splitting,
superposition coding and interference decoding for a network with any number of
relays and receivers. This second part focuses on a practical design example
for a network in which a base station transmits to three receivers with the aid
of two relay nodes. For this simple network, we explicitly evaluate the impact
of relay cognition and precisely characterize the trade offs between the total
energy consumption and the rate improvements provided by relay cooperation.
These closedform expressions provide important insights on the role of
cognition in larger networks and highlights interesting interference management
strategies. We also present a numerical simulation setup in which we fully
automate the derivation of achievable rate region for a general relay-assisted
downlink cellular network. Our simulations clearly show the great advantages
provided by cooperative strategies at the relays as compared to the
uncoordinated scenario under varying channel conditions and target rates. These
results are obtained by considering a large number of transmission strategies
for different levels of relay cognition and numerically determining one that is
the most energy efficient. The limited computational complexity of the
numerical evaluations makes this approach suitable for the optimization of
transmission strategies for larger networks.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:09:12 GMT""}]","2013-03-29"
"1303.7035","We-Fu Chang","We-Fu Chang, Wei-Ping Pan, and Fanrong Xu","An effective gauge-Higgs operators analysis of new physics associated
  with the Higgs","32 pages, 11 figures; UV complete model section revised, typos
  corrected, and refernces added","Phys.Rev.D88,033004(2013)","10.1103/PhysRevD.88.033004",,"hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the new physics(NP) related to the recent discovered 125 GeV Higgs
by employing an important subset of the standard model(SM) gauge invariant
dimension-six operators constructed by the the SM Higgs and gauge fields.
Explicitly, we perform a model-independent study on the production and decays
of the Higgs, the electric dipole moments(EDM) of the neutron and the electron,
and we take into account the anomalous magnetic dipole moments of muon and
electron as well.
  We find that, even all Higgs decay channels agree with the SM predictions,
the SM theoretical uncertainties provide a lot of room to host NP associated
with the 125 GeV boson. A linear relation is revealed in our numerical study
that $\mu_{ZZ}\simeq \mu_{WW}$ and $ 0.6 \lesssim \mu_{ZZ,WW} \lesssim 1.4$ at
95% CL with or without the EDM's constraints. The neutron and electron EDM's
severely constrain the relevant Wilson coefficients. Therefore the CP violating
components in the $h\rightarrow WW, ZZ$ channels are too small, $\sim{\cal
O}(10^{-5})$, to be detected at LHC. However, we point out that even the parity
of the 125GeV boson has been largely determined to be even in the $h\to ZZ$
channel, one should pay special attention to the potentially large CP violation
in the $h\to \gamma\gamma$ and $h\to \gamma Z$ channels. This should be
seriously checked in the future spin correlation experiments.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:18:18 GMT""},{""version"":""v2"",""created"":""Wed, 14 Aug 2013 21:14:49 GMT""}]","2013-08-16"
"1303.7036","Dibyendu Roy","Dibyendu Roy, C. J. Bolech, and Nayana Shah","Nature of the zero-bias conductance peak associated with Majorana bound
  states in topological phases of semiconductor-superconductor hybrid
  structures","10 pages, 5 figures",,,,"cond-mat.supr-con cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Rashba spin-orbit coupled semiconductor-superconductor hybrid structures in
the presence of Zeeman splitting have emerged as the first experimentally
realizable topological superconductor supporting zero-energy Majorana bound
states. However, recent experimental studies in these hybrid structures are not
in complete agreement with the theoretical predictions, for example, the
observed height of the zero-bias conductance peak (ZBCP) associated with the
Majorana bound states is less than 10% of the predicted quantized value 2e^2/h.
We try to understand the sources of various discrepancies between the recent
experiments and the earlier theories by starting from a microscopic theory and
studying non-equilibrium transport in these systems at arbitrary temperatures
and applied bias voltages. Our approach involves quantum Langevin equations and
non-equilibrium Green's functions. Here we are able to model the tunnel
coupling between the one-dimensional semiconductor-superconductor hybrid
structure and the metallic leads realistically; study the role of tunnel
coupling on the height of the ZBCP and the subgap conductance; predict the
nature of the splitting of the ZBCP with an increasing magnetic field beyond
the critical field; show the behavior of the ZBCP with an increasing
gate-controlled onsite potential; and study the evolution of the full
differential conductance across the topological quantum phase transition. When
the applied magnetic field is quite large compared to the Rashba splitting and
the bulk energy gap is much reduced, we find the ZBCP even for an onsite
potential much larger than the applied magnetic field. The height of the
corresponding ZBCP depends on the tunnel coupling even at zero temperature and
can be much smaller than 2e^2/h.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:36:29 GMT""}]","2013-03-29"
"1303.7037","Jonathan Spreer","Benjamin A. Burton, Thomas Lewiner, Jo\~ao Paix\~ao, Jonathan Spreer","Parameterized Complexity of Discrete Morse Theory","To appear in Proceedings of the Twenty-Ninth Annual Symposium on
  Computational Geometry (SoCG). 25 pages, 8 figures, 2 tables","ACM Trans. Math. Softw., 42(1):24 pages, 2016","10.1145/2738034",,"cs.CG cs.CC math.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Optimal Morse matchings reveal essential structures of cell complexes which
lead to powerful tools to study discrete geometrical objects, in particular
discrete 3-manifolds. However, such matchings are known to be NP-hard to
compute on 3-manifolds, through a reduction to the erasability problem.
  Here, we refine the study of the complexity of problems related to discrete
Morse theory in terms of parameterized complexity. On the one hand we prove
that the erasability problem is W[P]-complete on the natural parameter. On the
other hand we propose an algorithm for computing optimal Morse matchings on
triangulations of 3-manifolds which is fixed-parameter tractable in the
treewidth of the bipartite graph representing the adjacency of the 1- and
2-simplexes. This algorithm also shows fixed parameter tractability for
problems such as erasability and maximum alternating cycle-free matching. We
further show that these results are also true when the treewidth of the dual
graph of the triangulated 3-manifold is bounded. Finally, we investigate the
respective treewidths of simplicial and generalized triangulations of
3-manifolds.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:49:25 GMT""}]","2018-10-24"
"1303.7038","Jorge Mastache","Jorge Mastache, Axel de la Macorra","Extra relativistic degrees of freedom without extra particles using
  Planck data","9 pages, 7 figures, 2 Tables",,"10.1103/PhysRevD.88.043506",,"gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A recent number of analysis of cosmological data have shown indications for
the presence of extra radiation beyond the standard model at equality and
nucleosynthesis epoch, which has been usually interpreted as an effective
number of neutrinos, Neff > 3.046. In this work we establish the theoretical
basis for a particle physics-motivated model (Bound Dark Matter, BDM) which
explain the need of extra radiation. The BDM model describes dark matter
particles which are relativistic at a scale below a < ac, these particles
acquire mass with an initial velocity, vc, at scales a > ac due to
non-perturbative methods, as protons and neutrons do, this process is described
by a time dependent equation of state, w_bdm(a). Owing to this behavior the
amount of extra radiation change as a function of the scale factor, this entail
that the extra relativistic degrees of freedom Nex may also vary as a function
of the scale factor. This is favored by data at CMB and BBN epochs. We compute
the range of values of the BDM model parameters, xc = ac*vc, that explain the
values obtained for the 4He at BBN and Neff at equality. Combining different
analysis we compute the value xc = 4.13x10^{-5} and vc = 0.37. We conclude that
we can account for the apparent extra neutrino degrees of freedom Nex using a
phase transition in the dark matter with a time dependent equation of state
with no need for introducing extra relativistic particles.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:53:44 GMT""},{""version"":""v2"",""created"":""Mon, 29 Apr 2013 20:16:29 GMT""}]","2013-08-14"
"1303.7039","Sarabjot Singh","Sarabjot Singh and Jeffrey G. Andrews","Joint Resource Partitioning and Offloading in Heterogeneous Cellular
  Networks",,,,,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In heterogeneous cellular networks (HCNs), it is desirable to offload mobile
users to small cells, which are typically significantly less congested than the
macrocells. To achieve sufficient load balancing, the offloaded users often
have much lower SINR than they would on the macrocell. This SINR degradation
can be partially alleviated through interference avoidance, for example time or
frequency resource partitioning, whereby the macrocell turns off in some
fraction of such resources. Naturally, the optimal offloading strategy is
tightly coupled with resource partitioning; the optimal amount of which in turn
depends on how many users have been offloaded. In this paper, we propose a
general and tractable framework for modeling and analyzing joint resource
partitioning and offloading in a two-tier cellular network. With it, we are
able to derive the downlink rate distribution over the entire network, and an
optimal strategy for joint resource partitioning and offloading. We show that
load balancing, by itself, is insufficient, and resource partitioning is
required in conjunction with offloading to improve the rate of cell edge users
in co-channel heterogeneous networks.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 04:54:35 GMT""},{""version"":""v2"",""created"":""Wed, 21 Aug 2013 03:07:31 GMT""}]","2013-08-22"
"1303.7040","E.K. Liu","X. M. Zhang, R S. Ma, X. C. Liu, E. K. Liu, G. D. Liu, Z. Y. Liu, W.
  H. Wang, and G. H. Wu","Topological Insulators in Hexagonal Wurtzite-type Binary Compounds","20 pages, 5 figures, submitted for publication","EPL, 103 (2013) 57012","10.1209/0295-5075/103/57012",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose new topological insulators in hexagonal wurtzite-type binary
compounds based on the first principles calculations. It is found that two
compounds AgI and AuI are three-dimensional topological insulators with a
naturally opened band-gap at Fermi level. From band inversion mechanism point
view, this new family of topological insulators is similar with HgTe, which has
s (Gamma 6) - p (Gamma 8) band inversion. Our results strongly support that the
spin-orbit coupling is not an essential factor to the band inversion mechanism;
on the contrary, it is mainly responsible to the formation of a global band gap
for the studied topological insulators. We further theoretically explore the
feasibility of tuning the topological order of the studied compounds with two
types of strains. The results show that the uniaxial strain can contribute
extremely drastic impacts to the band inversion behavior, which provide an
effective approach to induce topological phase transition.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 05:05:35 GMT""}]","2013-10-21"
"1303.7041","Monica Patriche","Monica Patriche","The core of the games with fractional linear utility functions","15 pages",,,,"math.OC","http://creativecommons.org/licenses/by/3.0/","  We consider fractional linear programming production games for the
single-objective and multiobjective cases. We use the method of Chakraborty and
Gupta (2002) in order to transform the fractional linear programming problems
into linear programming problems. A cooperative game is attached and we prove
the non-emptiness of the core by using the duality theory from the linear
programming. In the multiobjective case, we give a characterization of the
Stable outcome of the associate cooperative game, which is balanced. We also
consider the cooperative game associated to an exchange economy with a finite
number of agents.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 05:29:34 GMT""}]","2013-03-29"
"1303.7042","John Page Dr","W.K. Hildebrand, A. Strybulevych, S.E. Skipetrov, B.A. van Tiggelen,
  and J.H. Page","Observation of infinite-range intensity correlations above, at and below
  the 3D Anderson localization transition","13 pages, 11 figures (main text plus supplemental information).
  Updated version includes an improved introductory paragraph, minor text
  revisions, a revised title and additional supplemental information on the
  experimental details","Physical Review Letters, 112, 073902 (2014)","10.1103/PhysRevLett.112.073902",,"cond-mat.dis-nn cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We investigate long-range intensity correlations on both sides of the
Anderson transition of classical waves in a three-dimensional (3D) disordered
material. Our ultrasonic experiments are designed to unambiguously detect a
recently predicted infinite-range C0 contribution, due to local density of
states fluctuations near the source. We find that these C0 correlations, in
addition to C2 and C3 contributions, are significantly enhanced near mobility
edges. Separate measurements of the inverse participation ratio reveal a link
between C0 and the anomalous dimension \Delta_2, implying that C0 may also be
used to explore the critical regime of the Anderson transition.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 05:40:58 GMT""},{""version"":""v2"",""created"":""Thu, 11 Jul 2013 16:16:38 GMT""},{""version"":""v3"",""created"":""Sat, 23 Nov 2013 00:42:02 GMT""}]","2014-02-27"
"1303.7043","Chunhua Shen","Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin
  Tang","Inductive Hashing on Manifolds","Appearing in IEEE Conf. Computer Vision and Pattern Recognition, 2013",,"10.1109/CVPR.2013.205",,"cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Learning based hashing methods have attracted considerable attention due to
their ability to greatly increase the scale at which existing algorithms may
operate. Most of these methods are designed to generate binary codes that
preserve the Euclidean distance in the original space. Manifold learning
techniques, in contrast, are better able to model the intrinsic structure
embedded in the original high-dimensional data. The complexity of these models,
and the problems with out-of-sample data, have previously rendered them
unsuitable for application to large-scale embedding, however. In this work, we
consider how to learn compact binary embeddings on their intrinsic manifolds.
In order to address the above-mentioned difficulties, we describe an efficient,
inductive solution to the out-of-sample data problem, and a process by which
non-parametric manifold learning may be used as the basis of a hashing method.
Our proposed approach thus allows the development of a range of new hashing
techniques exploiting the flexibility of the wide variety of manifold learning
approaches available. We particularly show that hashing on the basis of t-SNE .
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 05:45:21 GMT""}]","2016-11-17"
"1303.7044","Hwa Jeong Lee","Kyungpyo Hong, Ho Lee, Hwa Jeong Lee, and Seungsang Oh","Upper bound on the total number of knot $n$-mosaics","6 pages, 3 figures",,,,"math.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Lomonaco and Kauffman introduced a knot mosaic system to give a definition of
a quantum knot system which can be viewed as a blueprint for the construction
of an actual physical quantum system. A knot $n$-mosaic is an $n \times n$
matrix of 11 kinds of specific mosaic tiles representing a knot or a link by
adjoining properly that is called suitably connected. $D_n$ denotes the total
number of all knot $n$-mosaics. Already known is that $D_1=1$, $D_2=2$, and
$D_3=22$. In this paper we establish the lower and upper bounds on $D_n$
$$\frac{2}{275}(9 \cdot 6^{n-2} + 1)^2 \cdot 2^{(n-3)^2} \ \leq \ D_n \ \leq \
\frac{2}{275}(9 \cdot 6^{n-2} + 1)^2 \cdot (4.4)^{(n-3)^2}.$$ and find the
exact number of $D_4 = 2594$.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:02:21 GMT""},{""version"":""v2"",""created"":""Tue, 9 Apr 2013 07:56:34 GMT""},{""version"":""v3"",""created"":""Mon, 10 Nov 2014 12:38:34 GMT""}]","2014-11-11"
"1303.7045","Sudip Kumar haldar","Sudip Kumar Haldar, Barnali Chakrabarti, Tapan Kumar Das, Anindya
  Biswas","Correlated many-body calculation to study characteristics of Shannon
  information entropy for ultracold trapped interacting bosons","Accepted in Physical Review A (2013)","Phys. Rev. A 88, 033602 (2013)","10.1103/PhysRevA.88.033602",,"cond-mat.quant-gas quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A correlated many-body calculation is presented to characterize the Shannon
information entropy of trapped interacting bosons. We reformulate the one-body
Shannon information entropy in terms of the one-body probability density. The
minimum limit of the entropy uncertainty relation (EUR) is approached by making
$N$ very small in our numerical work. We examine the effect of correlations in
the calculation of information entropy. Comparison with the mean-field result
shows that the correlated basis function is indeed required to characterize the
important features of the information entropies. We also accurately calculate
the point of critical instability of an attractive BEC, which is in close
agreement with the experimental value. Next we calculate two-body entropies in
position and momentum spaces and study quantum correlations in the attractive
BEC.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:05:05 GMT""},{""version"":""v2"",""created"":""Tue, 13 Aug 2013 04:29:02 GMT""}]","2013-09-16"
"1303.7046","Kazunori Noguchi","Kazunori Noguchi","Ramified coverings of small categories",,,,,"math.CT math.AT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce a ramified covering of small categories, and we show three
properties of the notion: the Riemann-Hurwitz formula holds for a ramified
covering of finite categories, the zeta function of $C$ divides that of
$\widetilde{C}$ for a ramified covering $\map{P}{\widetilde{C}}{C}$ of finite
categories, and the classifying space of a $d$-fold ramified covering of small
categories is also a $d$-fold ramified covering in the sense of Dold
\cite{Dol86}.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:09:22 GMT""}]","2013-03-29"
"1303.7047","Jesse Vaitkus","Jesse A. Vaitkus and Andrew D. Greentree","Digital three-state adiabatic passage","8 pages, 9 figures, comments welcome","Phys. Rev. A 87, 063820 (2013)","10.1103/PhysRevA.87.063820",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We explore protocols for three-state adiabatic passage where the tunnel
matrix elements are varied digitally, rather than smoothly as is the case with
conventional adiabatic passage. In particular, we focus on the STIRAP and
related three-state schemes where the control is applied stepwise, with either
equal spaced levels for the tunnel matrix elements or uniform pulse lengths.
Our results show that the evolution typically shows the hallmarks of
conventional adiabatic passage, although with additional resonances exhibiting
no state transfer.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:16:11 GMT""},{""version"":""v2"",""created"":""Fri, 17 May 2013 07:15:37 GMT""}]","2013-07-09"
"1303.7048","Zuoqiang Shi","Thomas Y. Hou, Zuoqiang Shi, Peyman Tavallali","Convergence of a data-driven time-frequency analysis method",,,,,"math.NA cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In a recent paper, Hou and Shi introduced a new adaptive data analysis method
to analyze nonlinear and non-stationary data. The main idea is to look for the
sparsest representation of multiscale data within the largest possible
dictionary consisting of intrinsic mode functions of the form $\{a(t)
\cos(\theta(t))\}$, where $a \in V(\theta)$, $V(\theta)$ consists of the
functions smoother than $\cos(\theta(t))$ and $\theta'\ge 0$. This problem was
formulated as a nonlinear $L^0$ optimization problem and an iterative nonlinear
matching pursuit method was proposed to solve this nonlinear optimization
problem. In this paper, we prove the convergence of this nonlinear matching
pursuit method under some sparsity assumption on the signal. We consider both
well-resolved and sparse sampled signals. In the case without noise, we prove
that our method gives exact recovery of the original signal.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:20:13 GMT""}]","2013-03-29"
"1303.7049","Fang Li","Fang Li, Zongzhu Lin","Approach to artinian algebras via natural quivers","19 pages","Trans Amer Math Soc 364(3)(2012) 1395-1411",,,"math.RT math.RA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Given an Artinian algebra $A$ over a field $k$, there are several
combinatorial objects associated to $A$. They are the diagram $D_A$ as defined
in [DK], the natural quiver $\Delta_A$ defined in \cite{Li} (cf. Section 2),
and a generalized version of $k$-species $(A/r, r/r^2)$ with $r$ being the
Jacobson radical of $A$. When $A$ is splitting over the field $k$, the diagram
$D_A$ and the well-known ext-quiver $\Gamma_A$ are the same. The main objective
of this paper is to investigate the relations among these combinatorial objects
and in turn to use these relations to give a characterization of the algebra
$A$.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:27:54 GMT""}]","2013-03-29"
"1303.7050","Victor Chernozhukov","Victor Chernozhukov, Christian Hansen","Quantile Models with Endogeneity","32 pages","Annual Review of Economics, vol 5, 2013","10.1146/annurev-economics-080511-110952",,"stat.AP econ.EM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this article, we review quantile models with endogeneity. We focus on
models that achieve identification through the use of instrumental variables
and discuss conditions under which partial and point identification are
obtained. We discuss key conditions, which include monotonicity and
full-rank-type conditions, in detail. In providing this review, we update the
identification results of Chernozhukov and Hansen (2005, Econometrica). We
illustrate the modeling assumptions through economically motivated examples. We
also briefly review the literature on estimation and inference.
  Key Words: identification, treatment effects, structural models, instrumental
variables
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:32:37 GMT""}]","2017-10-03"
"1303.7051","Hannes Diener","J. Berger, D. Bridges, H. Diener, H. Schwichtenberg","Constructive aspects of Riemann's permutation theorem for series",,,,,"math.LO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The notions of permutable and weak-permutable convergence of a series
$\sum_{n=1}^{\infty}a_{n}$ of real numbers are introduced. Classically, these
two notions are equivalent, and, by Riemann's two main theorems on the
convergence of series, a convergent series is permutably convergent if and only
if it is absolutely convergent. Working within Bishop-style constructive
mathematics, we prove that Ishihara's principle \BDN implies that every
permutably convergent series is absolutely convergent. Since there are models
of constructive mathematics in which the Riemann permutation theorem for series
holds but \BDN does not, the best we can hope for as a partial converse to our
first theorem is that the absolute convergence of series with a permutability
property classically equivalent to that of Riemann implies \BDN. We show that
this is the case when the property is weak-permutable convergence.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:54:06 GMT""}]","2013-03-29"
"1303.7052","Kenichi Kasamatsu","Kenichi Kasamatsu, Hiromitsu Takeuchi, Makoto Tsubota, Muneto Nitta","Wall-vortex composite solitons in two-component Bose-Einstein
  condensates","16 pages. 11 figures","Phys.Rev.A88:013620,2013","10.1103/PhysRevA.88.013620",,"cond-mat.quant-gas hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study composite solitons, consisting of domain walls and vortex lines
attaching to the walls in two-component Bose-Einstein condensates. When the
total density of two components is homogeneous, the system can be mapped to the
O(3) nonlinear sigma model for the pseudospin representing the two-component
order parameter and the analytical solutions of the composite solitons can be
obtained. Based on the analytical solutions, we discuss the detailed structure
of the composite solitons in two-component condensates by employing the
generalized nonlinear sigma model, where all degrees of freedom of the original
Gross-Pitaevskii theory are active. The density inhomogeneity results in
reduction of the domain wall tension from that in the sigma model limit. We
find that the domain wall pulled by a vortex is logarithmically bent as a
membrane pulled by a pin, and it bends more flexibly than not only the domain
wall in the sigma model but also the expectation from the reduced tension.
Finally, we study the composite soliton structure for actual experimental
situations with trapped immiscible condensates under rotation through numerical
simulations of the coupled Gross-Pitaevskii equations.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 06:57:46 GMT""}]","2013-07-17"
"1303.7053","Vasiliy Rodionov","V.N.Rodionov","Non-Hermitian $\cal PT$-symmetric quantum mechanics of relativistic
  particles with the restriction of mass","14 pages, 3 figures",,,,"quant-ph hep-ph hep-th math-ph math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The modified Dirac equations for the massive particles with the replacement
of the physical mass $m$ with the help of the relation $m\rightarrow m_1+
\gamma_5 m_2$ are investigated. It is shown that for a fermion theory with a
$\gamma_5$-mass term, the limiting of the mass specter by the value $ m_{max}=
{m_1}^2/2m_2$ takes place. In this case the different regions of the unbroken
$\cal PT$ symmetry may be expressed by means of the restriction of the physical
mass $m\leq m_{max}$. It should be noted that in the approach which was
developed by C.Bender et al. for the $\cal PT$-symmetric version of the massive
Thirring model with $\gamma_5$-mass term, the region of the unbroken $\cal
PT$-symmetry was found in the form $m_1\geq m_2$ \cite{ft12}. However on the
basis of the mass limitation $m\leq m_{max}$ we obtain that the domain $m_1\geq
m_2$ consists of two different parametric sectors: i) $0\leq m_2 \leq
m_1/\sqrt{2}$ -this values of mass parameters $m_1,m_2$ correspond to the
traditional particles for which in the limit $m_{max}\rightarrow \infty$ the
modified models are converting to the ordinary Dirac theory with the physical
mass $m$; ii)$m_1/\sqrt{2}\leq m_2 \leq m_1$ - this is the case of the unusual
particles for which equations of motion does not have a limit, when
$m_{max}\rightarrow \infty$. The presence of this possibility lets hope for
that in Nature indeed there are some ""exotic fermion fields"".
  As a matter of fact the formulated criterions may be used as a major test in
the process of the division of considered models into ordinary and exotic
fermion theories. It is tempting to think that the quanta of the exotic fermion
field have a relation to the structure of the ""dark matter"".
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:00:32 GMT""}]","2013-03-29"
"1303.7054","Shen Feng","Shen Feng and Soung C. Liew","Wireless Broadcast with Physical-Layer Network Coding","23 pages, 18 figures, 6 tables",,,,"cs.IT cs.NI math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This work investigates the maximum broadcast throughput and its achievability
in multi-hop wireless networks with half-duplex node constraint. We allow the
use of physical-layer network coding (PNC). Although the use of PNC for unicast
has been extensively studied, there has been little prior work on PNC for
broadcast. Our specific results are as follows: 1) For single-source broadcast,
the theoretical throughput upper bound is n/(n+1), where n is the ""min
vertex-cut"" size of the network. 2) In general, the throughput upper bound is
not always achievable. 3) For grid and many other networks, the throughput
upper bound n/(n+1) is achievable. Our work can be considered as an attempt to
understand the relationship between max-flow and min-cut in half-duplex
broadcast networks with cycles (there has been prior work on networks with
cycles, but not half-duplex broadcast networks).
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:09:26 GMT""},{""version"":""v2"",""created"":""Sun, 4 Aug 2013 06:34:05 GMT""}]","2013-08-06"
"1303.7055","Xue Chang","Xue Chang, Chun Liu, Yi-Lei Tang","Phenomenological Aspects of R-parity Violating Supersymmetry with A
  Vector-like Extra Generation","30 pages, 1 table, 7 figures. Accepted for publication in Phys. Rev.
  D","Phys. Rev. D 87, 075012 (2013)","10.1103/PhysRevD.87.075012",,"hep-ph","http://creativecommons.org/licenses/by/3.0/","  Phenomenological analysis to the R-parity violating supersymmetry with a
vector-like extra generation is performed in detail. It is found that, via the
trilinear couplings, the correct neutrino spectrum can be obtained. The Higgs
mass rises to 125 GeV by new up-type Yukawa couplings of vector-like quarks
with no need of very heavy superpartners. Phenomena of new heavy fermions at
LHC are predicted.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:18:16 GMT""},{""version"":""v2"",""created"":""Sun, 31 Mar 2013 08:14:56 GMT""}]","2013-10-14"
"1303.7056","Hiroshi Okada","Yasuhiro Daikoku, Hiroshi Okada","Phenomenology of S_4 Flavor Symmetric extra U(1) model","33 pages, 7 tables, no figures; version accepted for publication in
  Physical Review D",,"10.1103/PhysRevD.88.015034","KIAS-P13019","hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study several phenomenologies of an E_6 inspired extra U(1) model with S_4
flavor symmetry. With the assignment of left-handed quarks and leptons to
S_4-doublet, SUSY flavor problem is softened. As the extra Higgs bosons are
neutrinophilic, baryon number asymmetry in the universe is realized by
leptogenesis without causing gravitino overproduction. We find that the allowed
region for the lightest chargino mass is given by 100-140 GeV, if the dark
matter is a singlino dominated neutralino whose mass is about 36 GeV.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:22:05 GMT""},{""version"":""v2"",""created"":""Wed, 26 Jun 2013 16:50:27 GMT""}]","2013-08-09"
"1303.7057","Vasant Natarajan","K. D. Rathod, Alok K. Singh, and Vasant Natarajan","Continuous beam of laser-cooled Yb atoms","5 pages, 4 figures","Europhysics Letters 102, 43001 (2013)","10.1209/0295-5075/102/43001",,"physics.atom-ph physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We demonstrate launching of laser-cooled Yb atoms in a continuous atomic
beam. The continuous cold beam has significant advantages over the more-common
pulsed fountain, which was also demonstrated by us recently. The cold beam is
formed in the following steps---(i) Atoms from a thermal beam are first Zeeman
slowed to a small final velocity, (ii) the slowed atoms are captured in a
two-dimensional magneto-optic trap (2D-MOT), and (iii) atoms are launched {\em
continuously} in the vertical direction using two sets of moving-molasses
beams, inclined at $\pm 15^\circ$ to the vertical. The cooling transition used
is the strongly-allowed ${^1S}_0 \rightarrow {^1P}_1$ transition at 399 nm. We
capture about $7 \times 10^6$ atoms in the 2D-MOT, and then launch them with a
vertical velocity of 13 m/s at a longitudinal temperature of 125(6) mK.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:30:19 GMT""},{""version"":""v2"",""created"":""Tue, 18 Jun 2013 11:48:14 GMT""}]","2013-06-19"
"1303.7058","Anushree Roy","Jaya Kumar Panda, Anushree Roy, Mauro Gemmi, Elena Husanu, Ang Li,
  Daniele Ercolani, and Lucia Sorba","Electronic Band Structure of Wurtzite GaP Nanowires via Resonance Raman
  Spectroscopy","24 pages, 6 figures",,"10.1063/1.4813625",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Raman measurements are performed on defect-free wurzite GaP nanowires.
Resonance Raman measurements are carried out over the excitation energy range
between 2.19 and 2.71 eV. Resonances at 2.38 eV and 2.67 eV of the E1(LO) mode
and at 2.67 eV of the A1(LO) are observed. The presence of these intensity
resonances clearly demonstrates the existence of energy states with Gamma_9hh
and Gamma_7V (Gamma_7C) symmetries of the valence (conduction) band and allows
to measure WZ phase GaP band energies at the Gamma point. In addition, we have
investigated temperature dependent resonant Raman measurements, which allowed
us to extrapolate the zero temperature values of Gamma point energies, along
with the crystal field and spin-orbit splitting energies. Above results provide
a feedback for refining available theoretical calculations to derive the
correct wurtzite III-V semiconductor band structure.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:34:30 GMT""}]","2015-06-15"
"1303.7059","Masaki Takayama","Masaki Takayama, Hideyuki Saio, Yoshifusa Ita","On the pulsation modes of OGLE small amplitude red giant variables in
  the LMC","8 pages, 13 figures, accepted for publication in MNRAS",,"10.1093/mnras/stt398",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We discuss the properties of pulsations in the OGLE Small Amplitude Red
Giants (OSARGs) in the Large Magellanic Cloud (LMC). We consider stars below
the red-giant tip in this paper. They are multi-periodic and form three
sequences in the period-luminosity plane. Comparing the periods and period
ratios with our theoretical models, we have found that these sequences
correspond to radial first to third overtones, and nonradial dipole p$_4$ and
quadrupole p$_2$ modes. The red-giant branch stars of OSARGs consist of stars
have initial masses of $\sim0.9 - 1.4M_\odot$ which corresponds to a luminosity
range of $\log L/L_\odot \simeq 2.8 - 3.4$. With these parameters, the scaled
optimal frequency $\nu_{\rm max}$ for solar-like oscillations goes through
roughly the middle of the three sequences in the period-luminosity plane,
suggesting the stochastic excitation is likely the cause of the pulsations in
OSARGs.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:52:34 GMT""}]","2015-06-15"
"1303.7060","Chung-Pin  Chou","Chung-Pin Chou","Low-lying quasiparticle excitations in strongly-correlated
  superconductors: An ansatz from BCS quasiparticle excitations?","4 pages, 2 figures",,"10.1016/j.jpcs.2013.05.027",,"cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The question about the existence of Bogoliubov's quasiparticles in the BCS
wave functions underneath Gutzwiller's projection is of importance to strongly
correlated systems. We develop a method to examine the two-particle excitations
of Gutzwiller-projected BCS wave functions by using the variational Monte Carlo
approach. We find that the exact Gutzwiller-projected quasiparticle (GQP)
dispersions are quantitatively reproduced by the Gutzwiller-projected
Bogoliubov quasiparticles (GBQP) except the regions where d-wave Cooper pairing
is strong. Since GQP still shows higher energy than GBQP near the antinodes, we
believe GBQP provides a reasonable description to the low-energy excitations in
strongly correlated superconducting systems. In addition, the intimate
connection between Gutzwiller's projection and d-wave Cooper pairing may also
imply that strong correlations play a significant role in the nodal-antinodal
dichotomy seen by photoemission experiments in cuprates.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 07:57:40 GMT""}]","2015-06-15"
"1303.7061","Hans C. Eggers","H. C. Eggers and B. Buschbeck","Internal cumulants for femtoscopy with fixed charged multiplicity","33 pages, 3 figures, 79 references","Advances in High Energy Physics Vol 2013, Article ID 230515","10.1155/2013/230515",,"hep-ex nucl-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A detailed understanding of all effects and influences on higher-order
correlations is essential. At low charged multiplicity, the effect of a
nonpoissonian multiplicity distribution can significantly distort correlations.
Evidently, the reference samples with respect to which correlations are
measured should yield a null result in the absence of correlations. We show how
the careful specification of desired properties necessarily leads to an
average-of-multinomials reference sample. The resulting internal cumulants and
their averaging over several multiplicities fulfil all requirements of
correctly taking into account nonpoissonian multiplicity distributions as well
as yielding a null result for uncorrelated fixed-N samples. Various correction
factors are shown to be approximations at best. Careful rederivation of
statistical variances and covariances within the frequentist approach yields
errors for cumulants that differ from those used so far. We finally briefly
discuss the implementation of the analysis through a multiple event buffer
algorithm.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:06:53 GMT""},{""version"":""v2"",""created"":""Fri, 28 Jun 2013 15:34:38 GMT""}]","2013-08-22"
"1303.7062","Boris E. Meierovich","Boris E. Meierovich","Galaxy rotation curves. The theory","17 pages, 13 figures","Physical Review D 87, D 87, 103510 (2013)","10.1103/PhysRevD.87.103510",,"gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The non-gauge vector field with as simple as possible Lagrangian
(\ref{Lagrangian}) turned out an adequate tool for macroscopic description of
the main properties of dark matter. The dependence of the velocity of a star on
the radius of the orbit $V\left(r\right) $ -- galaxy rotation curve -- is
derived analytically from the first principles\ completely within the
Einstein's general relativity. The Milgrom's empirical modification of
Newtonian dynamics in nonrelativistic limit (MOND) gets justified and specified
in detail. In particular, the transition to a plateau is accompanied by damping
oscillations. In the scale of a galaxy, and in the scale of the whole universe,
the dark matter is described by a vector field with the same energy-momentum
tensor. It is the evidence of the common physical nature. Now, when we have the
general expression (\ref{Tik b=c=0}) for the energy-momentum tensor of dark
matter, it is possible to analyze its influence on the structure and evolution
of super heavy stars and black holes.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:13:51 GMT""}]","2016-05-24"
"1303.7063","Dr. Georgios M. Nikolopoulos","Georgios M. Nikolopoulos","Statistics of a quantum-state-transfer Hamiltonian in the presence of
  disorder",,"Phys. Rev. A 87, 042311 (2013)","10.1103/PhysRevA.87.042311",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a statistical analysis on the performance of a protocol for the
faithful transfer of a quantum state in finite qubit or spin chains, in the
presence of diagonal and off-diagonal disorder. It is shown that the
average-state fidelity, typically employed in the literature for the
quantification of the transfer, may overestimate considerably the performance
of the protocol in a single realization, leading to faulty conclusions about
the success of the transfer.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:14:35 GMT""}]","2013-04-10"
"1303.7064","Yuriy Sydorenko","Oleksandr Chvartatskyi and Yuriy Sydorenko","A new (1+1)-dimensional matrix k-constrained KP hierarchy","20 pages. arXiv admin note: substantial text overlap with
  arXiv:1303.6510",,,,"nlin.SI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce a new generalization of matrix (1+1)-dimensional k-constrained
KP hierarchy. The new hierarchy contains matrix generalizations of stationary
DS systems, (2+1)-dimensional modified Korteweg-de Vries equation and the
Nizhnik equation. A binary Darboux transformation method is proposed for
integration of systems from this hierarchy.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:15:58 GMT""}]","2013-03-29"
"1303.7065","Chen Li","Chen Li, Tingnan Zhang, and Daniel I. Goldman","A Terradynamics of Legged Locomotion on Granular Media",,"Science 339, 1408-1411 (2013)","10.1126/science.1229163",,"physics.bio-ph cond-mat.soft physics.flu-dyn q-bio.QM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The theories of aero- and hydrodynamics predict animal movement and device
design in air and water through the computation of lift, drag, and thrust
forces. Although models of terrestrial legged locomotion have focused on
interactions with solid ground, many animals move on substrates that flow in
response to intrusion. However, locomotor-ground interaction models on such
flowable ground are often unavailable. We developed a force model for
arbitrarily-shaped legs and bodies moving freely in granular media, and used
this ""terradynamics"" to predict a small legged robot's locomotion on granular
media using various leg shapes and stride frequencies. Our study reveals a
complex but generic dependence of stresses in granular media on intruder depth,
orientation, and movement direction and gives insight into the effects of leg
morphology and kinematics on movement.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:20:28 GMT""}]","2019-11-04"
"1303.7066","Kazuhiko Seki","Kazuhiko Seki, Shigeyuki Komura and Sanoop Ramachandran","Growth kinetics of circular liquid domains on vesicles by
  diffusion-controlled coalescence","16pages, 3 figures","J. Phys.: Condens. Matter 25 (2013) 195105","10.1088/0953-8984/25/19/195105",,"cond-mat.soft physics.chem-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Motivated by recent experiments on multi-component membranes, the growth
kinetics of domains on vesicles is theoretically studied. It is known that the
steady-state rate of coalescence cannot be obtained by taking the long-time
limit of the coalescence rate when the membrane is regarded as an infinite
two-dimensional (2D) system. The steady-state rate of coalescence is obtained
by explicitly taking into account the spherical vesicle shape. Using the
expression of the 2D diffusion coefficient obtained in the limit of small
domain size, an analytical expression for the domain growth kinetics is
obtained when the circular shape is always maintained. For large domains, the
growth kinetics is discussed by investigating the size dependence of the
coalescence rate using the expression for the diffusion coefficient of
arbitrary domain size.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:24:04 GMT""}]","2013-04-26"
"1303.7067","Yishai Shperber","Yishai Shperber, Omer Sinwani, Netanel Naftalis, Daniel Bedau, James
  W. Reiner, and Lior Klein","Thermally assisted current-induced magnetization reversal in SrRuO3",,"Phys. Rev. B 87, 115118 (2013)","10.1103/PhysRevB.87.115118",,"cond-mat.mtrl-sci cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We inject a sequence of 1 ms current pulses into uniformly magnetized
patterns of the itinerant ferromagnet SrRuO3 until a magnetization reversal is
detected. We detect the effective temperature during the pulse and find that
the cumulative pulse time required to induce magnetization reversal depends
exponentially on 1/T. In addition, we find that the cumulative pulse time also
depends exponentially on the current amplitude. These observations indicate
current-induced magnetization reversal assisted by thermal fluctuations.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:30:26 GMT""}]","2013-03-29"
"1303.7068","Daniel Huber","W.D. Apel, J.C. Arteaga, L. B\""ahren, K. Bekk, M. Bertaina, P.L.
  Biermann, J. Bl\""umer, H. Bozdog, I.M. Brancus, P. Buchholz, S. Buitink, E.
  Cantoni, A. Chiavassa, K. Daumiller, V. de Souza, P. Doll, M. Ender, R.
  Engel, H. Falcke, M. Finger, D. Fuhrmann, H. Gemmeke, C. Grupen, A. Haungs,
  D. Heck, J.R. H\""orandel, A. Horneffer, D. Huber, T. Huege, P.G. Isar, K.-H.
  Kampert, D. Kang, O. Kr\""omer, J. Kuijpers, K. Link, P. Luczak, M. Ludwig,
  H.J. Mathes, M. Melissas, C. Morello, S. Nehls, J. Oehlschl\""ager, N.
  Palmieri, T. Pierog, J. Rautenberg, H. Rebel, M. Roth, C. R\""uhle, A.
  Saftoiu, H. Schieler, A. Schmidt, F.G. Schr\""oder, O. Sima, G. Toma, G.C.
  Trinchero, A. Weindl, J. Wochele, M. Wommer, J. Zabierowski, J.A. Zensus","Thunderstorm Observations by Air-Shower Radio Antenna Arrays",,"Advances in Space Research, Volume 48, Issue 7, 1 October 2011,
  Pages 1295-1303, ISSN 0273-1177, 10.1016/j.asr.2011.06.003.
  (http://www.sciencedirect.com/science/article/pii/S0273117711004297)","10.1016/j.asr.2011.06.003",,"astro-ph.HE astro-ph.IM","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Relativistic, charged particles present in extensive air showers lead to a
coherent emission of radio pulses which are measured to identify the shower
initiating high-energy cosmic rays. Especially during thunderstorms, there are
additional strong electric fields in the atmosphere, which can lead to further
multiplication and acceleration of the charged particles and thus have
influence on the form and strength of the radio emission. For a reliable energy
reconstruction of the primary cosmic ray by means of the measured radio signal
it is very important to understand how electric fields affect the radio
emission. In addition, lightning strikes are a prominent source of broadband
radio emissions that are visible over very long distances. This, on the one
hand, causes difficulties in the detection of the much lower signal of the air
shower. On the other hand the recorded signals can be used to study features of
the lightning development. The detection of cosmic rays via the radio emission
and the influence of strong electric fields on this detection technique is
investigated with the LOPES experiment in Karlsruhe, Germany. The important
question if a lightning is initiated by the high electron density given at the
maximum of a high-energy cosmic-ray air shower is also investigated, but could
not be answered by LOPES. But, these investigations exhibit the capabilities of
EAS radio antenna arrays for lightning studies. We report about the studies of
LOPES measured radio signals of air showers taken during thunderstorms and give
a short outlook to new measurements dedicated to search for correlations of
lightning and cosmic rays.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:38:46 GMT""}]","2013-03-29"
"1303.7069","Johannes Hoppenau","Johannes Hoppenau, Andreas Engel","On the work distribution in quasi-static processes","11 pages, 1 figure",,"10.1088/1742-5468/2013/06/P06004",,"cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We derive a systematic, multiple time-scale perturbation expansion for the
work distribution in isothermal quasi-static Langevin processes. To first order
we find a Gaussian distribution reproducing the result of Speck and Seifert
[Phys. Rev. E 70, 066112 (2004)]. Scrutinizing the applicability of
perturbation theory we then show that, irrespective of time-scale separation,
the expansion breaks down when applied to untypical work values from the tails
of the distribution. We thus reconcile the result of Speck and Seifert with
apparently conflicting exact expressions for the asymptotics of work
distributions in special systems and with an intuitive argument building on the
central limit theorem.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:47:17 GMT""},{""version"":""v2"",""created"":""Fri, 3 May 2013 08:53:57 GMT""}]","2015-06-15"
"1303.7070","Daniel Huber","D. Huber, W.D. Apel, J.C. Arteaga, L. B\""ahren, K. Bekk, M. Bertaina,
  P.L. Biermann, J. Bl\""umer, H. Bozdog, I.M. Brancus, P. Buchholz, E. Cantoni,
  A. Chiavassa, K. Daumiller, V. de Souza, F. Di Pierro, P. Doll, R. Engel, H.
  Falcke, M. Finger, B. Fuchs, D. Fuhrmann, H. Gemmeke, C. Grupen, A. Haungs,
  D. Heck, J.R. H\""orandel, A. Horneffer, T. Huege, P.G. Isar, K.-H. Kampert,
  D. Kang, O. Kr\""omer, J. Kuijpers, K. Link, P. Luczak, M. Ludwig, H.J.
  Mathes, M. Melissas, C. Morello, J. Oehlschl\""ager, N. Palmieri, T. Pierog,
  J. Rautenberg, H. Rebel, M. Roth, C. R\""uhle, A. Saftoiu, H. Schieler, A.
  Schmidt, F.G. Schr\""oder, O. Sima, G. Toma, G.C. Trinchero, A. Weindl, J.
  Wochele, M. Wommer, J. Zabierowski, J.A. Zensus","LOPES 3D reconfiguration and first measurements","Proceedings","Proceedings of the 32nd International Cosmic Ray Conference
  (ICRC2011), held 11-18 August, 2011 in Beijing, China. Vol. 3 HE1.4:
  Extensive Air Showers and HE Cosmic Rays., p.72",,,"astro-ph.IM astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Radio detection technique of high-energy cosmic rays is based on the
radio signal emitted by the charged particles in an air shower due to their
deflection in the Earth's magnetic field. The LOPES experiment at Karlsruhe
Institute of Technology, Germany with its simple dipoles made major
contributions to the revival of this technique. LOPES is working in the
frequency range from 40 to 80 MHz and was reconfigured several times to improve
and further develop the radio detection technique. In the current setup LOPES
consists of 10 tripole antennas which measure the complete electric field
vector of the radio emission from cosmic rays. LOPES is the first experiment
measuring all three vectorial components at once and thereby gaining the full
information about the electric field vector and not only a two-dimensional
projection. Such a setup including also measurements of the vertical electric
field component is expected to increase the sensitivity to inclined showers and
help to advance the understanding of the emission mechanism. We present the
reconfiguration and calibration procedure of LOPES 3D and discuss first
measurements.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:54:35 GMT""}]","2013-03-29"
"1303.7071","Sascha Trippe","Sascha Trippe (SNU, Seoul)","A Derivation of Modified Newtonian Dynamics","4 pages, 1 figure; to appear in JKAS (received 2013 February 14;
  revised 2013 March 22; accepted 2013 March 28)",,"10.5303/JKAS.2013.46.2.093",,"astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modified Newtonian Dynamics (MOND) is a possible solution for the missing
mass problem in galactic dynamics; its predictions are in good agreement with
observations in the limit of weak accelerations. However, MOND does not derive
from a physical mechanism and does not make predictions on the transitional
regime from Newtonian to modified dynamics; rather, empirical transition
functions have to be constructed from the boundary conditions and comparison to
observations. I compare the formalism of classical MOND to the scaling law
derived from a toy model of gravity based on virtual massive gravitons (the
""graviton picture"") which I proposed recently. I conclude that MOND naturally
derives from the ""graviton picture"" at least for the case of non-relativistic,
highly symmetric dynamical systems. This suggests that - to first order - the
""graviton picture"" indeed provides a valid candidate for the physical mechanism
behind MOND and gravity on galactic scales in general.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:55:28 GMT""}]","2017-01-18"
"1303.7072","Jian-Hua Zheng","Jian-Hua Zheng","Transfer operator and conformal measures for a class of maps having
  covering property","27 pages",,,,"math.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Let $(X,d)$ be a metric space and $X_0$ be an open and dense subset of $X$.
We develop the Walters' theory and discuss the existence of conformal measures
in terms of the Perron-Frobenius-Ruelle operator for a continuous map
$T:X_0\rightarrow X$ and the Bowen formula about Hausdorff dimension and
Poincar\'e exponent of some invariant subsests for $T$ with some expanding
property.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:56:20 GMT""}]","2013-03-29"
"1303.7073","Franz X. Bronold","Rafael L. Heinisch, Franz X. Bronold, and Holger Fehske","Surface electrons at plasma walls","To appear in Complex Plasmas: Scientific Challenges and Technological
  Opportunities, Editors: M. Bonitz, K. Becker, J. Lopez and H. Thomsen",,,,"physics.plasm-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this chapter we introduce a microscopic modelling of the surplus electrons
on the plasma wall which complements the classical description of the plasma
sheath. First we introduce a model for the electron surface layer to study the
quasistationary electron distribution and the potential at an unbiased plasma
wall. Then we calculate sticking coefficients and desorption times for electron
trapping in the image states. Finally we study how surplus electrons affect
light scattering and how charge signatures offer the possibility of a novel
charge measurement for dust grains.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 08:59:51 GMT""}]","2013-03-29"
"1303.7074","Livio Flaminio","Livio Flaminio, Giovanni Forni, Federico Rodriguez Hertz","Invariant Distributions for homogeneous flows","43 pages",,,,"math.DS","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We prove that every homogeneous flow on a finite-volume homogeneous manifold
has countably many independent invariant distributions unless it is conjugate
to a linear flow on a torus. We also prove that the same conclusion holds for
every affine transformation of a homogenous space which is not conjugate to a
toral translation. As a part of the proof, we have that any smooth partially
hyperbolic flow on any compact manifold has countably many distinct minimal
sets, hence countably many distinct ergodic probability measures. As a
consequence, the Katok and Greenfield-Wallach conjectures hold in all of the
above cases.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:12:30 GMT""},{""version"":""v2"",""created"":""Wed, 22 Jul 2015 17:00:44 GMT""}]","2015-07-23"
"1303.7075","Debajyoti Mukhopadhyay Prof.","Debajyoti Mukhopadhyay, Gitesh Sonawane, Parth Sarthi Gupta, Sagar
  Bhavsar, Vibha Mittal","Enhanced Security for Cloud Storage using File Encryption","6 pages, 4 figures",,,,"cs.CR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Cloud computing is a term coined to a network that offers incredible
processing power, a wide array of storage space and unbelievable speed of
computation. Social media channels, corporate structures and individual
consumers are all switching to the magnificent world of cloud computing. The
flip side to this coin is that with cloud storage emerges the security issues
of confidentiality, data integrity and data availability. Since the cloud is a
mere collection of tangible super computers spread across the world,
authentication and authorization for data access is more than a necessity. Our
work attempts to overcome these security threats. The proposed methodology
suggests the encryption of the files to be uploaded on the cloud. The integrity
and confidentiality of the data uploaded by the user is ensured doubly by not
only encrypting it but also providing access to the data only on successful
authentication.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:20:54 GMT""}]","2013-03-29"
"1303.7076","Hans Havlicek","Andrea Blunck, Hans Havlicek","Projective lines over Jordan systems and geometry of Hermitian matrices",,"Linear Algebra and its Applications 433 (2010) 672-680","10.1016/j.laa.2010.03.037",,"math.AG math.RA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Any set of $\sigma$-Hermitian matrices of size $n \times n$ over a field with
involution $\sigma$ gives rise to a projective line in the sense of ring
geometry and a projective space in the sense of matrix geometry. It is shown
that the two concepts are based upon the same set of points, up to some
notational differences.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:32:36 GMT""}]","2013-03-29"
"1303.7077","Oleg Verbitsky","Christoph Berkholz and Oleg Verbitsky","On the speed of constraint propagation and the time complexity of arc
  consistency testing","19 pages, 5 figures",,,,"cs.LO cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Establishing arc consistency on two relational structures is one of the most
popular heuristics for the constraint satisfaction problem. We aim at
determining the time complexity of arc consistency testing. The input
structures $G$ and $H$ can be supposed to be connected colored graphs, as the
general problem reduces to this particular case. We first observe the upper
bound $O(e(G)v(H)+v(G)e(H))$, which implies the bound $O(e(G)e(H))$ in terms of
the number of edges and the bound $O((v(G)+v(H))^3)$ in terms of the number of
vertices. We then show that both bounds are tight up to a constant factor as
long as an arc consistency algorithm is based on constraint propagation (like
any algorithm currently known).
  Our argument for the lower bounds is based on examples of slow constraint
propagation. We measure the speed of constraint propagation observed on a pair
$G,H$ by the size of a proof, in a natural combinatorial proof system, that
Spoiler wins the existential 2-pebble game on $G,H$. The proof size is bounded
from below by the game length $D(G,H)$, and a crucial ingredient of our
analysis is the existence of $G,H$ with $D(G,H)=\Omega(v(G)v(H))$. We find one
such example among old benchmark instances for the arc consistency problem and
also suggest a new, different construction.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:39:53 GMT""}]","2013-03-29"
"1303.7078","Stephane  Coen","Stephane Coen and Miro Erkintalo","Universal scaling laws of Kerr frequency combs","3 pages, 3 figures. Submitted to Optics Letters on 28 March 2013","Optics Letters 38 (2013) 1790-1792","10.1364/OL.38.001790",,"physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using the known solutions of the Lugiato-Lefever equation, we derive
universal trends of Kerr frequency combs. In particular, normalized properties
of temporal cavity soliton solutions lead us to a simple analytic estimate of
the maximum attainable bandwidth for given pump-resonator parameters. The
result is validated via comparison with past experiments encompassing a diverse
range of resonator configurations and parameters.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:40:32 GMT""}]","2013-10-22"
"1303.7079","Jeremy Leconte","J\'er\'emy Leconte, Francois Forget, Benjamin Charnay, Robin
  Wordsworth, Franck Selsis, Ehouarn Millour","3D climate modeling of close-in land planets: Circulation patterns,
  climate moist bistability and habitability","Accepted for publication in Astronomy and Astrophysics, complete
  abstract in the pdf, 18 pages, 18 figures",,"10.1051/0004-6361/201321042",,"astro-ph.EP physics.ao-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The inner edge of the classical habitable zone is often defined by the
critical flux needed to trigger the runaway greenhouse instability. This 1D
notion of a critical flux, however, may not be so relevant for inhomogeneously
irradiated planets, or when the water content is limited (land planets).
  Here, based on results from our 3D global climate model, we find that the
circulation pattern can shift from super-rotation to stellar/anti stellar
circulation when the equatorial Rossby deformation radius significantly exceeds
the planetary radius. Using analytical and numerical arguments, we also
demonstrate the presence of systematic biases between mean surface temperatures
or temperature profiles predicted from either 1D or 3D simulations.
  Including a complete modeling of the water cycle, we further demonstrate that
for land planets closer than the inner edge of the classical habitable zone,
two stable climate regimes can exist. One is the classical runaway state, and
the other is a collapsed state where water is captured in permanent cold traps.
We identify this ""moist"" bistability as the result of a competition between the
greenhouse effect of water vapor and its condensation. We also present
synthetic spectra showing the observable signature of these two states.
  Taking the example of two prototype planets in this regime, namely Gl581c and
HD85512b, we argue that they could accumulate a significant amount of water ice
at their surface. If such a thick ice cap is present, gravity driven ice flows
and geothermal flux should come into play to produce long-lived liquid water at
the edge and/or bottom of the ice cap. Consequently, the habitability of
planets at smaller orbital distance than the inner edge of the classical
habitable zone cannot be ruled out. Transiting planets in this regime represent
promising targets for upcoming observatories like EChO and JWST.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:44:27 GMT""}]","2015-06-15"
"1303.7080","Daniel Huber","W.D. Apel, J.C. Arteaga, L. B\""ahren, K. Bekk, M. Bertaina, P.L.
  Biermann, J. Bl\""umer, H. Bozdog, I.M. Brancus, A. Chiavassa, K. Daumiller,
  V. de Souza, F. Di Pierro, P. Doll, R. Engel, H. Falcke, B. Fuchs, D.
  Fuhrmann, H. Gemmeke, C. Grupen, A. Haungs, D. Heck, J.R. H\""orandel, A.
  Horneffer, D. Huber, T. Huege, P.G. Isar, K.-H. Kampert, D. Kang, O.
  Kr\""omer, J. Kuijpers, K. Link, P. Luczak, M. Ludwig, H.J. Mathes, M.
  Melissas, C. Morello, J. Oehlschl\""ager, N. Palmieri, T. Pierog, J.
  Rautenberg, H. Rebel, M. Roth, C. R\""uhle, A. Saftoiu, H. Schieler, A.
  Schmidt, F.G. Schr\""oder, O. Sima, G. Toma, G.C. Trinchero, A. Weindl, J.
  Wochele, J. Zabierowski, J.A. Zensus","LOPES 3D - vectorial measurements of radio emission from cosmic ray
  induced air showers","Proceedings ARENA 2012",,,,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  LOPES 3D is able to measure all three components of the electric field vector
of the radio emission from air showers. This allows a better comparison with
emission models. The measurement of the vertical component increases the
sensitivity to inclined showers. By measuring all three components of the
electric field vector LOPES 3D demonstrates by how much the reconstruction
accuracy of primary cosmic ray parameters increases. Thus LOPES 3D evaluates
the usefulness of vectorial measurements for large scale applications.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:04:38 GMT""}]","2013-03-29"
"1303.7081","Bastien Marmet","Bastien Marmet","Quasi-Stationary Distributions for Stochastic Approximation Algorithms
  with constant step size",,,,,"math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we investigate quasi-stationary distributions {\mu}_N of
stochastic approximation algorithms with constant step size which can be viewed
as random perturbations of a time-continuous dynamical system. Inspired by
ecological models these processes have a closed absorbing set corresponding to
extinction. Under some large deviation assumptions and the existence of an
interior attractor for the ODE, we show that the weak* limit points of the QSD
{\mu}_N are invariant measures for the ODE with support in the interior
attractors.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:16:58 GMT""},{""version"":""v2"",""created"":""Thu, 2 May 2013 12:57:46 GMT""}]","2013-05-03"
"1303.7082","Tukumuli Mila","St\'ephane Ballet, Alexis Bonnecaze and Mila Tukumuli","On the construction of elliptic Chudnovsky-type algorithms for
  multiplication in large extensions of finite fields","arXiv admin note: text overlap with arXiv:1107.0336 by other authors",,,,"math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We indicate a strategy in order to construct bilinear multiplication
algorithms of type Chudnovsky in large extensions of any finite field. In
particular, by using the symmetric version of the generalization of
Randriambololona specialized on the elliptic curves, we show that it is
possible to construct such algorithms with low bilinear complexity. More
precisely, if we only consider the Chudnovsky-type algorithms of type symmetric
elliptic, we show that the symmetric bilinear complexity of these algorithms is
in $O(n(2q)^{\log_q^*(n)})$ where $n$ corresponds to the extension degree, and
$\log_q^*(n)$ is the iterated logarithm. Moreover, we show that the
construction of such algorithms can be done in time polynomial in $n$. Finally,
applying this method we present the effective construction, step by step, of
such an algorithm of multiplication in the finite field $\F_{3^{57}}$.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:24:30 GMT""}]","2013-03-29"
"1303.7083","Ziv Goldfeld","Ziv Goldfeld, Haim H. Permuter and Benjamin M. Zaidel","The Finite State MAC with Cooperative Encoders and Delayed CSI",,"IEEE Transactions on Information Theory, Vol. 60, No. 10, October
  2014","10.1109/TIT.2014.2346494",,"cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we consider the finite-state multiple access channel (MAC)
with partially cooperative encoders and delayed channel state information
(CSI). Here partial cooperation refers to the communication between the
encoders via finite-capacity links. The channel states are assumed to be
governed by a Markov process. Full CSI is assumed at the receiver, while at the
transmitters, only delayed CSI is available. The capacity region of this
channel model is derived by first solving the case of the finite-state MAC with
a common message. Achievability for the latter case is established using the
notion of strategies, however, we show that optimal codes can be constructed
directly over the input alphabet. This results in a single codebook
construction that is then leveraged to apply simultaneous joint decoding.
Simultaneous decoding is crucial here because it circumvents the need to rely
on the capacity region's corner points, a task that becomes increasingly
cumbersome with the growth in the number of messages to be sent. The common
message result is then used to derive the capacity region for the case with
partially cooperating encoders. Next, we apply this general result to the
special case of the Gaussian vector MAC with diagonal channel transfer
matrices, which is suitable for modeling, e.g., orthogonal frequency division
multiplexing (OFDM)-based communication systems. The capacity region of the
Gaussian channel is presented in terms of a convex optimization problem that
can be solved efficiently using numerical tools. The region is derived by first
presenting an outer bound on the general capacity region and then suggesting a
specific input distribution that achieves this bound. Finally, numerical
results are provided that give valuable insight into the practical implications
of optimally using conferencing to maximize the transmission rates.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:30:39 GMT""},{""version"":""v2"",""created"":""Mon, 20 Jan 2014 13:51:56 GMT""},{""version"":""v3"",""created"":""Thu, 29 Jan 2015 12:11:30 GMT""}]","2016-11-17"
"1303.7084","Imre Ferenc Barna Dr.","I. F. Barna","Self-similar shock wave solutions of the non-linear Maxwell equations","6 pages, 2 figures published in Laser Phys. 24 (2014) 086002","Laser Phys. 24 086002 (2014)","10.1088/1054-660X/24/8/086002",,"physics.class-ph physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In our study we consider nonlinear, power-law field-dependent electrical
permitivity and magnetic permeability and investigate the time-dependent
Maxwell equations with the self-similar Ansatz. This is a first-order
hyperbolic PDE system which can conserve non-continuous initial conditions
describing electromagnetic shock-waves. Besides shock-waves other interesting
solutions (e.g. with localized compact support) can be found with delicate
physical properties. Such phenomena may happen in complex materials induced by
the planned powerful Extreme Light Infrastructure(ELI) laser pulses.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:31:22 GMT""},{""version"":""v2"",""created"":""Tue, 9 Sep 2014 14:04:51 GMT""}]","2014-09-10"
"1303.7085","Othman Benammar","Othman Benammar, Hicham Elasri, Abderrahim Sekkaki","Semantic Matching of Security Policies to Support Security Experts","SECURWARE 2012 : The Sixth International Conference on Emerging
  Security Information, Systems and Technologies",,,,"cs.CR cs.AI","http://creativecommons.org/licenses/by-nc-sa/3.0/","  Management of security policies has become increasingly difficult given the
number of domains to manage, taken into consideration their extent and their
complexity. Security experts has to deal with a variety of frameworks and
specification languages used in different domains that may belong to any Cloud
Computing or Distributed Systems. This wealth of frameworks and languages make
the management task and the interpretation of the security policies so
difficult. Each approach provides its own conflict management method or tool,
the security expert will be forced to manage all these tools, which makes the
field maintenance and time consuming expensive. In order to hide this
complexity and to facilitate some security experts tasks and automate the
others, we propose a security policies aligning based on ontologies process;
this process enables to detect and resolve security policies conflicts and to
support security experts in managing tasks.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:40:23 GMT""}]","2013-04-02"
"1303.7086","Jeremy Shears","Jeremy Shears and Theresa Hull","Dr. Harold Whichello: medicine and astronomy in Cheshire","Accepted for publication in the Journal of the British Astronomical
  Association. 20 pages, 9 figures",,,,"physics.hist-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dr. Harold Whichello (1870-1945) was a Cheshire General Practitioner and an
enthusiastic amateur astronomer. He joined the British Astronomical Association
in 1898 and undertook observations for its Lunar, Solar and Variable Star
Sections using a 6-inch Wray refractor. He also contributed lunar occultation
predictions and comet ephemerides to its Computing Section.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:47:33 GMT""}]","2013-03-29"
"1303.7087","Carlo Ferrigno","C. Ferrigno, R. Farinelli, E. Bozzo, K. Pottschmidt, D. Klochkov, P.
  Kretschmar","RX J0440.9+4431: a persistent Be/X-ray binary in outburst","Accepted for publication by A&A",,"10.1051/0004-6361/201321053",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The persistent Be/X-ray binary RX J0440.9+4431 flared in 2010 and 2011 and
has been followed by various X-ray facilities Swift, RXTE, XMM-Newton, and
INTEGRAL. We studied the source timing and spectral properties as a function of
its X-ray luminosity to investigate the transition from normal to flaring
activity and the dynamical properties of the system. We have determined the
orbital period from the long-term Swift/BAT light curve, but our determinations
of the spin period are not precise enough to constrain any orbital solution.
The source spectrum can always be described by a bulk-motion Comptonization
model of black body seed photons attenuated by a moderate photoelectric
absorption. At the highest luminosity, we measured a curvature of the spectrum,
which we attribute to a significant contribution of the radiation pressure in
the accretion process. This allows us to estimate that the transition from a
bulk-motion-dominated flow to a radiatively dominated one happens at a
luminosity of ~2e36 erg/s. The luminosity dependency of the size of the black
body emission region is found to be $r_{BB} \propto L_X^{0.39\pm0.02}$. This
suggests that either matter accreting onto the neutron star hosted in RX
J0440.9+4431 penetrates through closed magnetic field lines at the border of
the compact object magnetosphere or that the structure of the neutron star
magnetic field is more complicated than a simple dipole close to the surface
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:49:57 GMT""}]","2015-06-15"
"1303.7088","Jeremy Shears","Jeremy Shears, Ian Miller, Roger Pickard, Richard Sabo","Superoutbursts and grazing eclipses in the dwarf nova V1227 Herculis","Accepted for publication in the Journal of the British Astronomical
  Association. 17 pages, 6 figures. The target star of this paper, SDSS
  J165359.06+201010.4, has now received the official name of V1227 Her in the
  General Catalogue of Variable Stars. This version contains the new name",,,,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present photometry obtained during the 2012 May and September outbursts of
the frequently outbursting dwarf nova, V1227 Her. Superhumps were present in
both cases with a peak-to peak amplitude of up to 0.28 mag, showing these
events to be superoutbursts. We show for the first time that the system
undergoes small eclipses with a depth of up to 0.08 mag, lasting 11 to 14 min,
which are likely to be grazing eclipses of the accretion disc. The September
outburst was the better observed of the two and lasted at least 14 days with an
outburst amplitude of approximately 4 magnitudes. The mean superhump period was
Psh = 0.065103(20) d. Analysis of eclipse times of minimum gave an orbital
period Porb = 0.064419(26) d, although there is some ambiguity due to the
relatively short time over which the eclipses were observed. The fractional
superhump period excess, epsilon, was 0.0106(7).
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:54:45 GMT""},{""version"":""v2"",""created"":""Wed, 10 Apr 2013 13:50:55 GMT""}]","2013-04-11"
"1303.7089","Thomas Hausberger","Thomas Hausberger (I3M)","On the concept of (homo)morphism : a key notion in the learning of
  abstract algebra",,,,,"math.HO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This article is dedicated to the investigation of difficulties involved in
the understanding of the homomorphism concept. It doesn't restrict to
group-theory but on the contrary raises the issue of developing teaching
strategies aiming at gaining access to structuralist thinking. Emphasis is put
on epistemological analysis and its interaction with didactics in an attempt to
make Abstract Algebra more accessible.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:56:39 GMT""}]","2013-03-29"
"1303.7090","Nicolas Durrande","Nicolas Durrande (Mines Saint-\'Etienne MSE, LIMOS), James Hensman,
  Magnus Rattray, Neil D. Lawrence","Gaussian process models for periodicity detection","in PeerJ Computer Science, 2016",,,,"math.ST stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider the problem of detecting and quantifying the periodic component
of a function given noise-corrupted observations of a limited number of
input/output tuples. Our approach is based on Gaussian process regression which
provides a flexible non-parametric framework for modelling periodic data. We
introduce a novel decomposition of the covariance function as the sum of
periodic and aperiodic kernels. This decomposition allows for the creation of
sub-models which capture the periodic nature of the signal and its complement.
To quantify the periodicity of the signal, we derive a periodicity ratio which
reflects the uncertainty in the fitted sub-models. Although the method can be
applied to many kernels, we give a special emphasis to the Mat\'ern family,
from the expression of the reproducing kernel Hilbert space inner product to
the implementation of the associated periodic kernels in a Gaussian process
toolkit. The proposed method is illustrated by considering the detection of
periodically expressed genes in the arabidopsis genome.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:59:18 GMT""},{""version"":""v2"",""created"":""Wed, 17 Feb 2016 19:39:24 GMT""},{""version"":""v3"",""created"":""Fri, 19 Aug 2016 15:59:55 GMT""}]","2016-08-22"
"1303.7091","Colin Mrozinski","Colin Mrozinski","Quantum automorphism groups and SO(3)-deformations","Comments are welcome",,,,"math.QA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that any compact quantum group having the same fusion rules as the
ones of $SO(3)$ is the quantum automorphism group of a pair $(A, \varphi)$,
where $A$ is a finite dimensional $C^*$-algebra endowed with a homogeneous
faithful state. We also study the representation category of the quantum
automorphism group of $(A, \varphi)$ when $\varphi$ is not necessarily
positive, generalizing some known results, and we discuss the possibility of
classifying the cosemisimple (not necessarily compact) Hopf algebras whose
corepresentation semi-ring is isomorphic to that of $SO(3)$.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:01:16 GMT""},{""version"":""v2"",""created"":""Mon, 6 Jan 2014 19:58:30 GMT""}]","2014-01-07"
"1303.7092","Eric Gautier","Eric Gautier (CREST, ENSAE), Alexandre Tsybakov (CREST, ENSAE)","Pivotal estimation in high-dimensional regression via linear programming",,,,,"math.ST q-fin.ST stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose a new method of estimation in high-dimensional linear regression
model. It allows for very weak distributional assumptions including
heteroscedasticity, and does not require the knowledge of the variance of
random errors. The method is based on linear programming only, so that its
numerical implementation is faster than for previously known techniques using
conic programs, and it allows one to deal with higher dimensional models. We
provide upper bounds for estimation and prediction errors of the proposed
estimator showing that it achieves the same rate as in the more restrictive
situation of fixed design and i.i.d. Gaussian errors with known variance.
Following Gautier and Tsybakov (2011), we obtain the results under weaker
sensitivity assumptions than the restricted eigenvalue or assimilated
conditions.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:01:46 GMT""},{""version"":""v2"",""created"":""Mon, 15 Apr 2013 09:45:31 GMT""}]","2013-04-16"
"1303.7093","Aravind Kota Gopalakrishna","Aravind Kota Gopalakrishna, Tanir Ozcelebi, Antonio Liotta, Johan J.
  Lukkien","Relevance As a Metric for Evaluating Machine Learning Algorithms","To Appear at International Conference on Machine Learning and Data
  Mining (MLDM 2013), 14 pages, 6 figures",,"10.1007/978-3-642-39712-7_15",,"stat.ML cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In machine learning, the choice of a learning algorithm that is suitable for
the application domain is critical. The performance metric used to compare
different algorithms must also reflect the concerns of users in the application
domain under consideration. In this work, we propose a novel probability-based
performance metric called Relevance Score for evaluating supervised learning
algorithms. We evaluate the proposed metric through empirical analysis on a
dataset gathered from an intelligent lighting pilot installation. In comparison
to the commonly used Classification Accuracy metric, the Relevance Score proves
to be more appropriate for a certain class of applications.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:01:53 GMT""},{""version"":""v2"",""created"":""Fri, 5 Apr 2013 19:12:06 GMT""},{""version"":""v3"",""created"":""Mon, 8 Apr 2013 14:26:49 GMT""}]","2013-07-19"
"1303.7094","Kevin Wildrick","Zolt\'an M. Balogh, Jeremy T. Tyson, Kevin Wildrick","Frequency of Sobolev dimension distortion of horizontal subgroups of
  Heisenberg groups","22 pages, 3 figures",,,,"math.MG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the behavior of Sobolev mappings defined on the Heisenberg groups
with respect to a foliation by left cosets of a horizontal homogeneous
subgroup. We quantitatively estimate, in terms of Euclidean Hausdorff
dimension, the size of the set of cosets that are mapped onto sets of high
dimension. The proof of our main result combines ideas of Gehring and Mostow
about the absolute continuity of quasiconformal mappings with Mattila's
projection and slicing machinery.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:02:54 GMT""}]","2013-03-29"
"1303.7095","Annalisa Fasolino","L.W. van Heeringen, G. A. de Wijs, A. McCollam, J.C. Maan, and A.
  Fasolino","k.p subband structure of the LaAlO3/SrTiO3 interface","8 pages, 7 figures","Phys. Rev. B 88,205140 (2013)","10.1103/PhysRevB.88.205140",,"cond-mat.mes-hall cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Heterostructures made of transition metal oxides are new tailor-made
materials which are attracting much attention. We have constructed a 6-band k.p
Hamiltonian and used it within the envelope function method to calculate the
subband structure of a variety of LaAlO3/SrTiO3 heterostructures. By use of
density functional calculations, we determine the k.p parameters describing the
conduction band edge of SrTiO3: the three effective mass parameters, L=0.6104
eV AA^2, M=9.73 eV AA^2, N=-1.616 eV AA^2, the spin orbit splitting
Delta_SO=28.5 meV and the low temperature tetragonal distortion energy
splitting Delta_T=2.1 meV. For confined systems we find strongly anisotropic
non-parabolic subbands. As an application we calculate bands, density of states
and magnetic energy levels and compare the results to Shubnikov-de Haas quantum
oscillations observed in high magnetic fields. For typical heterostructures we
find that electric field strength at the interface of F = 0.1 meV/AA for a
carrier density of 7.2 10^{12} cm^-2 results in a subband structure that is
similar to experimental results.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:04:01 GMT""},{""version"":""v2"",""created"":""Wed, 8 Jan 2014 15:28:37 GMT""}]","2014-01-09"
"1303.7096","Martin Deraux","Martin Deraux (IF), Elisha Falbel (IMJ, INRIA Paris-Rocquencourt)","Complex hyperbolic geometry of the figure eight knot",,"Geom. Topol. 19 (2015) 237-293","10.2140/gt.2015.19.237",,"math.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that the figure eight knot complement admits a uniformizable
spherical CR structure, i.e. it occurs as the manifold at infinity of a complex
hyperbolic orbifold. The uniformization is unique provided we require the
peripheral subgroups to have unipotent holonomy.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:06:39 GMT""},{""version"":""v2"",""created"":""Tue, 11 Feb 2014 10:51:44 GMT""}]","2015-05-27"
"1303.7097","Andrea Cavagna","Alessandro Attanasi, Andrea Cavagna, Lorenzo Del Castello, Irene
  Giardina, Tomas S. Grigera, Asja Jeli\'c, Stefania Melillo, Leonardo Parisi,
  Oliver Pohl, Edward Shen, Massimiliano Viale","Superfluid transport of information in turning flocks of starlings",,"Nature Physics 10 (9), 691-696 (2014)","10.1038/nphys3035",,"cond-mat.stat-mech physics.bio-ph q-bio.PE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Collective decision-making in biological systems requires all individuals in
the group to go through a behavioural change of state. During this transition,
the efficiency of information transport is a key factor to prevent cohesion
loss and preserve robustness. The precise mechanism by which natural groups
achieve such efficiency, though, is currently not fully understood. Here, we
present an experimental study of starling flocks performing collective turns in
the field. We find that the information to change direction propagates across
the flock linearly in time with negligible attenuation, hence keeping group
decoherence to a minimum. This result contrasts with current theories of
collective motion, which predict a slower and dissipative transport of
directional information. We propose a novel theory whose cornerstone is the
existence of a conserved spin current generated by the gauge symmetry of the
system. The theory turns out to be mathematically identical to that of
superfluid transport in liquid helium and it explains the dissipationless
propagating mode observed in turning flocks. Superfluidity also provides a
quantitative expression for the speed of propagation of the information,
according to which transport must be swifter the stronger the group's
orientational order. This prediction is verified by the data. We argue that the
link between strong order and efficient decision-making required by
superfluidity may be the adaptive drive for the high degree of behavioural
polarization observed in many living groups. The mathematical equivalence
between superfluid liquids and turning flocks is a compelling demonstration of
the far-reaching consequences of symmetry and conservation laws across
different natural systems.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:14:09 GMT""}]","2014-10-10"
"1303.7098","Oscar Lorente-Esp\'in","Oscar Lorente-Esp\'in","Emission of fermions in little string theory","17 pages, no figures, minor corrections","Phys. Rev. D. 87, 064016 (2013)","10.1103/PhysRevD.87.064016",,"hep-th gr-qc","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is well-known that little string theory (LST) black holes radiate a purely
thermal spectrum of scalar particles. This theory lives in a Hagedorn phase
with a fixed Hagedorn temperature that does not depend on its mass. Therefore,
the theory keeps a thermal profile even taking into account self-gravitating
effects and the back-reaction of the metric. This has implications concerning
the information loss paradox; one would not be able to recover any information
from the LST black hole since the emission of scalar particles is totally
uncorrelated. Several studies of the emission spectrum in LST concern scalar
fields; it is our aim in this work to extend the study to the emission of
fermions in order to verify that the most relevant conclusion for the scalar
field remains valid for the fermions fields. Thus, we have calculated the
emission probability, the flux, and also the greybody factor corresponding to a
fermion field in LST background.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:15:31 GMT""},{""version"":""v2"",""created"":""Mon, 1 Apr 2013 20:12:19 GMT""}]","2013-04-03"
"1303.7099","Ludovic Marquis","Ludovic Marquis (IRMAR)","Around groups in Hilbert Geometry","~60 pages",,,,"math.GT math.GR math.MG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This is survey about action of group on Hilbert geometry. It will be a
chapter of the ""Handbook of Hilbert geometry"" edited by G. Besson, M. Troyanov
and A. Papadopoulos.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:16:57 GMT""},{""version"":""v2"",""created"":""Tue, 22 Apr 2014 13:16:26 GMT""}]","2014-04-23"
"1303.7100","Bertrand Lods","Luisa Arlotti, Bertrand Lods, Mustapha Mokhtar-Kharroubi
  (LM-Besan\c{c}on)","Non-autonomous Honesty theory in abstract state spaces with applications
  to linear kinetic equations",,,,,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We provide a honesty theory of substochastic evolution families in real
abstract state space, extending to an non-autonomous setting the result
obtained for $C_0$-semigroups in our recent contribution \textit{[On perturbed
substochastic semigroups in abstract state spaces, \textit{Z. Anal. Anwend.}
\textbf{30}, 457--495, 2011]}. The link with the honesty theory of perturbed
substochastic semigroups is established. Several applications to non-autonomous
linear kinetic equations (linear Boltzmann equation and fragmentation equation)
are provided.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:19:43 GMT""}]","2013-03-29"
"1303.7101","Paul Busch","J.C.G. Biniok and P. Busch","Multi-slit interferometry and commuting functions of position and
  momentum","Version 2 contains minor corrections and improvements and a new
  reference to to relevant work on modular variables by Y. Aharonov et al","Physical Review A 87 (2013) 062116 (7 pp.)","10.1103/PhysRevA.87.062116",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In a recent, modified double-pinhole diffraction experiment the existence of
an interference pattern was established indirectly along with a near-perfect
imaging of the double pinhole. Our theoretical analysis shows that the
experiment constitutes a preparation of a quantum state that is, to a good
approximation, a joint eigenstate of commuting functions of position and
momentum. Gaining information about the momentum distribution by means of the
particular experimental setup is thus possible with negligible impact on the
position distribution. Furthermore, we construct explicitly a class of states
simultaneously localised on periodic sets in position and momentum space, which
are therefore eigenstates of the observables being measured jointly (to a good
approximation) in multi-slit interferometry. Finally, we show that with an
appropriate change of settings the experiment demonstrates the mutual
disturbance of position and momentum measurements.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:22:08 GMT""},{""version"":""v2"",""created"":""Sun, 16 Jun 2013 10:18:06 GMT""}]","2013-06-27"
"1303.7102","Bogumi{\l}a \'Swie\.zewska","Maria Krawczyk, Dorota Sokolowska, Bogumila Swiezewska","2HDM with Z_2 symmetry in light of new LHC data","To appear in Proc. of Discrete 2012 (talk by M. Krawczyk), 7 pages, 4
  figures, v2: corrected Fig. 1b","2013 J. Phys.: Conf. Ser. 447 012050","10.1088/1742-6596/447/1/012050","IFT-1/2013","hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Properties of the Z_2-symmetric Two Higgs Doublet Models (2HDM) are discussed
and confronted with new LHC data for a 125 GeV Higgs particle. The particle
discovered at LHC in 2012 has properties expected for it in the Standard Model
(SM), with a possible enhancement in the two-photon channel. SM-like Higgs
scenarios can be realized in the Two Higgs Doublet Models with Z_2 (D) symmetry
within the normal Mixed Model (with scalar sector as in MSSM) and the Inert
Doublet Model (IDM), where a good Dark Matter (DM) candidate is present. Here
we discuss both of the models.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:35:08 GMT""},{""version"":""v2"",""created"":""Sat, 13 Jul 2013 14:00:05 GMT""}]","2013-09-13"
"1303.7103","Federico Penna","Federico Penna, Slawomir Stanczak","Decentralized Eigenvalue Algorithms for Distributed Signal Detection in
  Cognitive Networks","Submitted to IEEE JSAC Cognitive Radio Series",,,,"cs.DC cs.MA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we derive and analyze two algorithms -- referred to as
decentralized power method (DPM) and decentralized Lanczos algorithm (DLA) --
for distributed computation of one (the largest) or multiple eigenvalues of a
sample covariance matrix over a wireless network. The proposed algorithms,
based on sequential average consensus steps for computations of matrix-vector
products and inner vector products, are first shown to be equivalent to their
centralized counterparts in the case of exact distributed consensus. Then,
closed-form expressions of the error introduced by non-ideal consensus are
derived for both algorithms. The error of the DPM is shown to vanish
asymptotically under given conditions on the sequence of consensus errors.
Finally, we consider applications to spectrum sensing in cognitive radio
networks, and we show that virtually all eigenvalue-based tests proposed in the
literature can be implemented in a distributed setting using either the DPM or
the DLA. Simulation results are presented that validate the effectiveness of
the proposed algorithms in conditions of practical interest (large-scale
networks, small number of samples, and limited number of iterations).
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:42:28 GMT""}]","2013-03-29"
"1303.7104","Denis Khomitsky","D.V. Khomitsky, A.A. Chubanov","Edge states and topological properties of electrons on the bismuth on
  silicon surface with giant spin-orbit coupling","9 pages, 3 figures, published version","Journal of Experimental and Theoretical Physics, 2014, vol. 118,
  No. 3, pp. 457-466","10.1134/S1063776114020101",,"cond-mat.mes-hall quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We derive a model of localized edge states in the finite width strip for
two-dimensional electron gas formed in the hybrid system of bismuth monolayer
deposited on the silicon interface and described by the nearly-free electron
model with giant spin-orbit splitting. The edge states have the energy
dispersion in the bulk energy gap with the Dirac-like linear dependence on the
quasimomentum and the spin polarization coupled to the direction of
propagation, demonstrating the properties of topological insulator. The
topological stability of edge states is confirmed by the calculations of the
$Z_2$ invariant taken from the structure of the Pfaffian for the time reversal
operator for the filled bulk bands in the surface Brillouin zone which is shown
to have a stable number of zeros with the variations of material parameters.
The proposed properties of the edge states may support future advances in
experimental and technological applications of this new material in
nanoelectronics and spintronics.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:45:11 GMT""},{""version"":""v2"",""created"":""Tue, 2 Apr 2013 20:59:11 GMT""},{""version"":""v3"",""created"":""Tue, 4 Feb 2014 15:26:48 GMT""}]","2014-04-09"
"1303.7105","Robert van Leeuwen","Robert van Leeuwen and Gianluca Stefanucci","Equilibrium and nonequilibrium many-body perturbation theory: a unified
  framework based on the Martin-Schwinger hierarchy","17 pages, 6 figures","Journal of Physics: Conference Series 427, 012001 (2013)","10.1088/1742-6596/427/1/012001",,"cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a unified framework for equilibrium and nonequilibrium many-body
perturbation theory. The most general nonequilibrium many-body theory valid for
general initial states is based on a time-contour originally introduced by
Konstantinov and Perel'. The various other well-known formalisms of Keldysh,
Matsubara and the zero-temperature formalism are then derived as special cases
that arise under different assumptions. We further present a single simple
proof of Wick's theorem that is at the same time valid in all these flavors of
many-body theory. It arises simply as a solution of the equations of the
Martin-Schwinger hierarchy for the noninteracting many-particle Green's
function with appropriate boundary conditions. We further discuss a generalized
Wick theorem for general initial states on the Keldysh contour and derive how
the formalisms based on the Keldysh and Konstantinov-Perel'-contours are
related for the case of general initial states.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:47:43 GMT""}]","2013-03-29"
"1303.7106","Enrique Herrero","E. Herrero, A. F. Lanza, I. Ribas, C. Jordi, J. C. Morales","Photospheric activity, rotation and magnetic interaction in LHS 6343 A","14 pages, 16 figures",,"10.1051/0004-6361/201220518",,"astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Context. The Kepler mission has recently discovered a brown dwarf companion
transiting one member of the M4V+M5V visual binary system LHS 6343 AB with an
orbital period of 12.71 days. Aims. The particular interest of this transiting
system lies in the synchronicity between the transits of the brown dwarf C
component and the main modulation observed in the light curve, which is assumed
to be caused by rotating starspots on the A component. We model the activity of
this star by deriving maps of the active regions that allow us to study stellar
rotation and the possible interaction with the brown dwarf companion. Methods.
An average transit profile was derived, and the photometric perturbations due
to spots occulted during transits are removed to derive more precise transit
parameters. We applied a maximum entropy spot model to fit the out-of-transit
optical modulation as observed by Kepler during an uninterrupted interval of
500 days. It assumes that stellar active regions consist of cool spots and
bright faculae whose visibility is modulated by stellar rotation. Results.
Thanks to the extended photometric time series, we refine the determination of
the transit parameters and find evidence of spots that are occulted by the
brown dwarf during its transits. The modelling of the out-of-transit light
curve of LHS 6343 A reveals several starspots rotating with a slightly longer
period than the orbital period of the brown dwarf, i.e., 13.13 +- 0.02 days. No
signature attributable to differential rotation is observed. We find evidence
of a persistent active longitude on the M dwarf preceding the sub- companion
point by 100 deg and lasting for at least 500 days. This can be relevant for
understanding how magnetic interaction works in low-mass binary and star-planet
systems.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:54:04 GMT""}]","2015-06-15"
"1303.7107","Yuriy Bunkov","Yury Bunkov","Direct Majorana quasiparticles heat capacity observation by $^3$He Dark
  Matter detector","6 pages, 3 figures",,,,"physics.ins-det astro-ph.CO hep-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Majorana fermion: fermion that is its own antiparticle, was predicted by
Majorana in 1937. No fundamental particles are known to be Majorana fermions,
although there are speculations that the neutrino is one. Many proposed
theories assumes that the mysterious 'dark matter', which forms the greatest
part of the universe, is composed of Majorana fermions. Even Majorana does not
yet observed as a stable particle, its can be also exist as a quasiparticle in
the edge of topological isolators. Here we reports the Dark Matter bolometer
time constant deviation which is the result of additional Majorana heat
capacity.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:56:49 GMT""}]","2013-03-29"
"1303.7108","Boris Chorny","Boris Chorny","A classification of small homotopy functors from spectra to spectra","Final version, to appear in Fund. Math",,,,"math.AT math.CT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show that every small homotopy functor from spectra to spectra is weakly
equivalent to a filtered colimit of representable functors represented in
cofibrant spectra. Moreover, we present this classification as a Quillen
equivalence of the category of small functors from spectra to spectra equipped
with the homotopy model structure and the opposite of the pro-category of
spectra with the strict model structure.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:58:38 GMT""},{""version"":""v2"",""created"":""Tue, 3 Nov 2015 16:14:18 GMT""}]","2015-11-04"
"1303.7109","Tomomi Kouzu","Tomomi Kouzu, Makoto S. Tashiro, Yukikatsu Terada, Shin'ya Yamada, Aya
  Bamba, Teruaki Enoto, Koji Mori, Yasushi Fukazawa, Kazuo Makishima","Spectral Variation of the Hard X-ray Emission from the Crab Nebula with
  the Suzaku Hard X-ray Detector","18 pages, 10 figures, PASJ accepted",,"10.1093/pasj/65.4.74",,"astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Crab Nebula is one of the brightest and most stable sources in the X-ray
sky. Year-scale flux variation from the object was recently revealed in the
hard X-ray band by four satellites. This marked the first detection of
year-scale variability from pulsar wind nebulae in the hard X-ray band. The
Crab Nebula has been observed at least once a year for calibration purposes
with the Suzaku Hard X-ray Detector (HXD) since its launch in 2005. In order to
investigate possible spectral changes as well as flux variation, the archival
data of the HXD were analyzed. The flux variation reported by other instruments
was confirmed in the 25 -- 100 keV band by the HXD in a few percent level, but
flux above 100 keV did not follow the trend in variation below 100 keV. The
hardness ratios produced utilizing the PIN and GSO sensors installed in the HXD
exhibit significant scattering, thereby indicating spectral variations in the
hard X-ray. The spectral changes are quantified by spectral fitting with a
broken power-law model. The difference between the two photon indexes of the
broken power-law model in harder and softer energy bands is in the range of <
2.54. Taking into account flux variation of 6.3% and spectral variation
time-scale of a few days, multi components of the broken power-law-shaped
synchrotron emission with different cooling times are suggested.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:09:22 GMT""}]","2015-06-15"
"1303.7110","Tuvi Etzion","Tuvi Etzion","The q-Analog of the Middle Levels Problem","12 pages",,,,"math.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The well-known middle levels problem is to find a Hammiltonian cycle in the
graph induced from the binary Hamming graph $\cH_2(2k+1)$ by the words of
weight $k$ or $k+1$. In this paper we define the $q$-analog of the middle
levels problem. Let $n=2k+1$ and let $q$ be a power of a prime number. Consider
the set of $(k+1)$-dimensional subspaces and the set of $k$-dimensional
subspaces of $\F_q^n$. Can these subspaces be ordered in a way that for any two
adjacent subspaces $X$ and $Y$, either $X \subset Y$ or $Y \subset X$? A
construction method which yields many Hamiltonian cycles for any given $q$ and
$k=2$ is presented.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:21:24 GMT""},{""version"":""v2"",""created"":""Fri, 27 Sep 2013 04:59:04 GMT""},{""version"":""v3"",""created"":""Mon, 10 Mar 2014 09:17:35 GMT""},{""version"":""v4"",""created"":""Wed, 12 Mar 2014 14:51:36 GMT""}]","2014-03-13"
"1303.7111","Tobias Baier","Gowrishankar Seshadri and Tobias Baier","Effect of Electro-Osmotic Flow on Energy Conversion on Superhydrophobic
  Surfaces",,,"10.1063/1.4802044",,"physics.flu-dyn","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It has been suggested that superhydrophobic surfaces, due to the presence of
a no-shear zone, can greatly enhance transport of surface charges, leading to a
considerable increase in the streaming potential. This could find potential use
in micro-energy harvesting devices. In this paper, we show using analytical and
numerical methods, that when a streaming potential is generated in such
superhydrophobic geometries, the reverse electro-osmotic flow and hence current
generated by this, is significant. A decrease in streaming potential compared
to what was earlier predicted is expected. We also show that, due to the
electro-osmotic streaming-current, a saturation in both the power extracted and
efficiency of energy conversion is achieved in such systems for large values of
the free surface charge densities. Nevertheless, under realistic conditions,
such microstructured devices with superhydrophobic surfaces have the potential
to even reach energy conversion efficiencies only achieved in nanostructured
devices so far.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:37:25 GMT""}]","2015-06-15"
"1303.7112","Marvin Rose","Marvin Rose, Clive Tadhunter, Joanna Holt, Javier Rodr\'iguez Zaur\'in","On the nature of the red, 2MASS selected AGN in the local Universe I: an
  optical spectroscopic study","30 pages, 22 figures; accepted for publication in MNRAS",,"10.1093/mnras/stt564",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present optical spectra for a representative sample of 27 nearby (z < 0.2)
2MASS-selected AGN with red near-IR colours (J-K 2.0). The spectra were taken
with the ISIS spectrograph on the WHT with the aim of determining the nature of
the red 2MASS AGN, in particular whether they are young quasars obscured by
their natal cocoon of gas and dust. We compare our findings with those obtained
for comparison samples of PG quasars and unobscured type 1 AGN.
  The spectra show a remarkable variety, including moderately reddened type 1
objects (45%), type 1 objects that appear similar to traditional UV/optical
selected AGN (11%), narrow-line Seyfert 1 AGN (15%), type 2 AGN (22%) and
HII/composite objects (7%). The high Balmer decrements that we measure in many
of the type 1 objects are consistent with their red J-KS colours being due to
moderate levels of dust extinction (0.2 < E(B-V) < 1.2). However, we measure
only modest velocity shifts and widths for the broader [OIII]{\lambda}5007
emission line components that are similar to those measured in the comparison
samples. This suggests that the outflows in the red 2MASS objects are not
unusual compared with those of optical/UV selected AGN of similar luminosity.
In addition, the Eddington ratios for the 2MASS sample are relatively modest.
  Overall, based on their optical spectra, we find no clear evidence that the
population of red, 2MASS selected AGN at low redshifts represents young
quasars. Most plausibly, these objects are normal type 1 AGN that are
moderately obscured by material in the outer layers of the circum-nuclear tori
or in the disks of the host galaxies.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:40:22 GMT""}]","2015-06-15"
"1303.7113","Suman Sinha Dr.","Pradipta Kumar Mandal and Suman Sinha","Characterization of kinetic coarsening in a random-field Ising model","Double-column, 4 pages, 6 figures","Phys. Rev. E 89, 042144 (2014)","10.1103/PhysRevE.89.042144",,"cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We report a study of nonequilibrium relaxation in a two-dimensional random
field Ising model at a nonzero temperature. We attempt to observe the
coarsening from a different perspective with a particular focus on three
dynamical quantities that characterize the kinetic coarsening. We provide a
simple generalized scaling relation of coarsening supported by numerical
results. The excellent data collapse of the dynamical quantities justifies our
proposition. The scaling relation corroborates the recent observation that the
average linear domain size satisfies different scaling behavior in different
time regimes.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:42:24 GMT""},{""version"":""v2"",""created"":""Mon, 7 Jul 2014 12:56:56 GMT""}]","2014-07-08"
"1303.7114","Andreas Steenpass","Magdaleen S. Marais and Andreas Steenpass","The Classification of Real Singularities Using SINGULAR. Part I:
  Splitting Lemma and Simple Singularities","12 pages, 1 table","J. Symb. Comput. 68 (2015), 61-71","10.1016/j.jsc.2014.08.007",,"math.AG math.AC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present algorithms to classify isolated hypersurface singularities over
the real numbers according to the classification by V.I. Arnold (Arnold et al.,
1985). This first part covers the splitting lemma and the simple singularities;
a second and a third part will be devoted to the unimodal singularities up to
corank 2. All algorithms are implemented in the SINGULAR library
realclassify.lib (Marais and Steenpass, 2012).
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:48:25 GMT""},{""version"":""v2"",""created"":""Tue, 5 Nov 2013 11:26:32 GMT""},{""version"":""v3"",""created"":""Fri, 13 Feb 2015 10:04:27 GMT""}]","2016-01-15"
"1303.7115","Dmitry Namiot","Dmitry Namiot, Manfred Schneps-Schneppe","Smart Cities Software from the developer's point of view","8 pages, submitted to 6-th Conference Applied Information and
  Communication Technology AICT2013",,,,"cs.CY cs.NI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The paper discusses the current state and development proposals for Smart
Cities and Future Internet projects. Definitions of a Smart City can vary but
usually tend to suggest the use of innovative Info-Communication technologies
such as the Internet of Things and Web 2.0 to deliver more effective and
efficient public services that improve living and working conditions and create
more sustainable urban environments. Our goal is to analyze the current
proposals from the developer's point of view, highlight the really new
elements, the positions borrowed from the existing tools as well as propose
some new extensions. We would like to discuss the possible extensions for the
existing proposals and describe add-ons that, by our opinion, let keep the
future research inline with the modern approaches in the web development
domain.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:53:22 GMT""}]","2013-03-29"
"1303.7116","Johannes Knebel","Johannes Knebel, Torben Kr\""uger, Markus F. Weber, Erwin Frey","Coexistence and Survival in Conservative Lotka-Volterra Networks","5 pages, 3 figures","Phys. Rev. Lett. 110, 168106 (2013)","10.1103/PhysRevLett.110.168106","LMU-ASC 63/12","q-bio.PE cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Analyzing coexistence and survival scenarios of Lotka-Volterra (LV) networks
in which the total biomass is conserved is of vital importance for the
characterization of long-term dynamics of ecological communities. Here, we
introduce a classification scheme for coexistence scenarios in these
conservative LV models and quantify the extinction process by employing the
Pfaffian of the network's interaction matrix. We illustrate our findings on
global stability properties for general systems of four and five species and
find a generalized scaling law for the extinction time.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:57:46 GMT""}]","2013-04-26"
"1303.7117","Brittany Terese Fasy","Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, Larry
  Wasserman, Sivaraman Balakrishnan, Aarti Singh","Confidence sets for persistence diagrams","Published in at http://dx.doi.org/10.1214/14-AOS1252 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Annals of Statistics 2014, Vol. 42, No. 6, 2301-2339","10.1214/14-AOS1252","IMS-AOS-AOS1252","math.ST cs.CG cs.LG stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Persistent homology is a method for probing topological properties of point
clouds and functions. The method involves tracking the birth and death of
topological features (2000) as one varies a tuning parameter. Features with
short lifetimes are informally considered to be ""topological noise,"" and those
with a long lifetime are considered to be ""topological signal."" In this paper,
we bring some statistical ideas to persistent homology. In particular, we
derive confidence sets that allow us to separate topological signal from
topological noise.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:59:00 GMT""},{""version"":""v2"",""created"":""Fri, 7 Feb 2014 16:36:57 GMT""},{""version"":""v3"",""created"":""Thu, 20 Nov 2014 08:16:51 GMT""}]","2014-11-21"
"1303.7118","Karen Masters","Karen L. Masters (ICG Portsmouth)","A Zoo of Galaxies","15 pages, 8 figures. Proceedings of Invited Discourse at the 27th IAU
  General Assembly, in Beijing, China, August 2012. To appear in Highlights of
  Astronomy, Volume 16",,,,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We live in a universe filled with galaxies with an amazing variety of sizes
and shapes. One of the biggest challenges for astronomers working in this field
is to understand how all these types relate to each other in the background of
an expanding universe. Modern astronomical surveys (like the Sloan Digital Sky
Survey) have revolutionised this field of astronomy, by providing vast numbers
of galaxies to study. The sheer size of the these databases made traditional
visual classification of the types galaxies impossible and in 2007 inspired the
Galaxy Zoo project (www.galaxyzoo.org); starting the largest ever scientific
collaboration by asking members of the public to help classify galaxies by type
and shape. Galaxy Zoo has since shown itself, in a series of now more than 30
scientific papers, to be a fantastic database for the study of galaxy
evolution. In this Invited Discourse I spoke a little about the historical
background of our understanding of what galaxies are, of galaxy classification,
about our modern view of galaxies in the era of large surveys. I finish with
showcasing some of the contributions galaxy classifications from the Galaxy Zoo
project are making to our understanding of galaxy evolution.
  This publication has been made possible by the participation of more than
200,000 volunteers in the Galaxy Zoo project. Their contributions are
individually acknowledged at http://www.galaxyzoo.org/volunteers. KLM
acknowledges funding from the Peter and Patricia Gruber Foundation as the 2008
Peter and Patricia Gruber Foundation IAU Fellow, and from a 2010 Leverhulme
Trust Early Career Fellowship, as well as support from the Royal Astronomical
Society to attend the 28th GA of the IAU.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:14:26 GMT""}]","2013-03-29"
"1303.7119","Marta Galicka","S. Safaei, P. Kacman, and R. Buczko","The topological-crystalline-insulator (Pb,Sn)Te - surface states and
  their spin-polarization","8 pages, 7 figures, submitted to Phys. Rev. B",,"10.1103/PhysRevB.88.045305",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Using a tight-binding approach we study theoretically the nature of surface
states in Pb0.4Sn0.6Te - the newly discovered
topological-crystalline-insulator. Apart from the studied before (001) surface
states, two other surface families, {011} and {111}, in which the mirror
symmetry of the crystal's rock-salt structure plays the same role in
topological protection, are considered. Our calculations show that while in
(111) surface states of (Pb,Sn)Te four single topologically protected
Dirac-cones should appear, for the (110) surface states the protection is
lifted for two L points. In this case, instead of the Dirac points energy gaps
occur in the surface states, due to the interaction between the two L valleys.
In all studied cases a chiral spin texture is obtained.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:16:47 GMT""}]","2013-12-05"
"1303.7120","Sen Zhang","Sen Zhang","Pre-acceleration from Landau-Lifshitz Series","16 pages",,"10.1093/ptep/ptt099","OIQP-13-05","hep-th math-ph math.MP physics.class-ph physics.plasm-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Landau-Lifshitz equation is considered as an approximation of the
Abraham-Lorentz-Dirac equation. It is derived from the Abraham-Lorentz-Dirac
equation by treating radiation reaction terms as a perturbation. However, while
the Abraham-Lorentz-Dirac equation has pathological solutions of
pre-acceleration and runaway, the Landau-Lifshitz equation and its finite
higher order extensions are free of these problems. So it seems mysterious that
the property of solutions of these two equations is so different. In this paper
we show that the problems of pre-acceleration and runaway appear when one
consider a series of all-order perturbation which we call it the
Landau-Lifshitz series. We show that the Landau-Lifshitz series diverges in
general. Hence a resummation is necessary to obtain a well-defined solution
from the Landau-Lifshitz series. This resummation leads the pre-accelerating
and the runaway solutions. The analysis is focusing on the non-relativistic
case, but we can extend the results obtained here to relativistic case at least
in one dimension.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:39:08 GMT""}]","2014-05-07"
"1303.7121","Andrii Neronov","R.Durrer, A.Neronov","Cosmological Magnetic Fields: Their Generation, Evolution and
  Observation","to appear in Astronomy & Astrophysics Review",,"10.1007/s00159-013-0062-7",,"astro-ph.CO astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We review the possible mechanisms for the generation of cosmological magnetic
fields, discuss their evolution in an expanding Universe filled with the cosmic
plasma and provide a critical review of the literature on the subject. We put
special emphasis on the prospects for observational tests of the proposed
cosmological magnetogenesis scenarios using radio and gamma-ray astronomy and
ultra high energy cosmic rays. We argue that primordial magnetic fields are
observationally testable. They lead to magnetic fields in the intergalactic
medium with magnetic field strength and correlation length in a well defined
range. We also state the unsolved questions in this fascinating open problem of
cosmology and propose future observations to address them.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:42:03 GMT""},{""version"":""v2"",""created"":""Tue, 7 May 2013 09:57:37 GMT""}]","2015-06-15"
"1303.7122","Fabi\'an Riquelme","Andreas Polym\'eris and Fabi\'an Riquelme","On the Complexity of the Decisive Problem in Simple, Regular and
  Weighted Games","12 pages",,,,"cs.GT cs.CC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the computational complexity of an important property of simple,
regular and weighted games, which is decisiveness. We show that this concept
can naturally be represented in the context of hypergraph theory, and that
decisiveness can be decided for simple games in quasi-polynomial time, and for
regular and weighted games in polynomial time. The strongness condition poses
the main difficulties, while properness reduces the complexity of the problem,
especially if it is amplified by regularity. On the other hand, regularity also
allows to specify the problem instances much more economically, implying a
reconsideration of the corresponding complexity measure that, as we prove, has
important structural as well as algorithmic consequences.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:51:25 GMT""},{""version"":""v2"",""created"":""Tue, 9 Jul 2013 11:44:39 GMT""}]","2013-07-10"
"1303.7123","Wladimir  Neves","Xavier Carvajal, Wladimir Neves","Persistence property in weighted Sobolev spaces for nonlinear dispersive
  equations","25 pages",,,,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We generalize the Abstract Interpolation Lemma proved by the authors in [2].
Using this extension, we show in a more general context, the persistence
property for the generalized Korteweg-de Vries equation, see (1.2), in the
weighted Sobolev space with low regularity in the weight. The method used can
be applied for other nonlinear dispersive models, for instance the
multidimensional nonlinear Schrodinger equation.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:52:05 GMT""}]","2013-03-29"
"1303.7124","Daniel Mazin","Daniel Mazin, Martin Raue, Bagmeet Behera, Susumu Inoue, Yoshiyuki
  Inoue, Takeshi Nakamori, Tomonori Totani (for the CTA Consortium)","Potential of EBL and cosmology studies with the Cherenkov Telescope
  Array","12 pages, 9 figures, to appear in Astroparticle Physics. arXiv admin
  note: text overlap with arXiv:1005.1196","Astroparticle Physics Volume 43, Pages 241-251 (March 2013)","10.1016/j.astropartphys.2012.09.002","MPP-2013-98","astro-ph.CO astro-ph.HE","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Very high energy (VHE, E >100 GeV) gamma-rays are absorbed via interaction
with low-energy photons from the extragalactic background light (EBL) if the
involved photon energies are above the threshold for electron-positron pair
creation. The VHE gamma-ray absorption, which is energy dependent and increases
strongly with redshift, distorts the VHE spectra observed from distant objects.
The observed energy spectra of the AGNs carry, therefore, an imprint of the
EBL. The detection of VHE gamma-ray spectra of distant sources (z = 0.11 -
0.54) by current generation Imaging Atmospheric Cherenkov Telescopes (IACTs)
enabled to set strong upper limits on the EBL density, using certain basic
assumptions about blazar physics. In this paper it is studied how the improved
sensitivity of the Cherenkov Telescope Array (CTA) and its enlarged energy
coverage will enlarge our knowledge about the EBL and its sources. CTA will
deliver a large sample of AGN at different redshifts with detailed measured
spectra. In addition, it will provide the exciting opportunity to use gamma ray
bursts (GRBs) as probes for the EBL density at high redshifts.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:52:06 GMT""}]","2019-08-14"
"1303.7125","Sean Benson","The LHCb collaboration: R. Aaij, C. Abellan Beteta, B. Adeva, M.
  Adinolfi, C. Adrover, A. Affolder, Z. Ajaltouni, J. Albrecht, F. Alessio, M.
  Alexander, S. Ali, G. Alkhazov, P. Alvarez Cartelle, A.A. Alves Jr, S. Amato,
  S. Amerio, Y. Amhis, L. Anderlini, J. Anderson, R. Andreassen, R.B. Appleby,
  O. Aquines Gutierrez, F. Archilli, A. Artamonov, M. Artuso, E. Aslanides, G.
  Auriemma, S. Bachmann, J.J. Back, C. Baesso, V. Balagura, W. Baldini, R.J.
  Barlow, C. Barschel, S. Barsuk, W. Barter, Th. Bauer, A. Bay, J. Beddow, F.
  Bedeschi, I. Bediaga, S. Belogurov, K. Belous, I. Belyaev, E. Ben-Haim, M.
  Benayoun, G. Bencivenni, S. Benson, J. Benton, A. Berezhnoy, R. Bernet, M.-O.
  Bettler, M. van Beuzekom, A. Bien, S. Bifani, T. Bird, A. Bizzeti, P.M.
  Bj{\o}rnstad, T. Blake, F. Blanc, J. Blouw, S. Blusk, V. Bocci, A. Bondar, N.
  Bondar, W. Bonivento, S. Borghi, A. Borgia, T.J.V. Bowcock, E. Bowen, C.
  Bozzi, T. Brambach, J. van den Brand, J. Bressieux, D. Brett, M. Britsch, T.
  Britton, N.H. Brook, H. Brown, I. Burducea, A. Bursche, G. Busetto, J.
  Buytaert, S. Cadeddu, O. Callot, M. Calvi, M. Calvo Gomez, A. Camboni, P.
  Campana, D. Campora Perez, A. Carbone, G. Carboni, R. Cardinale, A. Cardini,
  H. Carranza-Mejia, L. Carson, K. Carvalho Akiba, G. Casse, M. Cattaneo, Ch.
  Cauet, M. Charles, Ph. Charpentier, P. Chen, N. Chiapolini, M. Chrzaszcz, K.
  Ciba, X. Cid Vidal, G. Ciezarek, P.E.L. Clarke, M. Clemencic, H.V. Cliff, J.
  Closier, C. Coca, V. Coco, J. Cogan, E. Cogneras, P. Collins, A.
  Comerma-Montells, A. Contu, A. Cook, M. Coombes, S. Coquereau, G. Corti, B.
  Couturier, G.A. Cowan, D. Craik, S. Cunliffe, R. Currie, C. D'Ambrosio, P.
  David, P.N.Y. David, A. Davis, I. De Bonis, K. De Bruyn, S. De Capua, M. De
  Cian, J.M. De Miranda, L. De Paula, W. De Silva, P. De Simone, D. Decamp, M.
  Deckenhoff, L. Del Buono, D. Derkach, O. Deschamps, F. Dettori, A. Di Canto,
  H. Dijkstra, M. Dogaru, S. Donleavy, F. Dordei, A. Dosil Su\'arez, D.
  Dossett, A. Dovbnya, F. Dupertuis, R. Dzhelyadin, A. Dziurda, A. Dzyuba, S.
  Easo, U. Egede, V. Egorychev, S. Eidelman, D. van Eijk, S. Eisenhardt, U.
  Eitschberger, R. Ekelhof, L. Eklund, I. El Rifai, Ch. Elsasser, D. Elsby, A.
  Falabella, C. F\""arber, G. Fardell, C. Farinelli, S. Farry, V. Fave, D.
  Ferguson, V. Fernandez Albor, F. Ferreira Rodrigues, M. Ferro-Luzzi, S.
  Filippov, C. Fitzpatrick, M. Fontana, F. Fontanelli, R. Forty, O. Francisco,
  M. Frank, C. Frei, M. Frosini, S. Furcas, E. Furfaro, A. Gallas Torreira, D.
  Galli, M. Gandelman, P. Gandini, Y. Gao, J. Garofoli, P. Garosi, J. Garra
  Tico, L. Garrido, C. Gaspar, R. Gauld, E. Gersabeck, M. Gersabeck, T.
  Gershon, Ph. Ghez, V. Gibson, V.V. Gligorov, C. G\""obel, D. Golubkov, A.
  Golutvin, A. Gomes, H. Gordon, M. Grabalosa G\'andara, R. Graciani Diaz, L.A.
  Granado Cardoso, E. Graug\'es, G. Graziani, A. Grecu, E. Greening, S.
  Gregson, O. Gr\""unberg, B. Gui, E. Gushchin, Yu. Guz, T. Gys, C.
  Hadjivasiliou, G. Haefeli, C. Haen, S.C. Haines, S. Hall, T. Hampson, S.
  Hansmann-Menzemer, N. Harnew, S.T. Harnew, J. Harrison, T. Hartmann, J. He,
  V. Heijne, K. Hennessy, P. Henrard, J.A. Hernando Morata, E. van Herwijnen,
  E. Hicks, D. Hill, M. Hoballah, C. Hombach, P. Hopchev, W. Hulsbergen, P.
  Hunt, T. Huse, N. Hussain, D. Hutchcroft, D. Hynds, V. Iakovenko, M. Idzik,
  P. Ilten, R. Jacobsson, A. Jaeger, E. Jans, P. Jaton, F. Jing, M. John, D.
  Johnson, C.R. Jones, B. Jost, M. Kaballo, S. Kandybei, M. Karacson, T.M.
  Karbach, I.R. Kenyon, U. Kerzel, T. Ketel, A. Keune, B. Khanji, O. Kochebina,
  I. Komarov, R.F. Koopman, P. Koppenburg, M. Korolev, A. Kozlinskiy, L.
  Kravchuk, K. Kreplin, M. Kreps, G. Krocker, P. Krokovny, F. Kruse, M.
  Kucharczyk, V. Kudryavtsev, T. Kvaratskheliya, V.N. La Thi, D. Lacarrere, G.
  Lafferty, A. Lai, D. Lambert, R.W. Lambert, E. Lanciotti, G. Lanfranchi, C.
  Langenbruch, T. Latham, C. Lazzeroni, R. Le Gac, J. van Leerdam, J.-P. Lees,
  R. Lef\`evre, A. Leflat, J. Lefran\c{c}ois, S. Leo, O. Leroy, B. Leverington,
  Y. Li, L. Li Gioi, M. Liles, R. Lindner, C. Linn, B. Liu, G. Liu, S. Lohn, I.
  Longstaff, J.H. Lopes, E. Lopez Asamar, N. Lopez-March, H. Lu, D. Lucchesi,
  J. Luisier, H. Luo, F. Machefert, I.V. Machikhiliyan, F. Maciuc, O. Maev, S.
  Malde, G. Manca, G. Mancinelli, U. Marconi, R. M\""arki, J. Marks, G.
  Martellotti, A. Martens, L. Martin, A. Mart\'in S\'anchez, M. Martinelli, D.
  Martinez Santos, D. Martins Tostes, A. Massafferri, R. Matev, Z. Mathe, C.
  Matteuzzi, E. Maurice, A. Mazurov, J. McCarthy, R. McNulty, A. Mcnab, B.
  Meadows, F. Meier, M. Meissner, M. Merk, D.A. Milanes, M.-N. Minard, J.
  Molina Rodriguez, S. Monteil, D. Moran, P. Morawski, M.J. Morello, R.
  Mountain, I. Mous, F. Muheim, K. M\""uller, R. Muresan, B. Muryn, B. Muster,
  P. Naik, T. Nakada, R. Nandakumar, I. Nasteva, M. Needham, N. Neufeld, A.D.
  Nguyen, T.D. Nguyen, C. Nguyen-Mau, M. Nicol, V. Niess, R. Niet, N. Nikitin,
  T. Nikodem, A. Nomerotski, A. Novoselov, A. Oblakowska-Mucha, V. Obraztsov,
  S. Oggero, S. Ogilvy, O. Okhrimenko, R. Oldeman, M. Orlandea, J.M. Otalora
  Goicochea, P. Owen, A. Oyanguren, B.K. Pal, A. Palano, M. Palutan, J. Panman,
  A. Papanestis, M. Pappagallo, C. Parkes, C.J. Parkinson, G. Passaleva, G.D.
  Patel, M. Patel, G.N. Patrick, C. Patrignani, C. Pavel-Nicorescu, A. Pazos
  Alvarez, A. Pellegrino, G. Penso, M. Pepe Altarelli, S. Perazzini, D.L.
  Perego, E. Perez Trigo, A. P\'erez-Calero Yzquierdo, P. Perret, M.
  Perrin-Terrin, G. Pessina, K. Petridis, A. Petrolini, A. Phan, E. Picatoste
  Olloqui, B. Pietrzyk, T. Pila\v{r}, D. Pinci, S. Playfer, M. Plo Casasus, F.
  Polci, G. Polok, A. Poluektov, E. Polycarpo, D. Popov, B. Popovici, C.
  Potterat, A. Powell, J. Prisciandaro, V. Pugatch, A. Puig Navarro, G. Punzi,
  W. Qian, J.H. Rademacker, B. Rakotomiaramanana, M.S. Rangel, I. Raniuk, N.
  Rauschmayr, G. Raven, S. Redford, M.M. Reid, A.C. dos Reis, S. Ricciardi, A.
  Richards, K. Rinnert, V. Rives Molina, D.A. Roa Romero, P. Robbe, E.
  Rodrigues, P. Rodriguez Perez, S. Roiser, V. Romanovsky, A. Romero Vidal, J.
  Rouvinet, T. Ruf, F. Ruffini, H. Ruiz, P. Ruiz Valls, G. Sabatino, J.J.
  Saborido Silva, N. Sagidova, P. Sail, B. Saitta, C. Salzmann, B. Sanmartin
  Sedes, M. Sannino, R. Santacesaria, C. Santamarina Rios, E. Santovetti, M.
  Sapunov, A. Sarti, C. Satriano, A. Satta, M. Savrie, D. Savrina, P. Schaack,
  M. Schiller, H. Schindler, M. Schlupp, M. Schmelling, B. Schmidt, O.
  Schneider, A. Schopper, M.-H. Schune, R. Schwemmer, B. Sciascia, A. Sciubba,
  M. Seco, A. Semennikov, K. Senderowska, I. Sepp, N. Serra, J. Serrano, P.
  Seyfert, M. Shapkin, I. Shapoval, P. Shatalov, Y. Shcheglov, T. Shears, L.
  Shekhtman, O. Shevchenko, V. Shevchenko, A. Shires, R. Silva Coutinho, T.
  Skwarnicki, N.A. Smith, E. Smith, M. Smith, M.D. Sokoloff, F.J.P. Soler, F.
  Soomro, D. Souza, B. Souza De Paula, B. Spaan, A. Sparkes, P. Spradlin, F.
  Stagni, S. Stahl, O. Steinkamp, S. Stoica, S. Stone, B. Storaci, M.
  Straticiuc, U. Straumann, V.K. Subbiah, S. Swientek, V. Syropoulos, M.
  Szczekowski, P. Szczypka, T. Szumlak, S. T'Jampens, M. Teklishyn, E.
  Teodorescu, F. Teubert, C. Thomas, E. Thomas, J. van Tilburg, V. Tisserand,
  M. Tobin, S. Tolk, D. Tonelli, S. Topp-Joergensen, N. Torr, E. Tournefier, S.
  Tourneur, M.T. Tran, M. Tresch, A. Tsaregorodtsev, P. Tsopelas, N. Tuning, M.
  Ubeda Garcia, A. Ukleja, D. Urner, U. Uwer, V. Vagnoni, G. Valenti, R.
  Vazquez Gomez, P. Vazquez Regueiro, S. Vecchi, J.J. Velthuis, M. Veltri, G.
  Veneziano, M. Vesterinen, B. Viaud, D. Vieira, X. Vilasis-Cardona, A.
  Vollhardt, D. Volyanskyy, D. Voong, A. Vorobyev, V. Vorobyev, C. Vo{\ss}, H.
  Voss, R. Waldi, R. Wallace, S. Wandernoth, J. Wang, D.R. Ward, N.K. Watson,
  A.D. Webber, D. Websdale, M. Whitehead, J. Wicht, J. Wiechczynski, D.
  Wiedner, L. Wiggers, G. Wilkinson, M.P. Williams, M. Williams, F.F. Wilson,
  J. Wishahi, M. Witek, S.A. Wotton, S. Wright, S. Wu, K. Wyllie, Y. Xie, F.
  Xing, Z. Xing, Z. Yang, R. Young, X. Yuan, O. Yushchenko, M. Zangoli, M.
  Zavertyaev, F. Zhang, L. Zhang, W.C. Zhang, Y. Zhang, A. Zhelezov, A.
  Zhokhov, L. Zhong, A. Zvyagin","First measurement of the CP-violating phase in $B_s^0 \to \phi \phi$
  decays","9 pages, 3 figures","Phys. Rev. Lett. 110, 241802 (2013)","10.1103/PhysRevLett.110.241802","LHCb-PAPER-2013-007, CERN-PH-EP-2013-046","hep-ex","http://creativecommons.org/licenses/by/3.0/","  A first flavour-tagged measurement of the time-dependent CP-violating
asymmetry in $B_s^0 \to \phi\phi$ decays is presented. In this decay channel,
the CP-violating weak phase arises due to CP violation in the interference
between $B_s^0$-$\bar{B}_s^0$ mixing and the $b \to s \bar{s} s $ gluonic
penguin decay amplitude. Using a sample of $pp$ collision data corresponding to
an integrated luminosity of $1.0\; fb^{-1}$ and collected at a centre-of-mass
energy of $7 \rm TeV$ with the LHCb detector, $880\ \B_s^0 \to \phi\phi$ signal
decays are obtained. The CP-violating phase is measured to be in the interval
[-2.46, -0.76] \rm rad$ at 68% confidence level. The p-value of the Standard
Model prediction is 16%.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:56:55 GMT""},{""version"":""v2"",""created"":""Fri, 14 Jun 2013 11:04:34 GMT""}]","2013-06-17"
"1303.7126","Huai-liang Chang","Huai-Liang Chang, Jun Li, Wei-Ping Li","Witten's top Chern class via cosection localization","35 pages",,,,"math.AG math-ph math.MP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  For a Landau Ginzburg space ([C^n/G],W), we construct the Witten's top Chern
classes as algebraic cycles via cosection localized virtual cycles in case all
sectors are narrow. We verify all axioms of such classes. We derive an explicit
formula of such classes in the free case. We prove that this construction is
equivalent to the prior constructions of Polishchuk-Vaintrob, of Chiodo and of
Fan-Jarvis-Ruan.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:00:01 GMT""}]","2013-03-29"
"1303.7127","Alexios Balatsoukas-Stimming","A. Balatsoukas-Stimming, A. J. Raymond, W. J. Gross, and A. Burg","Hardware Architecture for List SC Decoding of Polar Codes",,,,,"cs.IT cs.AR math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a hardware architecture and algorithmic improvements for list SC
decoding of polar codes. More specifically, we show how to completely avoid
copying of the likelihoods, which is algorithmically the most cumbersome part
of list SC decoding. The hardware architecture was synthesized for a
blocklength of N = 1024 bits and list sizes L = 2, 4 using a UMC 90nm VLSI
technology. The resulting decoder can achieve a coded throughput of 181 Mbps at
a frequency of 459 MHz.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:04:54 GMT""},{""version"":""v2"",""created"":""Fri, 14 Jun 2013 14:51:17 GMT""},{""version"":""v3"",""created"":""Thu, 27 Feb 2014 15:53:10 GMT""}]","2014-02-28"
"1303.7128","Robert Stencel","Robert E. Stencel","Results of the Recent \epsilon Aurigae Eclipse Campaign","15 pages. See also Journal AAVSO vol. 40 #2 pp.618-632","Central European Astrophys. Bulletin Vol. 37, 2013",,"University of Denver Observatories Report 2013-1","astro-ph.SR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Results of the 2010 eclipse campaign are described, and preliminary
interpretations proposed. These include photometric, interferometric,
spectroscopic, astrometric and polarimetric observational results. Next steps,
along with continued monitoring, include simulations and other future work.
Numerous acknowledgements are appropriate for the many participants in making
this international effort a success.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:12:10 GMT""},{""version"":""v2"",""created"":""Fri, 29 Mar 2013 02:37:10 GMT""}]","2013-04-01"
"1303.7129","Anna Hasenfratz","Anna Hasenfratz, Anqi Cheng, Gregory Petropoulos and David Schaich","Reaching the chiral limit in many flavor systems","7 pages, 5 figures; Contribution to SCGT12 ""KMI-GCOE Workshop on
  Strong Coupling Gauge Theories in the LHC Perspective"", 4-7 Dec. 2012, Nagoya
  University",,"10.1142/9789814566254_0004",,"hep-lat","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a brief overview of our recent lattice studies of SU(3) gauge
theory with N_f = 8 and 12 fundamental fermions, including some new and
yet-unpublished results.
  To explore relatively unfamiliar systems beyond lattice QCD, we carry out a
wide variety of investigations with the goal of synthesizing the results to
better understand the non-perturbative dynamics of these systems.
  All our findings are consistent with conformal infrared dynamics in the
12-flavor system, but with 8 flavors we observe puzzling behavior that requires
further investigation.
  Our new Monte Carlo renormalization group technique exploits the Wilson flow
to obtain more direct predictions of a 12-flavor IR fixed point.
  Studies of N_f = 12 bulk and finite-temperature transitions also indicate IR
conformality, while our current results for the 8-flavor phase diagram do not
yet provide clear signs of spontaneous chiral symmetry breaking.
  From the Dirac eigenvalue spectrum we extract the mass anomalous dimension
gamma_m, and predict gamma*_m = 0.32(3) at the 12-flavor fixed point.
  The N_f = 8 system again shows interesting behavior, with a large anomalous
dimension across a wide range of energy scales.
  We use the eigenvalue density to predict the chiral condensate, and compare
this approach with direct and partially-quenched measurements.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:12:12 GMT""}]","2017-08-23"
"1303.7130","Hyundae Lee","Hyeonbae Kang and Hyundae Lee","Coated inclusions of finite conductivity neutral to multiple fields in
  two dimensional conductivity or anti-plane elasticity",,"Eur. J. Appl. Math 25 (2014) 329-338","10.1017/S0956792514000060",,"math.AP","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider the problem of neutral inclusions for two-dimensional
conductivity and anti-plane elasticity. The neutral inclusion, when inserted in
a matrix having a uniform field, does not disturb the field outside the
inclusion. The inclusion consists of a core and a shell. We show that if the
inclusion is neutral to two linearly independent fields, then the core and the
shell are confocal ellipses.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:15:01 GMT""}]","2019-02-20"
"1303.7131","Nicolas Regnault","E. Dobardzic, M.V. Milovanovic, N. Regnault","On the geometrical description of fractional Chern insulators based on
  static structure factor calculations","13 pages, 7 figures, published version","Phys. Rev. B 88, 115117 (2013)","10.1103/PhysRevB.88.115117",,"cond-mat.str-el","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study the static structure factor of the fractional Chern insulator
Laughlin-like state and provide analytical forms for this quantity in the
long-distance limit. In the course of this we identify averaged over Brillouin
zone Fubini Study metric as the relevant metric in the long-distance limit. We
discuss under which conditions the static structure factor will assume the
usual behavior of Laughlin-like fractional quantum Hall system i.e. the
scenario of Girvin, MacDonald, and Platzman [Phys. Rev. B 33, 2481 (1986)]. We
study the influence of the departure of the averaged over Brillouin zone Fubini
Study metric from its fractional quantum Hall value which appears in the
long-distance analysis as an effective change of the filling factor. According
to our exact diagonalization results on the Haldane model and analytical
considerations we find persistence of fractional Chern insulator state even in
this region of the parameter space.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:17:11 GMT""},{""version"":""v2"",""created"":""Fri, 25 Oct 2013 01:43:03 GMT""}]","2013-10-28"
"1303.7132","Davide Mandelli","Davide Mandelli and Andrea Vanossi and Erio Tosatti","Stick-Slip Nanofriction in Trapped Cold Ion Chains","9 pages, 13 figures",,"10.1103/PhysRevB.87.195418",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Stick-slip -- the sequence of mechanical instabilities through which a slider
advances on a solid substrate -- is pervasive throughout sliding friction, from
nano to geological scales. Here we suggest that trapped cold ions in an optical
lattice can also be of help in understanding stick-slip friction, and also the
way friction changes when one of the sliders undergoes structural transitions.
For that scope, we simulated the dynamical properties of a 101-ions chain,
driven to slide back and forth by a slowly oscillating electric field in an
incommensurate periodic ""corrugation"" potential of increasing magnitude U0. We
found the chain sliding to switch, as U0 increases and before the Aubry
transition, from a smooth-sliding regime with low dissipation to a stick-slip
regime with high dissipation. In the stick-slip regime the onset of overall
sliding is preceded by precursor events consisting of partial slips of few ions
only, leading to partial depinning of the chain, a nutshell remnant of
precursor events at the onset of motion also observed in macroscopic sliders.
Seeking to identify the possible effects on friction of a structural
transition, we reduced the trapping potential aspect ratio until the ion chain
shape turned from linear to zigzag. Dynamic friction was found to rise at the
transition, reflecting the opening of newer dissipation channels.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:21:44 GMT""},{""version"":""v2"",""created"":""Thu, 2 May 2013 12:53:35 GMT""}]","2015-06-15"
"1303.7133","Roberta Cardinale","LHCb collaboration: R. Aaij, C. Abellan Beteta, A. Adametz, B. Adeva,
  M. Adinolfi, C. Adrover, A. Affolder, Z. Ajaltouni, J. Albrecht, F. Alessio,
  M. Alexander, S. Ali, G. Alkhazov, P. Alvarez Cartelle, A.A. Alves Jr, S.
  Amato, Y. Amhis, L. Anderlini, J. Anderson, R. Andreassen, R.B. Appleby, O.
  Aquines Gutierrez, F. Archilli, A. Artamonov, M. Artuso, E. Aslanides, G.
  Auriemma, S. Bachmann, J.J. Back, C. Baesso, V. Balagura, W. Baldini, R.J.
  Barlow, C. Barschel, S. Barsuk, W. Barter, Th. Bauer, A. Bay, J. Beddow, I.
  Bediaga, S. Belogurov, K. Belous, I. Belyaev, E. Ben-Haim, M. Benayoun, G.
  Bencivenni, S. Benson, J. Benton, A. Berezhnoy, R. Bernet, M.-O. Bettler, M.
  van Beuzekom, A. Bien, S. Bifani, T. Bird, A. Bizzeti, P.M. Bj{\o}rnstad, T.
  Blake, F. Blanc, C. Blanks, J. Blouw, S. Blusk, A. Bobrov, V. Bocci, A.
  Bondar, N. Bondar, W. Bonivento, S. Borghi, A. Borgia, T.J.V. Bowcock, E.
  Bowen, C. Bozzi, T. Brambach, J. van den Brand, J. Bressieux, D. Brett, M.
  Britsch, T. Britton, N.H. Brook, H. Brown, I. Burducea, A. Bursche, J.
  Buytaert, S. Cadeddu, O. Callot, M. Calvi, M. Calvo Gomez, A. Camboni, P.
  Campana, A. Carbone, G. Carboni, R. Cardinale, A. Cardini, H. Carranza-Mejia,
  L. Carson, K. Carvalho Akiba, G. Casse, M. Cattaneo, Ch. Cauet, M. Charles,
  Ph. Charpentier, P. Chen, N. Chiapolini, M. Chrzaszcz, K. Ciba, X. Cid Vidal,
  G. Ciezarek, P.E.L. Clarke, M. Clemencic, H.V. Cliff, J. Closier, C. Coca, V.
  Coco, J. Cogan, E. Cogneras, P. Collins, A. Comerma-Montells, A. Contu, A.
  Cook, M. Coombes, S. Coquereau, G. Corti, B. Couturier, G.A. Cowan, D. Craik,
  S. Cunliffe, R. Currie, C. D'Ambrosio, P. David, P.N.Y. David, I. De Bonis,
  K. De Bruyn, S. De Capua, M. De Cian, J.M. De Miranda, L. De Paula, W. De
  Silva, P. De Simone, D. Decamp, M. Deckenhoff, H. Degaudenzi, L. Del Buono,
  C. Deplano, D. Derkach, O. Deschamps, F. Dettori, A. Di Canto, J. Dickens, H.
  Dijkstra, M. Dogaru, F. Domingo Bonal, S. Donleavy, F. Dordei, A. Dosil
  Su\'arez, D. Dossett, A. Dovbnya, F. Dupertuis, R. Dzhelyadin, A. Dziurda, A.
  Dzyuba, S. Easo, U. Egede, V. Egorychev, S. Eidelman, D. van Eijk, S.
  Eisenhardt, U. Eitschberger, R. Ekelhof, L. Eklund, I. El Rifai, Ch.
  Elsasser, D. Elsby, A. Falabella, C. F\""arber, G. Fardell, C. Farinelli, S.
  Farry, V. Fave, D. Ferguson, V. Fernandez Albor, F. Ferreira Rodrigues, M.
  Ferro-Luzzi, S. Filippov, C. Fitzpatrick, M. Fontana, F. Fontanelli, R.
  Forty, O. Francisco, M. Frank, C. Frei, M. Frosini, S. Furcas, E. Furfaro, A.
  Gallas Torreira, D. Galli, M. Gandelman, P. Gandini, Y. Gao, J. Garofoli, P.
  Garosi, J. Garra Tico, L. Garrido, C. Gaspar, R. Gauld, E. Gersabeck, M.
  Gersabeck, T. Gershon, Ph. Ghez, V. Gibson, V.V. Gligorov, C. G\""obel, D.
  Golubkov, A. Golutvin, A. Gomes, H. Gordon, M. Grabalosa G\'andara, R.
  Graciani Diaz, L.A. Granado Cardoso, E. Graug\'es, G. Graziani, A. Grecu, E.
  Greening, S. Gregson, O. Gr\""unberg, B. Gui, E. Gushchin, Yu. Guz, T. Gys, C.
  Hadjivasiliou, G. Haefeli, C. Haen, S.C. Haines, S. Hall, T. Hampson, S.
  Hansmann-Menzemer, N. Harnew, S.T. Harnew, J. Harrison, P.F. Harrison, T.
  Hartmann, J. He, V. Heijne, K. Hennessy, P. Henrard, J.A. Hernando Morata, E.
  van Herwijnen, E. Hicks, D. Hill, M. Hoballah, C. Hombach, P. Hopchev, W.
  Hulsbergen, P. Hunt, T. Huse, N. Hussain, D. Hutchcroft, D. Hynds, V.
  Iakovenko, P. Ilten, R. Jacobsson, A. Jaeger, E. Jans, F. Jansen, P. Jaton,
  F. Jing, M. John, D. Johnson, C.R. Jones, B. Jost, M. Kaballo, S. Kandybei,
  M. Karacson, T.M. Karbach, I.R. Kenyon, U. Kerzel, T. Ketel, A. Keune, B.
  Khanji, O. Kochebina, I. Komarov, R.F. Koopman, P. Koppenburg, M. Korolev, A.
  Kozlinskiy, L. Kravchuk, K. Kreplin, M. Kreps, G. Krocker, P. Krokovny, F.
  Kruse, M. Kucharczyk, V. Kudryavtsev, T. Kvaratskheliya, V.N. La Thi, D.
  Lacarrere, G. Lafferty, A. Lai, D. Lambert, R.W. Lambert, E. Lanciotti, G.
  Lanfranchi, C. Langenbruch, T. Latham, C. Lazzeroni, R. Le Gac, J. van
  Leerdam, J.-P. Lees, R. Lef\`evre, A. Leflat, J. Lefran\c{c}ois, O. Leroy, Y.
  Li, L. Li Gioi, M. Liles, R. Lindner, C. Linn, B. Liu, G. Liu, J. von Loeben,
  J.H. Lopes, E. Lopez Asamar, N. Lopez-March, H. Lu, J. Luisier, H. Luo, F.
  Machefert, I.V. Machikhiliyan, F. Maciuc, O. Maev, S. Malde, G. Manca, G.
  Mancinelli, N. Mangiafave, U. Marconi, R. M\""arki, J. Marks, G. Martellotti,
  A. Martens, L. Martin, A. Mart\'in S\'anchez, M. Martinelli, D. Martinez
  Santos, D. Martins Tostes, A. Massafferri, R. Matev, Z. Mathe, C. Matteuzzi,
  M. Matveev, E. Maurice, A. Mazurov, J. McCarthy, R. McNulty, B. Meadows, F.
  Meier, M. Meissner, M. Merk, D.A. Milanes, M.-N. Minard, J. Molina Rodriguez,
  S. Monteil, D. Moran, P. Morawski, R. Mountain, I. Mous, F. Muheim, K.
  M\""uller, R. Muresan, B. Muryn, B. Muster, P. Naik, T. Nakada, R. Nandakumar,
  I. Nasteva, M. Needham, N. Neufeld, A.D. Nguyen, T.D. Nguyen, C. Nguyen-Mau,
  M. Nicol, V. Niess, R. Niet, N. Nikitin, T. Nikodem, S. Nisar, A. Nomerotski,
  A. Novoselov, A. Oblakowska-Mucha, V. Obraztsov, S. Oggero, S. Ogilvy, O.
  Okhrimenko, R. Oldeman, M. Orlandea, J.M. Otalora Goicochea, P. Owen, B.K.
  Pal, A. Palano, M. Palutan, J. Panman, A. Papanestis, M. Pappagallo, C.
  Parkes, C.J. Parkinson, G. Passaleva, G.D. Patel, M. Patel, G.N. Patrick, C.
  Patrignani, C. Pavel-Nicorescu, A. Pazos Alvarez, A. Pellegrino, G. Penso, M.
  Pepe Altarelli, S. Perazzini, D.L. Perego, E. Perez Trigo, A. P\'erez-Calero
  Yzquierdo, P. Perret, M. Perrin-Terrin, G. Pessina, K. Petridis, A.
  Petrolini, A. Phan, E. Picatoste Olloqui, B. Pietrzyk, T. Pila\v{r}, D.
  Pinci, S. Playfer, M. Plo Casasus, F. Polci, G. Polok, A. Poluektov, E.
  Polycarpo, D. Popov, B. Popovici, C. Potterat, A. Powell, J. Prisciandaro, V.
  Pugatch, A. Puig Navarro, W. Qian, J.H. Rademacker, B. Rakotomiaramanana,
  M.S. Rangel, I. Raniuk, N. Rauschmayr, G. Raven, S. Redford, M.M. Reid, A.C.
  dos Reis, S. Ricciardi, A. Richards, K. Rinnert, V. Rives Molina, D.A. Roa
  Romero, P. Robbe, E. Rodrigues, P. Rodriguez Perez, G.J. Rogers, S. Roiser,
  V. Romanovsky, A. Romero Vidal, J. Rouvinet, T. Ruf, H. Ruiz, G. Sabatino,
  J.J. Saborido Silva, N. Sagidova, P. Sail, B. Saitta, C. Salzmann, B.
  Sanmartin Sedes, M. Sannino, R. Santacesaria, C. Santamarina Rios, E.
  Santovetti, M. Sapunov, A. Sarti, C. Satriano, A. Satta, M. Savrie, D.
  Savrina, P. Schaack, M. Schiller, H. Schindler, S. Schleich, M. Schlupp, M.
  Schmelling, B. Schmidt, O. Schneider, A. Schopper, M.-H. Schune, R.
  Schwemmer, B. Sciascia, A. Sciubba, M. Seco, A. Semennikov, K. Senderowska,
  I. Sepp, N. Serra, J. Serrano, P. Seyfert, M. Shapkin, I. Shapoval, P.
  Shatalov, Y. Shcheglov, T. Shears, L. Shekhtman, O. Shevchenko, V.
  Shevchenko, A. Shires, R. Silva Coutinho, T. Skwarnicki, N.A. Smith, E.
  Smith, M. Smith, K. Sobczak, M.D. Sokoloff, F.J.P. Soler, F. Soomro, D.
  Souza, B. Souza De Paula, B. Spaan, A. Sparkes, P. Spradlin, F. Stagni, S.
  Stahl, O. Steinkamp, S. Stoica, S. Stone, B. Storaci, M. Straticiuc, U.
  Straumann, V.K. Subbiah, S. Swientek, V. Syropoulos, M. Szczekowski, P.
  Szczypka, T. Szumlak, S. T'Jampens, M. Teklishyn, E. Teodorescu, F. Teubert,
  C. Thomas, E. Thomas, J. van Tilburg, V. Tisserand, M. Tobin, S. Tolk, D.
  Tonelli, S. Topp-Joergensen, N. Torr, E. Tournefier, S. Tourneur, M.T. Tran,
  M. Tresch, A. Tsaregorodtsev, P. Tsopelas, N. Tuning, M. Ubeda Garcia, A.
  Ukleja, D. Urner, U. Uwer, V. Vagnoni, G. Valenti, R. Vazquez Gomez, P.
  Vazquez Regueiro, S. Vecchi, J.J. Velthuis, M. Veltri, G. Veneziano, M.
  Vesterinen, B. Viaud, D. Vieira, X. Vilasis-Cardona, A. Vollhardt, D.
  Volyanskyy, D. Voong, A. Vorobyev, V. Vorobyev, C. Vo\ss, H. Voss, R. Waldi,
  R. Wallace, S. Wandernoth, J. Wang, D.R. Ward, N.K. Watson, A.D. Webber, D.
  Websdale, M. Whitehead, J. Wicht, J. Wiechczynski, D. Wiedner, L. Wiggers, G.
  Wilkinson, M.P. Williams, M. Williams, F.F. Wilson, J. Wishahi, M. Witek,
  S.A. Wotton, S. Wright, S. Wu, K. Wyllie, Y. Xie, F. Xing, Z. Xing, Z. Yang,
  R. Young, X. Yuan, O. Yushchenko, M. Zangoli, M. Zavertyaev, F. Zhang, L.
  Zhang, W.C. Zhang, Y. Zhang, A. Zhelezov, L. Zhong, A. Zvyagin","Measurements of the branching fractions of $B^{+} \to p \bar p K^{+}$
  decays","14 pages, 5 figures","Eur.Phys.J. C73 (2013) 2462","10.1140/epjc/s10052-013-2462-2","LHCb-PAPER-2012-047, CERN-PH-EP-2013-040","hep-ex","http://creativecommons.org/licenses/by/3.0/","  The branching fractions of the decay $B^{+} \to p \bar p K^{+}$ for different
intermediate states are measured using data, corresponding to an integrated
luminosity of $1.0 fb^{-1}$, collected by the LHCb experiment. The total
branching fraction, its charmless component $(M_{p\bar p} <2.85 {GeV/}c^{2})$
and the branching fractions via the resonant $c\bar c$ states $\eta_{c}(1S)$
and $\psi(2S)$ relative to the decay via a $J/\psi$ intermediate state are
{align*} \frac{{\mathcal B}(B^{+} \to p \bar p K^{+})_{total}}{{\mathcal
B}(B^{+} \to J/\psi K^{+} \to p \bar p K^{+})}=& \, 4.91 \pm 0.19 \, {(\rm
stat)} \pm 0.14 \, {(\rm syst)}, \frac{{\mathcal B}(B^{+} \to p \bar p
K^{+})_{M_{p\bar p} <2.85 {GeV/}c^{2}}}{{\mathcal B}(B^{+} \to J/\psi K^{+} \to
p \bar p K^{+})}=& \, 2.02 \pm 0.10 \, {(\rm stat)}\pm 0.08 \, {(\rm syst)},
\frac{{\mathcal B} (B^{+} \to \eta_{c}(1S) K^{+} \to p \bar p K^{+})}{{\mathcal
B}(B^{+} \to J/\psi K^{+} \to p \bar p K^{+})} = & \, 0.578 \pm 0.035 \, {(\rm
stat)} \pm 0.027 \, {(\rm syst)}, \frac{{\mathcal B} (B^{+} \to \psi(2S) K^{+}
\to p \bar p K^{+})}{{\mathcal B}(B^{+} \to J/\psi K^{+} \to p \bar p K^{+})}=&
\, 0.080 \pm 0.012 \, {(\rm stat)} \pm 0.009 \, {(\rm syst)}. {align*} Upper
limits on the $B^{+}$ branching fractions into the $\eta_{c}(2S)$ meson and
into the charmonium-like states X(3872) and X(3915) are also obtained.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:24:56 GMT""},{""version"":""v2"",""created"":""Tue, 16 Jul 2013 12:51:41 GMT""}]","2016-08-11"
"1303.7134","David Dudal","D. Dudal, M. S. Guimaraes, L. F. Palhares, S. P. Sorella","Confinement and dynamical chiral symmetry breaking in a non-perturbative
  renormalizable quark model","10 pages, 3 figures. v2: extended version; added intermediate
  computations + extra references. v3: partially rewritten version, new title,
  extra background material (21 pages). v4: version accepted for publication in
  Ann.Phys. (23 pages)",,,,"hep-ph hep-lat hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Inspired by the construction of the Gribov-Zwanziger action in the Landau
gauge, we introduce a quark model exhibiting both confinement and chiral
symmetry aspects. An important feature is the incorporation of spontaneous
chiral symmetry breaking in a renormalizable fashion. The quark propagator in
the condensed vacuum turns out to be of a confining type. Besides a real pole,
it exhibits complex conjugate poles. The resulting spectral form is explicitly
shown to violate positivity, indicative of its unphysical character. Moreover,
the ensuing quark mass function fits well to existing lattice data. To further
validate the physical nature of the model, we identify a massless pseudoscalar
(i.e. a pion) in the chiral limit and present estimates for the rho meson mass
and decay constant.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:26:12 GMT""},{""version"":""v2"",""created"":""Thu, 17 Oct 2013 13:54:44 GMT""},{""version"":""v3"",""created"":""Tue, 18 Aug 2015 13:12:29 GMT""},{""version"":""v4"",""created"":""Wed, 6 Jan 2016 10:23:28 GMT""}]","2016-01-07"
"1303.7135","Bernard Derrida","B. Derrida and M. Retaux","Finite size corrections to the large deviation function of the density
  in the one dimensional symmetric simple exclusion process",,,"10.1007/s10955-013-0797-6",,"cond-mat.stat-mech","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The symmetric simple exclusion process is one of the simplest
out-of-equilibrium systems for which the steady state is known. Its large
deviation functional of the density has been computed in the past both by
microscopic and macroscopic approaches. Here we obtain the leading finite size
correction to this large deviation functional. The result is compared to the
similar corrections for equilibrium systems.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:31:38 GMT""}]","2015-06-15"
"1303.7136","EPTCS","Delia Kesner (Universit\'e Paris-Diderot), Petrucio Viana
  (Universidade Federal Fluminense)","Proceedings Seventh Workshop on Logical and Semantic Frameworks, with
  Applications",,"EPTCS 113, 2013","10.4204/EPTCS.113",,"cs.LO cs.PL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This document contains the proceedings of the Seventh International Workshop
on Logical and Semantic Frameworks, with Applications, which was held on
September 29 and 30, 2012, in Rio de Janeiro, Brazil. It contains 11 regular
papers (9 long and 2 short) accepted for presentation at the meeting, as well
as extended abstracts of invited talks by Torben Bra\""uner (Roskilde
University, Denmark), Maribel Fern\'andez (King's College London, United
Kingdom), Edward Hermann Haeusler (PUC-Rio, Brazil) and Alexandre Miquel
(\'Ecole Normale Sup\'erieure de Lyon, France).
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:45:17 GMT""}]","2013-03-29"
"1303.7137","Maksims Fiosins","A. Andronov and M. Fioshin","Discrete Optimization of Statistical Sample Sizes in Simulation by Using
  the Hierarchical Bootstrap Method","9 pages","proceedings of the 6th Tartu Conference, 1999",,,"cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Bootstrap method application in simulation supposes that value of random
variables are not generated during the simulation process but extracted from
available sample populations. In the case of Hierarchical Bootstrap the
function of interest is calculated recurrently using the calculation tree. In
the present paper we consider the optimization of sample sizes in each vertex
of the calculation tree. The dynamic programming method is used for this aim.
Proposed method allows to decrease a variance of system characteristic
estimators.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:48:44 GMT""}]","2013-03-29"
"1303.7138","Isabelle Robert-Philip","Armand Lebreton, Izo Abram, R\'emy Braive, Isabelle Sagnes, Isabelle
  Robert-Philip and Alexios Beveratos","Unequivocal differentiation of coherent and chaotic light through
  interferometric photon correlation measurements","5 pages, 3 figures",,"10.1103/PhysRevLett.110.163603",,"quant-ph physics.optics","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a novel experimental technique that can differentiate
unequivocally between chaotic light and coherent light with amplitude
fluctuations, and thus permits to characterize unambiguously the output of a
laser. This technique consists of measuring the second-order intensity
cross-correlation at the outputs of an unbalanced Michelson interferometer. It
is applied to a chaotic light source and to the output of a semiconductor
nanolaser whose ""standard"" intensity correlation function above-threshold
displays values compatible with a mixture of coherent and chaotic light. Our
experimental results demonstrate that the output of such lasers is not
partially chaotic but is indeed a coherent state with amplitude fluctuations.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:49:35 GMT""}]","2015-06-15"
"1303.7139","Sean Gryb B","Sean Gryb and Karim Thebault","Symmetry and Evolution in Quantum Gravity","29 pages. 1 table (version accepted to Foundations of Physics)","Found. Phys., Vol 44, Issue 3 (2014), Page 305-34","10.1007/s10701-014-9789-x",,"gr-qc hep-th","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose an operator constraint equation for the wavefunction of the
Universe that admits genuine evolution. While the corresponding classical
theory is equivalent to the canonical decomposition of General Relativity, the
quantum theory makes predictions that are distinct from Wheeler-DeWitt
cosmology. Furthermore, the local symmetry principle - and corresponding
observables - of the theory have a direct interpretation in terms of a
conventional gauge theory, where the gauge symmetry group is that of spatial
conformal diffeomorphisms (that preserve the spatial volume of the Universe).
The global evolution is in terms of an arbitrary parameter that serves only as
an unobservable label for successive states of the Universe. Our proposal
follows unambiguously from a suggestion of York whereby the independently
specifiable initial data in the action principle of General Relativity is given
by a conformal geometry and the spatial average of the York time on the
spacelike hypersurfaces that bound the variation. Remarkably, such a
variational principle uniquely selects the form of the constraints of the
theory so that we can establish a precise notion of both symmetry and evolution
in quantum gravity.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:50:07 GMT""},{""version"":""v2"",""created"":""Tue, 25 Mar 2014 11:21:22 GMT""}]","2014-04-03"
"1303.7140","Alexander Stibor","Georg Sch\""utz, Alexander Rembold, Andreas Pooch, Henrike Prochel and
  Alexander Stibor","Effective beam separation schemes for the measurement of the electric
  Aharonov-Bohm effect in an ion interferometer",,"Ultramicroscopy 158, 65 (2015)","10.1016/j.ultramic.2015.06.016",,"physics.optics physics.ins-det quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose an experiment for the first proof of the type I electric
Aharonov-Bohm effect in an ion interferometer for hydrogen. The performances of
three different beam separation schemes are simulated and compared. The
coherent ion beam is generated by a single atom tip (SAT) source and separated
by either two biprisms with a quadrupole lens, two biprisms with an einzel-lens
or three biprisms. The beam path separation is necessary to introduce two metal
tubes that can be pulsed with different electric potentials. The high time
resolution of a delay line detector allows to work with a continuous ion beam
and circumvents the pulsed beam operation as originally suggested by Aharonov
and Bohm. We demonstrate, that the higher mass and therefore lower velocity of
ions compared to electrons combined with the high expected SAT ion emission
puts the direct proof of this quantum effect for the first time into reach of
current technical possibilities. Thereby a high coherent ion detection rate is
crucial to avoid long integration times that allow the influence of dephasing
noise from the environment. We can determine the period of the expected matter
wave interference pattern and the signal on the detector by determining the
superposition angle of the coherent partial beams. Our simulations were tested
with an electron interferometer setup and agree with the experimental results.
We determine the separation scheme with three biprisms to be most efficient and
predict a total signal acquisition time of only 80 s to measure a phase shift
from 0 to 2$\pi$ due to the electric Aharonov-Bohm effect.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:51:03 GMT""},{""version"":""v2"",""created"":""Sun, 20 Oct 2013 15:41:47 GMT""},{""version"":""v3"",""created"":""Tue, 19 Nov 2013 19:21:08 GMT""},{""version"":""v4"",""created"":""Fri, 19 Dec 2014 21:35:08 GMT""},{""version"":""v5"",""created"":""Tue, 28 Jul 2015 18:32:55 GMT""}]","2017-03-24"
"1303.7141","Andrey Palnichenko","N.S. Sidorov, A.V. Palnichenko, D. V. Shakhrai, V. V. Avdonin, O. M.
  Vyaselev, S.S. Khasanov","Superconductivity of Mg/MgO interface formed by shock-wave pressure",,"Physica C 488 (2013) 18-24","10.1016/j.physc.2013.02.012",,"cond-mat.supr-con","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A mixture of Mg and MgO has been subjected to a shock-wave pressure of 170
kbar. The ac susceptibility measurements of the product has revealed a
metastable superconductivity with Tc=30 K, characterized by glassy dynamics of
the shielding currents below Tc. Comparison of the ac susceptibility and the dc
magnetization measurements infers that the superconductivity arises within the
interfacial layer formed between metallic Mg and its oxide due to the
shock-wave treatment.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:52:35 GMT""}]","2013-03-29"
"1303.7142","Sarah C. Gallagher","S. C. Gallagher, M. M. Abado (U. of Western Ontario), J. E. Everett
  (Northwestern U.), S. Keating (U. Toronto), R. P. Deo","Why a Windy Torus?","8 pages. Proceedings article for the Torus Workshop 2012 held at U.
  Texas at San Antonio Dec 5-7, 2012. C. Packham. R. Mason, A. Alonson-Herrero
  (eds.)",,,,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Mass ejection in the form of winds or jets appears to be as fundamental to
quasar activity as accretion, and can be directly observed in many objects with
broadened and blue-shifted UV absorption features. A convincing argument for
radiation pressure driving this ionized outflow can be made within the dust
sublimation radius. Beyond, radiation pressure is even more important, as high
energy photons from the central engine can now push on dust grains. This
physics underlies the dusty-wind model for the putative obscuring torus.
Specifically, the dusty wind in our model is first launched from the outer
accretion disk as a magneto-centrifugal wind and then accelerated and shaped by
radiation pressure from the central continuum. Such a wind can plausibly
account for both the necessary obscuring medium to explain the ratio of
broad-to-narrow-line objects and the mid-infrared emission commonly seen in
quasar spectral energy distributions. A convincing demonstration that
large-scale, organized magnetic fields are present in radio-quiet active
galactic nuclei is now required to bolster the case for this paradigm.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:54:52 GMT""}]","2013-03-29"
"1303.7143","Fred Espen Benth","Fred Espen Benth, Andr\'e S\""u{\ss}","Integration theory for infinite dimensional volatility modulated
  Volterra processes","Published at http://dx.doi.org/10.3150/15-BEJ696 in the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)","Bernoulli 2016, Vol. 22, No. 3, 1383-1430","10.3150/15-BEJ696","IMS-BEJ-BEJ696","math.PR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We treat a stochastic integration theory for a class of Hilbert-valued,
volatility-modulated, conditionally Gaussian Volterra processes. We apply
techniques from Malliavin calculus to define this stochastic integration as a
sum of a Skorohod integral, where the integrand is obtained by applying an
operator to the original integrand, and a correction term involving the
Malliavin derivative of the same altered integrand, integrated against the
Lebesgue measure. The resulting integral satisfies many of the expected
properties of a stochastic integral, including an It\^{o} formula. Moreover, we
derive an alternative definition using a random-field approach and relate both
concepts. We present examples related to fundamental solutions to partial
differential equations.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:55:59 GMT""},{""version"":""v2"",""created"":""Thu, 17 Mar 2016 14:22:02 GMT""}]","2016-03-18"
"1303.7144","Yu-Ru Lin Yu-Ru Lin","Yu-Ru Lin, Drew Margolin, Brian Keegan, Andrea Baronchelli, David
  Lazer","#Bigbirds Never Die: Understanding Social Dynamics of Emergent Hashtag","Proceedings of the 7th International AAAI Conference on Weblogs and
  Social Media (ICWSM 2013)",,,,"cs.SI physics.data-an physics.soc-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We examine the growth, survival, and context of 256 novel hashtags during the
2012 U.S. presidential debates. Our analysis reveals the trajectories of
hashtag use fall into two distinct classes: ""winners"" that emerge more quickly
and are sustained for longer periods of time than other ""also-rans"" hashtags.
We propose a ""conversational vibrancy"" framework to capture dynamics of
hashtags based on their topicality, interactivity, diversity, and prominence.
Statistical analyses of the growth and persistence of hashtags reveal novel
relationships between features of this framework and the relative success of
hashtags. Specifically, retweets always contribute to faster hashtag adoption,
replies extend the life of ""winners"" while having no effect on ""also-rans.""
This is the first study on the lifecycle of hashtag adoption and use in
response to purely exogenous shocks. We draw on theories of uses and
gratification, organizational ecology, and language evolution to discuss these
findings and their implications for understanding social influence and
collective action in social media more generally.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:58:45 GMT""}]","2013-03-29"
"1303.7145","Sangbum Cho","Sangbum Cho and Yuya Koda","The genus two Goeritz group of $S^2 \times S^1$","minor changes; to appear in Mathematical Research Letters",,,,"math.GT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The genus-g Goeritz group is the group of isotopy classes of
orientation-preserving homeomorphisms of a closed orientable 3-manifold that
preserve a given genus-g Heegaard splitting of the manifold. In this work, we
show that the genus-2 Goeritz group of $S^2 \times S^1$ is finitely presented,
and give its explicit presentation.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:01:09 GMT""},{""version"":""v2"",""created"":""Sat, 18 Jan 2014 09:17:04 GMT""}]","2014-01-21"
"1303.7146","Bozena Piatek","Bozena Piatek and Rafa Espinola","Diversities, hyperconvexity and fixed points",,,,,"math.MG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diversities have been recently introduced as a generalization of metrics for
which a rich tight span theory could be stated. In this work we take up a
number of questions about hyperconvexity, diversities and fixed points of
nonexpansive mappings. Most of these questions are motivated by the study of
the connection between a hyperconvex diversity and its induced metric space for
which we provide some answers. Examples are given, for instance, showing that
such a metric space need not be hyperconvex but still we prove, as our main
result, that they enjoy the fixed point property for nonexpansive mappings
provided the diversity is bounded and that this boundedness condition cannot be
transferred from the diversity to the induced metric space.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:07:10 GMT""}]","2016-10-05"
"1303.7147","Semen Gorfman","Semen Gorfman, Oleg Schmidt, Vladimir Tsirelson, Michael Ziolkowski,
  Ullrich Pietsch","Crystallography under external electric field","10 pages, 10 figures",,"10.1002/zaac.201200497",,"cond-mat.mtrl-sci","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Structural response of crystals to an applied external perturbation is
important as a key for understanding microscopic origin of physical properties.
Experimental investigation of structural response is a great challenge for
modern structure analysis. We demonstrate how advanced X-ray diffraction
techniques facilitate probing tiny (10-4 {\AA}) distortions of bond lengths
under a permanent electric field. We also discuss details of the experimental
procedure essential for reaching such precision. We ask whether the experiment
can be used to evaluate chemical bonds in crystals by their sensitivity to an
external electric field and discuss if the bond deformations can be predicted
using the bond-valence model or the Bader's theory of atoms in molecules and
crystals. Finally, we describe the new time-resolved studies of a structural
response to a dynamical switch of applied electric field. These results give
access to the time-lining of piezoelectric effect on a microsecond time scale.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:23:13 GMT""}]","2016-05-24"
"1303.7148","Randy Dumas","Randy K. Dumas, Ezio Iacocca, Stefano Bonetti, Sohrab Sani, Majid
  Mohseni, Anders Eklund, Johan Persson, Olle Heinonen, and Johan {\AA}kerman","Spin wave mode coexistence on the nano-scale: A consequence of the
  Oersted field induced asymmetric energy landscape","20 pages (including supplementary material), 6 figures",,"10.1103/PhysRevLett.110.257202",,"cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It has been argued that if multiple spin wave modes are competing for the
same centrally located energy source, as in a nanocontact spin torque
oscillator, that only one mode should survive in the steady state. Here, the
experimental conditions necessary for mode coexistence are explored. Mode
coexistence is facilitated by the local field asymmetries induced by the
spatially inhomogeneous Oersted field, which leads to a physical separation of
the modes, and is further promoted by spin wave localization at reduced applied
field angles. Finally, both simulation and experiment reveal a low frequency
signal consistent with the intermodulation of two coexistent modes.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:26:30 GMT""}]","2015-06-15"
"1303.7149","Andre Vellino","Andr\'e Vellino","Usage-based vs. Citation-based Methods for Recommending Scholarly
  Research Articles","4 pages, 4 figures, ACM Recommender Systems Workshop 2012, Dublin
  Ireland",,,,"cs.DL cs.IR","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There are two principal data sources for collaborative filtering recommenders
in scholarly digital libraries: usage data obtained from harvesting a large,
distributed collection of Open URL web logs and citation data obtained from the
journal articles. This study explores the characteristics of recommendations
generated by implementations of these two methods: the 'bX' system by ExLibris
and an experimental citation-based recommender, Sarkanto. Recommendations from
each system were compared according to their semantic similarity to the seed
article that was used to generate them. Since the full text of the articles was
not available for all the recommendations in both systems, the semantic
similarity between the seed article and the recommended articles was deemed to
be the semantic distance between the journals in which the articles were
published. The semantic distance between journals was computed from the
""semantic vectors"" distance between all the terms in the full-text of the
available articles in that journal and this study shows that citation-based
recommendations are more semantically diverse than usage-based ones. These
recommenders are complementary since most of the time, when one recommender
produces recommendations the other does not.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:27:16 GMT""},{""version"":""v2"",""created"":""Fri, 29 Mar 2013 02:21:53 GMT""}]","2013-04-01"
"1303.7150","Quesne Christiane","I. Marquette, C. Quesne","New ladder operators for a rational extension of the harmonic oscillator
  and superintegrability of some two-dimensional systems","22 pages, published version","J. Math. Phys. 54 (2013) 102102, 12 pages","10.1063/1.4823771","ULB/229/CQ/13/1","math-ph math.MP nlin.SI quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  New ladder operators are constructed for a rational extension of the harmonic
oscillator associated with type III Hermite exceptional orthogonal polynomials
and characterized by an even integer $m$. The eigenstates of the Hamiltonian
separate into $m+1$ infinite-dimensional unitary irreducible representations of
the corresponding polynomial Heisenberg algebra. These ladder operators are
used to construct a higher-order integral of motion for two superintegrable
two-dimensional systems separable in cartesian coordinates. The polynomial
algebras of such systems provide for the first time an algebraic derivation of
the whole spectrum through their finite-dimensional unitary irreducible
representations.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:33:53 GMT""},{""version"":""v2"",""created"":""Wed, 3 Jul 2013 12:21:39 GMT""},{""version"":""v3"",""created"":""Fri, 11 Oct 2013 12:49:51 GMT""}]","2015-06-15"
"1303.7151","Akira Endo","A. Endo, C. Sfiligoj, S. J. C. Yates, J. J. A. Baselmans, D. J. Thoen,
  S. M. H. Javadzadeh, P. P. van der Werf, A. M. Baryshev, and T. M. Klapwijk","On-chip filter bank spectroscopy at 600-700 GHz using NbTiN
  superconducting resonators","4 pages, 3 figures, submitted to Applied Physics Letters",,,,"astro-ph.IM cond-mat.supr-con","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We experimentally demonstrate the principle of an on-chip submillimeter wave
filter bank spectrometer, using superconducting microresonators as narrow
band-separation filters. The filters are made of NbTiN/SiNx/NbTiN microstrip
line resonators, which have a resonance frequency in the range of 614-685
GHz---two orders of magnitude higher in frequency than what is currently
studied for use in circuit quantum electrodynamics and photodetectors. The
frequency resolution of the filters decreases from 350 to 140 with increasing
frequency, most likely limited by dissipation of the resonators.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:34:49 GMT""},{""version"":""v2"",""created"":""Fri, 21 Jun 2013 06:45:24 GMT""}]","2013-06-24"
"1303.7152","Victor Chernozhukov","Victor Chernozhukov, Denis Chetverikov, Kengo Kato","Anti-concentration and honest, adaptive confidence bands","Published in at http://dx.doi.org/10.1214/14-AOS1235 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Annals of Statistics 2014, Vol. 42, No. 5, 1787-1818","10.1214/14-AOS1235","IMS-AOS-AOS1235","math.ST stat.TH","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modern construction of uniform confidence bands for nonparametric densities
(and other functions) often relies on the classical Smirnov-Bickel-Rosenblatt
(SBR) condition; see, for example, Gin\'{e} and Nickl [Probab. Theory Related
Fields 143 (2009) 569-596]. This condition requires the existence of a limit
distribution of an extreme value type for the supremum of a studentized
empirical process (equivalently, for the supremum of a Gaussian process with
the same covariance function as that of the studentized empirical process). The
principal contribution of this paper is to remove the need for this classical
condition. We show that a considerably weaker sufficient condition is derived
from an anti-concentration property of the supremum of the approximating
Gaussian process, and we derive an inequality leading to such a property for
separable Gaussian processes. We refer to the new condition as a generalized
SBR condition. Our new result shows that the supremum does not concentrate too
fast around any value. We then apply this result to derive a Gaussian
multiplier bootstrap procedure for constructing honest confidence bands for
nonparametric density estimators (this result can be applied in other
nonparametric problems as well). An essential advantage of our approach is that
it applies generically even in those cases where the limit distribution of the
supremum of the studentized empirical process does not exist (or is unknown).
This is of particular importance in problems where resolution levels or other
tuning parameters have been chosen in a data-driven fashion, which is needed
for adaptive constructions of the confidence bands. Finally, of independent
interest is our introduction of a new, practical version of Lepski's method,
which computes the optimal, nonconservative resolution levels via a Gaussian
multiplier bootstrap method.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:39:44 GMT""},{""version"":""v2"",""created"":""Fri, 29 Mar 2013 23:07:46 GMT""},{""version"":""v3"",""created"":""Wed, 4 Dec 2013 19:28:36 GMT""},{""version"":""v4"",""created"":""Tue, 23 Sep 2014 12:56:01 GMT""}]","2014-09-24"
"1303.7153","Lingfeng Zhang","L.-F. Zhang, L. Covaci, M. V. Milo\v{s}evi\'c, G.R. Berdiyorov, F.M.
  Peeters","Vortex states in nanoscale superconducting squares: the influence of
  quantum confinement",,"Phys. Rev. B 88, 144501 (2013)","10.1103/PhysRevB.88.144501",,"cond-mat.supr-con","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Bogoliubov-de Gennes theory is used to investigate the effect of the size of
a superconducting square on the vortex states in the quantum confinement
regime. When the superconducting coherence length is comparable to the Fermi
wavelength, the shape resonances of the superconducting order parameter have
strong influence on the vortex configuration. Several unconventional vortex
states, including asymmetric ones, giant multi-vortex combinations, and states
comprising giant antivortex, were found as ground states and their stability
was found to be very sensitive on the value of $k_F\xi_0$, the size of the
sample $W$, and the magnetic flux $\Phi$. By increasing the temperature and/or
enlarging the size of the sample, quantum confinement is suppressed and the
conventional mesoscopic vortex states as predicted by the Ginzburg-Laudau (GL)
theory are recovered. However, contrary to the GL results we found that the
states containing symmetry-induced vortex-antivortex pairs are stable over the
whole temperature range. It turns out that the inhomogeneous order parameter
induced by quantum confinement favors vortex-antivortex molecules, as well as
giant vortices with a rich structure in the vortex core - unattainable in the
GL domain.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:43:43 GMT""},{""version"":""v2"",""created"":""Fri, 29 Mar 2013 15:40:00 GMT""},{""version"":""v3"",""created"":""Wed, 2 Oct 2013 15:41:23 GMT""}]","2014-01-20"
"1303.7154","Antonio Di Lorenzo","Antonio Di Lorenzo","Quantum state tomography from sequential measurement of two variables in
  a single setup","16 pages, 2 figures, revtex4-1. Comments welcome. v2: sacrificed
  generality to clarity; v3: minor changes, references added","Phys. Rev. A 88, 042114 (2013)","10.1103/PhysRevA.88.042114",,"quant-ph cond-mat.mes-hall","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We demonstrate that the task of determining an unknown quantum state can be
accomplished efficiently by making a sequential measurement of two observables
$\hat{A}$ and $\hat{B}$, provided that the two observables are chosen in such a
way that their eigenstates may form bases connected by a discrete Fourier
transform. The state can be pure or mixed, the dimension of the Hilbert space
and the coupling strength are arbitrary, and the experimental setup is fixed.
The concept of Moyal quasicharacteristic function is introduced for
finite-dimensional Hilbert spaces.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:50:06 GMT""},{""version"":""v2"",""created"":""Mon, 24 Jun 2013 12:32:15 GMT""},{""version"":""v3"",""created"":""Sat, 19 Oct 2013 11:18:51 GMT""}]","2013-10-22"
"1303.7155","Kate Jones","Kate L. Jones","Transfer reaction experiments with radioactive beams: from halos to the
  r-process","From the proceedings of Nobel Symposium 152: Physics with Radioactive
  Beams, Goteborg, Sweden, Spring 2012","Phys. Scr. T152 (2013) 014020","10.1088/0031-8949/2013/T152/014020",,"nucl-ex","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Transfer reactions are a powerful probe of the properties of atomic nuclei.
When used in inverse kinematics with radioactive ion beams they can provide
detailed information on the structure of exotic nuclei and can inform
nucleosynthesis calculations. There are a number of groups around the world who
use these reactions, usually with particle detection in large silicon arrays.
Sometimes these arrays are coupled to gamma-ray detectors, and occasionally
smaller arrays of silicon detectors are mounted within a solenoid magnet.
Modern techniques using transfer reactions in inverse kinematics are covered,
with specific examples, many from measurements made with beams from the
Holifield Radioactive Ion Beam Facility at Oak Ridge National Laboratory.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:55:02 GMT""}]","2013-03-29"
"1303.7156","Angelo Felice Lopez","Salvatore Cacciola and Angelo Felice Lopez","Nakamaye's theorem on log canonical pairs","v2: We removed, in the introduction, the phrase about Choi's papers,
  as he uses Nakamaye's theorem in the semiample case. Updated references. v3:
  added reference to Ambro's ""Quasi-log varieties"". v4: improved exposition in
  sections 1, 2 and 4; slightly corrected the statement of Lemma 3.1","Ann. Inst. Fourier 64 (2014), n. 6, 2283-2298",,,"math.AG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We generalize Nakamaye's description, via intersection theory, of the
augmented base locus of a big and nef divisor on a normal pair with
log-canonical singularities or, more generally, on a normal variety with non-lc
locus of dimension at most 1. We also generalize
Ein-Lazarsfeld-Mustata-Nakamaye-Popa's description, in terms of valuations, of
the subvarieties of the restricted base locus of a big divisor on a normal pair
with klt singularities.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:55:16 GMT""},{""version"":""v2"",""created"":""Fri, 19 Apr 2013 18:39:10 GMT""},{""version"":""v3"",""created"":""Tue, 23 Apr 2013 18:29:21 GMT""},{""version"":""v4"",""created"":""Sun, 23 Mar 2014 19:46:35 GMT""}]","2014-12-24"
"1303.7157","Changqing Jin","K. Zhao, Z. Deng, X. C. Wang, W. Han, J. L. Zhu, X. Li, Q.Q. Liu, R.C.
  Yu, T. Goko, B. Frandsen, Lian Liu, Fanlong Ning, Y.J. Uemura, H. Dabkowska,
  G.M. Luke, H. Luetkens, E. Morenzoni, S.R. Dunsiger, A. Senyshyn, P. B\""oni,
  C.Q. Jin","New Diluted Ferromagnetic Semiconductor isostructural to 122 type iron
  pnictide superconductor with TC up to 180 K","12 pages, 3 Figures, 3 tables","Nature Communications 4:1442 | DOI: 10.1038(2013)",,,"cond-mat.mtrl-sci cond-mat.str-el quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Diluted magnetic semiconductors (DMS) have received much attention due to its
potential applications to spintronics devices. A prototypical system (Ga,Mn)As
has been widely studied since 1990s. The simultaneous spin and charge doping
via hetero-valence (Ga3+,Mn2+) substitution, however, resulted in severely
limited solubility without availability of bulk specimens. Previously we
synthesized a new diluted ferromagnetic semiconductor of bulk Li(Zn,Mn)As with
Tc up to 50K, where isovalent (Zn,Mn) spin doping was separated from charge
control via Li concentrations. Here we report the synthesis of a new diluted
ferromagnetic semiconductor (Ba1-xKx)(Zn1-yMny)2As2, isostructural to iron 122
system, where holes are doped via (Ba2+, K1+), while spins via (Zn2+,Mn2+)
substitutions. Bulk samples with x=0.1-0.3 and y=0.05-0.15 exhibit
ferromagnetic order with TC up to 180K, comparable to that of record high Tc
for Ga(MnAs), significantly enhanced than Li(Zn,Mn)As. Moreover the
(Ba,K)(Zn,Mn)2As2 shares the same 122 crystal structure with semiconducting
BaZn2As2, antiferromagnetic BaMn2As2, and superconducting (Ba,K)Fe2As2, which
makes them promising to the development of multilayer functional devices.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 15:55:29 GMT""}]","2013-03-29"
"1303.7227","Monica Patriche","Monica Patriche","New results on equilibria of fuzzy abstract economies","13 pages. arXiv admin note: substantial text overlap with
  arXiv:1303.6979",,,,"math.OC","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We obtain new equilibrium theorems for fuzzy abstract economies with
correspondences being w-upper semicontinuous or having e-USS-property.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 21:10:48 GMT""}]","2013-04-01"
"1303.7228","Peter Mitchell","P. D. Mitchell, C. G. Lacey, C. M. Baugh, S. Cole","How well can we really estimate the stellar masses of galaxies from
  broadband photometry?","29 pages, 16 Figures, changed to match published MNRAS version",,"10.1093/mnras/stt1280",,"astro-ph.CO","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The estimated stellar masses of galaxies are widely used to characterize how
the galaxy population evolves over cosmic time. If stellar masses can be
estimated in a robust manner, free from any bias, global diagnostics such as
the stellar mass function can be used to constrain the physics of galaxy
formation. We explore how galaxy stellar masses, estimated by fitting
broad-band spectral energy distributions (SEDs) with stellar population models,
can be biased as a result of commonly adopted assumptions for the
star-formation and chemical enrichment histories, recycled fractions and dust
attenuation curves of galaxies. We apply the observational technique of
broad-band SED fitting to model galaxy SEDs calculated by the theoretical
galaxy formation model GALFORM, isolating the effect of each of these
assumptions. We find that, averaged over the entire galaxy population, the
common assumption of exponentially declining star-formation histories does not
adversely affect stellar mass estimation. We show that fixing the metallicity
in SED fitting or using sparsely sampled metallicity grids can introduce mass
dependent systematics into stellar mass estimates. We find that the common
assumption of a star-dust geometry corresponding to a uniform foreground dust
screen can cause the stellar masses of dusty model galaxies to be significantly
underestimated. Finally, we show that stellar mass functions recovered by
applying SED fitting to model galaxies at high redshift can differ
significantly in both shape and normalization from the intrinsic mass functions
predicted by a given model. Given these differences, our methodology of using
stellar masses estimated from model galaxy SEDs offers a new, self-consistent
way to compare model predictions with observations.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 22:30:07 GMT""},{""version"":""v2"",""created"":""Wed, 4 Sep 2013 17:29:22 GMT""}]","2015-06-15"
"1303.7229","Qi-Ren Zhang","Qi-Ren Zhang","Quantum electrodynamics in a laser and the electron laser collision","15 pages, 2 figures","Chin. Phys. B Vol. 23, No. 1 (2014) 010306","10.1088/1674-1056/23/1/010306",,"quant-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Quantum electrodynamics in a laser is formulated, in which the electron-laser
interaction is exactly considered, while the interaction of an electron and a
single photon is considered by perturbation. The formulation is applied to the
electron-laser collisions. The effect of coherence between photons in the laser
is therefore fully considered in these collisions. The possibility of
$\gamma-$ray laser generation by use of this kind of collision is discussed.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:40:07 GMT""},{""version"":""v2"",""created"":""Tue, 16 Jul 2013 08:54:28 GMT""},{""version"":""v3"",""created"":""Tue, 26 Nov 2013 08:47:50 GMT""},{""version"":""v4"",""created"":""Tue, 7 Jan 2014 02:31:37 GMT""},{""version"":""v5"",""created"":""Fri, 8 Aug 2014 09:02:10 GMT""}]","2014-08-11"
"1303.7230","Oleg A. Vasilyev","Oleg A. Vasilyev, Boris A. Klumov, Alexei V. Tkachenko","Precursors of order in aggregates of patchy particles","12 pages, 3 figures",,"10.1103/PhysRevE.88.012302",,"cond-mat.soft","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We study computationally the local structure of aggregated systems of patchy
particles. By calculating the probability distribution functions of various
rotational invariants we can identify the precursors of orientation order in
amorphous phase. Surprisingly, the strongest signature of local order is
observed for 4-patch particles with tetrahedral symmetry, not for 6-patch
particles with the cubic one. This trend is exactly opposite to their known
ability to crystallize. We relate this anomaly to the observation that a
generic aggregate of patchy systems has coordination number close to 4. Our
results also suggest a significant correlation between rotational order in the
studied liquids with the corresponding crystalline phases, making this approach
potentially useful for a broader range of patchy systems.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:28:14 GMT""}]","2015-06-15"
"1303.7400","Bent Flyvbjerg","Bent Flyvbjerg","Policy and Planning for Large Infrastructure Projects: Problems, Causes,
  Cures","arXiv admin note: substantial text overlap with arXiv:1303.6571,
  arXiv:1303.6654, arXiv:1303.6571, arXiv:1302.3642",,,"Policy Research Working Paper, WPS 3781, World Bank, Washington, DC,
  2005, 32 pp","q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper argues, first, that a major problem in the planning of large
infrastructure projects is the high level of misinformation about costs and
benefits that decision makers face in deciding whether to build, and the high
risks such misinformation generates. Second, it explores the causes of
misinformation and risk, mainly in the guise of optimism bias and strategic
misrepresentation. Finally, the paper presents a number of measures aimed at
improving planning and decision making for large infrastructure projects,
including changed incentive structures and better planning methods. Thus the
paper is organized as a simple triptych consisting in problems, causes, and
cures.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:04:30 GMT""}]","2013-04-01"
"1303.7401","Bent Flyvbjerg","Bent Flyvbjerg","Measuring Inaccuracy in Travel Demand Forecasting: Methodological
  Considerations Regarding Ramp Up and Sampling",,"Transportation Research A, vol. 39, no. 6, July 2005, 522-530","10.1016/j.tra.2005.02.003",,"q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Project promoters, forecasters, and managers sometimes object to two things
in measuring inaccuracy in travel demand forecasting: (1) using the forecast
made at the time of making the decision to build as the basis for measuring
inaccuracy and (2) using traffic during the first year of operations as the
basis for measurement. This paper presents the case against both objections.
First, if one is interested in learning whether decisions about building
transport infrastructure are based on reliable information, then it is exactly
the traffic forecasted at the time of making the decision to build that is of
interest. Second, although ideally studies should take into account so-called
demand ""ramp up"" over a period of years, the empirical evidence and practical
considerations do not support this ideal requirement, at least not for large-N
studies. Finally, the paper argues that large samples of inaccuracy in travel
demand forecasts are likely to be conservatively biased, i.e., accuracy in
travel demand forecasts estimated from such samples would likely be higher than
accuracy in travel demand forecasts in the project population. This bias must
be taken into account when interpreting the results from statistical analyses
of inaccuracy in travel demand forecasting.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:41:50 GMT""}]","2013-04-01"
"1303.7402","Bent Flyvbjerg","Bent Flyvbjerg","Cost Overruns and Demand Shortfalls in Urban Rail and Other
  Infrastructure",,"Transportation Planning and Technology, vol. 30, no. 1, February
  2007, 9-30","10.1080/03081060701207938",,"q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Risk, including economic risk, is increasingly a concern for public policy
and management. The possibility of dealing effectively with risk is hampered,
however, by lack of a sound empirical basis for risk assessment and management.
The paper demonstrates the general point for cost and demand risks in urban
rail projects. The paper presents empirical evidence that allow valid economic
risk assessment and management of urban rail projects, including benchmarking
of individual or groups of projects. Benchmarking of the Copenhagen Metro is
presented as a case in point. The approach developed is proposed as a model for
other types of policies and projects in order to improve economic and financial
risk assessment and management in policy and planning.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 12:14:47 GMT""}]","2013-04-01"
"1303.7403","Bent Flyvbjerg","Bent Flyvbjerg, Massimo Garbuio, and Dan Lovallo","Delusion and Deception in Large Infrastructure Projects: Two Models for
  Explaining and Preventing Executive Disaster",,"California Management Review, vol. 51, no. 2, Winter 2009, 170-193","10.1225/CMR423",,"q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Economist recently reported that infrastructure spending is the largest
it is ever been as a share of world GDP. With $22 trillion in projected
investments over the next ten years in emerging economies alone, the magazine
calls it the ""biggest investment boom in history."" The efficiency of
infrastructure planning and execution is therefore particularly important at
present. Unfortunately, the private sector, the public sector and
private/public sector partnerships have a dismal record of delivering on large
infrastructure cost and performance promises. This paper explains why and how
to solve the problem.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:51:20 GMT""}]","2013-04-01"
"1303.7404","Bent Flyvbjerg","Bent Flyvbjerg, Nils Bruzelius, and Werner Rothengatter","Megaprojects and Risk: An Anatomy of Ambition","Cambridge University Press, 2003",,,,"q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Back cover text: Megaprojects and Risk provides the first detailed
examination of the phenomenon of megaprojects. It is a fascinating account of
how the promoters of multibillion-dollar megaprojects systematically and
self-servingly misinform parliaments, the public and the media in order to get
projects approved and built. It shows, in unusual depth, how the formula for
approval is an unhealthy cocktail of underestimated costs, overestimated
revenues, undervalued environmental impacts and overvalued economic development
effects. This results in projects that are extremely risky, but where the risk
is concealed from MPs, taxpayers and investors. The authors not only explore
the problems but also suggest practical solutions drawing on theory and hard,
scientific evidence from the several hundred projects in twenty nations that
illustrate the book. Accessibly written, it will be essential reading in its
field for students, scholars, planners, economists, auditors, politicians,
journalists and interested citizens.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 10:29:04 GMT""}]","2013-04-01"
"1303.7405","Bent Flyvbjerg","Bent Flyvbjerg","How Planners Deal with Uncomfortable Knowledge: The Dubious Ethics of
  the American Planning Association","Flyvbjerg, Bent, 2013, ""How Planners Deal with Uncomfortable
  Knowledge: The Dubious Ethics of the American Planning Association,"" Cities",,"10.1016/J.CITIES.2012.10.016",,"q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With a point of departure in the concept ""uncomfortable knowledge,"" this
article presents a case study of how the American Planning Association (APA)
deals with such knowledge. APA was found to actively suppress publicity of
malpractice concerns and bad planning in order to sustain a boosterish image of
planning. In the process, APA appeared to disregard and violate APA's own Code
of Ethics. APA justified its actions with a need to protect APA members'
interests, seen as preventing planning and planners from being presented in
public in a bad light. The current article argues that it is in members'
interest to have malpractice critiqued and reduced, and that this best happens
by exposing malpractice, not by denying or diverting attention from it as APA
did in this case. Professions, organizations, and societies that stifle
critique tend to degenerate and become socially and politically irrelevant
""zombie institutions."" The article asks whether such degeneration has set in
for APA and planning. Finally, it is concluded that more debate about APA's
ethics and actions is needed for improving planning practice. Nine key
questions are presented to constructively stimulate such debate.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:04:53 GMT""}]","2013-04-01"
"1304.0265","Alexander Budzier","Bent Flyvbjerg and Alexander Budzier","Why Your IT Project Might Be Riskier Than You Think",,"Harvard Business Review, Vol. 89 (2011), No. 9, pp. 23-25",,,"q-fin.GN","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Out-of-control information technology (IT) projects have ended the careers of
top managers, such as EADS CEO Noel Forgeard and Levi Strauss' CIO David
Bergen. Moreover, IT projects have brought down whole companies, like Kmart in
the US and Auto Windscreen in the UK. Software and other IT is now such an
integral part of most business processes and products that CEOs must know their
IT risks, which are typically substantial and overlooked. The analysis of a
sample of 1,471 IT projects showed that the average cost overrun was 27% - but
that figure masks a far more alarming 'fat tail' risk. Fully one in six of the
projects in the sample was a Black Swan, with a cost overrun of 200%, on
average, and a schedule overrun of almost 70%. This highlights the true pitfall
of IT change initiatives: It's not that they're particularly prone to high cost
overruns on average - it is that there are a disproportionate number of Black
Swans. By focusing on averages instead of the more damaging outliers, most
managers and consultants have been missing the real risk in doing IT. In
conclusion, the article outlines ideas as to what can be done to avoid Black
Swans.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 13:59:48 GMT""},{""version"":""v2"",""created"":""Tue, 16 Apr 2013 17:19:27 GMT""}]","2013-04-17"
"1304.1491","Fahiem Bacchus","Fahiem Bacchus","Lp : A Logic for Statistical Information","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-1-6","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This extended abstract presents a logic, called Lp, that is capable of
representing and reasoning with a wide variety of both qualitative and
quantitative statistical information. The advantage of this logical formalism
is that it offers a declarative representation of statistical knowledge;
knowledge represented in this manner can be used for a variety of reasoning
tasks. The logic differs from previous work in probability logics in that it
uses a probability distribution over the domain of discourse, whereas most
previous work (e.g., Nilsson [2], Scott et al. [3], Gaifinan [4], Fagin et al.
[5]) has investigated the attachment of probabilities to the sentences of the
logic (also, see Halpern [6] and Bacchus [7] for further discussion of the
differences). The logic Lp possesses some further important features. First, Lp
is a superset of first order logic, hence it can represent ordinary logical
assertions. This means that Lp provides a mechanism for integrating statistical
information and reasoning about uncertainty into systems based solely on logic.
Second, Lp possesses transparent semantics, based on sets and probabilities of
those sets. Hence, knowledge represented in Lp can be understood in terms of
the simple primative concepts of sets and probabilities. And finally, the there
is a sound proof theory that has wide coverage (the proof theory is complete
for certain classes of models). The proof theory captures a sufficient range of
valid inferences to subsume most previous probabilistic uncertainty reasoning
systems. For example, the linear constraints like those generated by Nilsson's
probabilistic entailment [2] can be generated by the proof theory, and the
Bayesian inference underlying belief nets [8] can be performed. In addition,
the proof theory integrates quantitative and qualitative reasoning as well as
statistical and logical reasoning. In the next section we briefly examine
previous work in probability logics, comparing it to Lp. Then we present some
of the varieties of statistical information that Lp is capable of expressing.
After this we present, briefly, the syntax, semantics, and proof theory of the
logic. We conclude with a few examples of knowledge representation and
reasoning in Lp, pointing out the advantages of the declarative representation
offered by Lp. We close with a brief discussion of probabilities as degrees of
belief, indicating how such probabilities can be generated from statistical
knowledge encoded in Lp. The reader who is interested in a more complete
treatment should consult Bacchus [7].
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:36:47 GMT""}]","2013-04-08"
"1304.1492","Kenneth Basye","Kenneth Basye, Thomas L. Dean","Map Learning with Indistinguishable Locations","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-7-13","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Nearly all spatial reasoning problems involve uncertainty of one sort or
another. Uncertainty arises due to the inaccuracies of sensors used in
measuring distances and angles. We refer to this as directional uncertainty.
Uncertainty also arises in combining spatial information when one location is
mistakenly identified with another. We refer to this as recognition
uncertainty. Most problems in constructing spatial representations (maps) for
the purpose of navigation involve both directional and recognition uncertainty.
In this paper, we show that a particular class of spatial reasoning problems
involving the construction of representations of large-scale space can be
solved efficiently even in the presence of directional and recognition
uncertainty. We pay particular attention to the problems that arise due to
recognition uncertainty.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:36:53 GMT""}]","2013-04-08"
"1304.1493","Carlo Berzuini","Carlo Berzuini, Riccardo Bellazzi, Silvana Quaglini","Temporal Reasoning with Probabilities","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-14-21","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper we explore representations of temporal knowledge based upon the
formalism of Causal Probabilistic Networks (CPNs). Two different
?continuous-time? representations are proposed. In the first, the CPN includes
variables representing ?event-occurrence times?, possibly on different time
scales, and variables representing the ?state? of the system at these times. In
the second, the CPN describes the influences between random variables with
values in () representing dates, i.e. time-points associated with the
occurrence of relevant events. However, structuring a system of inter-related
dates as a network where all links commit to a single specific notion of cause
and effect is in general far from trivial and leads to severe difficulties. We
claim that we should recognize explicitly different kinds of relation between
dates, such as ?cause?, ?inhibition?, ?competition?, etc., and propose a method
whereby these relations are coherently embedded in a CPN using additional
auxiliary nodes corresponding to ""instrumental"" variables. Also discussed,
though not covered in detail, is the topic concerning how the quantitative
specifications to be inserted in a temporal CPN can be learned from specific
data.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:36:59 GMT""}]","2013-04-08"
"1304.1494","Piero P. Bonissone","Piero P. Bonissone","Now that I Have a Good Theory of Uncertainty, What Else Do I Need?","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-22-33","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Rather than discussing the isolated merits of a nominative theory of
uncertainty, this paper focuses on a class of problems, referred to as Dynamic
Classification Problem (DCP), which requires the integration of many theories,
including a prescriptive theory of uncertainty. We start by analyzing the
Dynamic Classification Problem and by defining its induced requirements on a
supporting (plausible) reasoning system. We provide a summary of the underlying
theory (based on the semantics of many-valed logics) and illustrate the
constraints imposed upon it to ensure the modularity and computational
performance required by the applications. We describe the technologies used for
knowledge engineering (such as object-based simulator to exercise requirements,
and development tools to build the Knowledge Base and functionally validate
it). We emphasize the difference between development environment and run-time
system, describe the rule cross-compiler, and the real-time inference engine
with meta-reasoning capabilities. Finally, we illustrate how our proposed
technology satisfies the pop's requirements and analyze some of the lessons
reamed from its applications to situation assessment problems for Pilot's
Associate and Submarine Commander Associate.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:05 GMT""}]","2013-04-08"
"1304.1495","Piero P. Bonissone","Piero P. Bonissone, David A. Cyrluk, James W. Goodwin, Jonathan
  Stillman","Uncertainty and Incompleteness","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-34-45","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Two major difficulties in using default logics are their intractability and
the problem of selecting among multiple extensions. We propose an approach to
these problems based on integrating nommonotonic reasoning with plausible
reasoning based on triangular norms. A previously proposed system for reasoning
with uncertainty (RUM) performs uncertain monotonic inferences on an acyclic
graph. We have extended RUM to allow nommonotonic inferences and cycles within
nonmonotonic rules. By restricting the size and complexity of the nommonotonic
cycles we can still perform efficient inferences. Uncertainty measures provide
a basis for deciding among multiple defaults. Different algorithms and
heuristics for finding the optimal defaults are discussed.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:11 GMT""}]","2013-04-08"
"1304.1496","Lashon B. Booker","Lashon B. Booker, Naveen Hota, Connie Loggia Ramsey","BaRT: A Bayesian Reasoning Tool for Knowledge Based Systems","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-46-53","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  As the technology for building knowledge based systems has matured, important
lessons have been learned about the relationship between the architecture of a
system and the nature of the problems it is intended to solve. We are
implementing a knowledge engineering tool called BART that is designed with
these lessons in mind. BART is a Bayesian reasoning tool that makes belief
networks and other probabilistic techniques available to knowledge engineers
building classificatory problem solvers. BART has already been used to develop
a decision aid for classifying ship images, and it is currently being used to
manage uncertainty in systems concerned with analyzing intelligence reports.
This paper discusses how state-of-the-art probabilistic methods fit naturally
into a knowledge based approach to classificatory problem solving, and
describes the current capabilities of BART.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:17 GMT""}]","2013-04-08"
"1304.1497","Eugene Charniak","Eugene Charniak, Robert P. Goldman","Plan Recognition in Stories and in Life","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-54-59","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Plan recognition does not work the same way in stories and in ""real life""
(people tend to jump to conclusions more in stories). We present a theory of
this, for the particular case of how objects in stories (or in life) influence
plan recognition decisions. We provide a Bayesian network formalization of a
simple first-order theory of plans, and show how a particular network parameter
seems to govern the difference between ""life-like"" and ""story-like"" response.
We then show why this parameter would be influenced (in the desired way) by a
model of speaker (or author) topic selection which assumes that facts in
stories are typically ""relevant"".
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:23 GMT""}]","2013-04-08"
"1304.1498","R. Martin Chavez","R. Martin Chavez, Gregory F. Cooper","An Empirical Evaluation of a Randomized Algorithm for Probabilistic
  Inference","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-60-70","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In recent years, researchers in decision analysis and artificial intelligence
(Al) have used Bayesian belief networks to build models of expert opinion.
Using standard methods drawn from the theory of computational complexity,
workers in the field have shown that the problem of probabilistic inference in
belief networks is difficult and almost certainly intractable. K N ET, a
software environment for constructing knowledge-based systems within the
axiomatic framework of decision theory, contains a randomized approximation
scheme for probabilistic inference. The algorithm can, in many circumstances,
perform efficient approximate inference in large and richly interconnected
models of medical diagnosis. Unlike previously described stochastic algorithms
for probabilistic inference, the randomized approximation scheme computes a
priori bounds on running time by analyzing the structure and contents of the
belief network. In this article, we describe a randomized algorithm for
probabilistic inference and analyze its performance mathematically. Then, we
devote the major portion of the paper to a discussion of the algorithm's
empirical behavior. The results indicate that the generation of good trials
(that is, trials whose distribution closely matches the true distribution),
rather than the computation of numerous mediocre trials, dominates the
performance of stochastic simulation. Key words: probabilistic inference,
belief networks, stochastic simulation, computational complexity theory,
randomized algorithms.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:29 GMT""}]","2013-04-08"
"1304.1499","Marvin S. Cohen","Marvin S. Cohen","Decision Making ""Biases"" and Support for Assumption-Based Higher-Order
  Reasoning","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-71-80","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Unaided human decision making appears to systematically violate consistency
constraints imposed by normative theories; these biases in turn appear to
justify the application of formal decision-analytic models. It is argued that
both claims are wrong. In particular, we will argue that the ""confirmation
bias"" is premised on an overly narrow view of how conflicting evidence is and
ought to be handled. Effective decision aiding should focus on supporting the
contral processes by means of which knowledge is extended into novel situations
and in which assumptions are adopted, utilized, and revised. The Non- Monotonic
Probabilist represents initial work toward such an aid.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:35 GMT""}]","2013-04-08"
"1304.1500","Didier Dubois","Didier Dubois, Jerome Lang, Henri Prade","Automated Reasoning Using Possibilistic Logic: Semantics, Belief
  Revision and Variable Certainty Weights","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-81-87","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper an approach to automated deduction under uncertainty,based on
possibilistic logic, is proposed ; for that purpose we deal with clauses
weighted by a degree which is a lower bound of a necessity or a possibility
measure, according to the nature of the uncertainty. Two resolution rules are
used for coping with the different situations, and the refutation method can be
generalized. Besides the lower bounds are allowed to be functions of variables
involved in the clause, which gives hypothetical reasoning capabilities. The
relation between our approach and the idea of minimizing abnormality is briefly
discussed. In case where only lower bounds of necessity measures are involved,
a semantics is proposed, in which the completeness of the extended resolution
principle is proved. Moreover deduction from a partially inconsistent knowledge
base can be managed in this approach and displays some form of
non-monotonicity.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:41 GMT""}]","2013-04-08"
"1304.1501","Christopher Elsaesser","Christopher Elsaesser, Max Henrion","How Much More Probable is ""Much More Probable""? Verbal Expressions for
  Probability Updates","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-88-94","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Bayesian inference systems should be able to explain their reasoning to
users, translating from numerical to natural language. Previous empirical work
has investigated the correspondence between absolute probabilities and
linguistic phrases. This study extends that work to the correspondence between
changes in probabilities (updates) and relative probability phrases, such as
""much more likely"" or ""a little less likely."" Subjects selected such phrases to
best describe numerical probability updates. We examined three hypotheses about
the correspondence, and found the most descriptively accurate of these three to
be that each such phrase corresponds to a fixed difference in probability
(rather than fixed ratio of probabilities or of odds). The empirically derived
phrase selection function uses eight phrases and achieved a 72% accuracy in
correspondence with the subjects' actual usage.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:47 GMT""}]","2013-04-08"
"1304.1502","Henri Farrency","Henri Farrency, Henri Prade","Positive and Negative Explanations of Uncertain Reasoning in the
  Framework of Possibility Theory","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-95-101","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents an approach for developing the explanation capabilities
of rule-based expert systems managing imprecise and uncertain knowledge. The
treatment of uncertainty takes place in the framework of possibility theory
where the available information concerning the value of a logical or numerical
variable is represented by a possibility distribution which restricts its more
or less possible values. We first discuss different kinds of queries asking for
explanations before focusing on the two following types : i) how, a particular
possibility distribution is obtained (emphasizing the main reasons only) ; ii)
why in a computed possibility distribution, a particular value has received a
possibility degree which is so high, so low or so contrary to the expectation.
The approach is based on the exploitation of equations in max-min algebra. This
formalism includes the limit case of certain and precise information.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:53 GMT""}]","2013-04-08"
"1304.1503","Kenneth W. Fertig","Kenneth W. Fertig, John S. Breese","Interval Influence Diagrams","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-102-111","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We describe a mechanism for performing probabilistic reasoning in influence
diagrams using interval rather than point valued probabilities. We derive the
procedures for node removal (corresponding to conditional expectation) and arc
reversal (corresponding to Bayesian conditioning) in influence diagrams where
lower bounds on probabilities are stored at each node. The resulting bounds for
the transformed diagram are shown to be optimal within the class of constraints
on probability distributions that can be expressed exclusively as lower bounds
on the component probabilities of the diagram. Sequences of these operations
can be performed to answer probabilistic queries with indeterminacies in the
input and for performing sensitivity analysis on an influence diagram. The
storage requirements and computational complexity of this approach are
comparable to those for point-valued probabilistic inference mechanisms, making
the approach attractive for performing sensitivity analysis and where
probability information is not available. Limited empirical data on an
implementation of the methodology are provided.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:37:59 GMT""}]","2013-04-08"
"1304.1504","Robert Fung","Robert Fung, Kuo-Chu Chang","Weighing and Integrating Evidence for Stochastic Simulation in Bayesian
  Networks","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-112-117","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Stochastic simulation approaches perform probabilistic inference in Bayesian
networks by estimating the probability of an event based on the frequency that
the event occurs in a set of simulation trials. This paper describes the
evidence weighting mechanism, for augmenting the logic sampling stochastic
simulation algorithm [Henrion, 1986]. Evidence weighting modifies the logic
sampling algorithm by weighting each simulation trial by the likelihood of a
network's evidence given the sampled state node values for that trial. We also
describe an enhancement to the basic algorithm which uses the evidential
integration technique [Chin and Cooper, 1987]. A comparison of the basic
evidence weighting mechanism with the Markov blanket algorithm [Pearl, 1987],
the logic sampling algorithm, and the evidence integration algorithm is
presented. The comparison is aided by analyzing the performance of the
algorithms in a simple example network.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:05 GMT""}]","2013-04-08"
"1304.1505","Dan Geiger","Dan Geiger, Tom S. Verma, Judea Pearl","d-Separation: From Theorems to Algorithms","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-118-125","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An efficient algorithm is developed that identifies all independencies
implied by the topology of a Bayesian network. Its correctness and maximality
stems from the soundness and completeness of d-separation with respect to
probability theory. The algorithm runs in time O (l E l) where E is the number
of edges in the network.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:11 GMT""}]","2013-04-08"
"1304.1506","Maria Angeles Gil","Maria Angeles Gil, Pramod Jain","The Effects of Perfect and Sample Information on Fuzzy Utilities in
  Decision-Making","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-126-133","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we first consider a Bayesian framework and model the ""utility
function"" in terms of fuzzy random variables. On the basis of this model, we
define the ""prior (fuzzy) expected utility"" associated with each action, and
the corresponding ""posterior (fuzzy) expected utility given sample information
from a random experiment"". The aim of this paper is to analyze how sample
information can affect the expected utility. In this way, by using some fuzzy
preference relations, we conclude that sample information allows a decision
maker to increase the expected utility on the average. The upper bound on the
value of the expected utility is when the decision maker has perfect
information. Applications of this work to the field of artificial intelligence
are presented through two examples.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:16 GMT""}]","2013-04-08"
"1304.1507","Moises Goldszmidt","Moises Goldszmidt, Judea Pearl","Deciding Consistency of Databases Containing Defeasible and Strict
  Information","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-134-141","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose a norm of consistency for a mixed set of defeasible and strict
sentences, based on a probabilistic semantics. This norm establishes a clear
distinction between knowledge bases depicting exceptions and those containing
outright contradictions. We then define a notion of entailment based also on
probabilistic considerations and provide a characterization of the relation
between consistency and entailment. We derive necessary and sufficient
conditions for consistency, and provide a simple decision procedure for testing
consistency and deciding whether a sentence is entailed by a database. Finally,
it is shown that if al1 sentences are Horn clauses, consistency and entailment
can be tested in polynomial time.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:23 GMT""}]","2013-04-08"
"1304.1508","Joseph Y. Halpern","Joseph Y. Halpern","The Relationship between Knowledge, Belief and Certainty","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-142-151","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We consider the relation between knowledge and certainty, where a fact is
known if it is true at all worlds an agent considers possible and is certain if
it holds with probability 1. We identify certainty with probabilistic belief.
We show that if we assume one fixed probability assignment, then the logic
KD45, which has been identified as perhaps the most appropriate for belief,
provides a complete axiomatization for reasoning about certainty. Just as an
agent may believe a fact although phi is false, he may be certain that a fact
phi, is true although phi is false. However, it is easy to see that an agent
can have such false (probabilistic) beliefs only at a set of worlds of
probability 0. If we restrict attention to structures where all worlds have
positive probability, then S5 provides a complete axiomatization. If we
consider a more general setting, where there might be a different probability
assignment at each world, then by placing appropriate conditions on the support
of the probability function (the set of worlds which have non-zero
probability), we can capture many other well-known modal logics, such as T and
S4. Finally, we consider which axioms characterize structures satisfying
Miller's principle.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:29 GMT""}]","2013-04-08"
"1304.1509","Othar Hansson","Othar Hansson, Andy Mayer","Heuristic Search as Evidential Reasoning","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-152-161","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  BPS, the Bayesian Problem Solver, applies probabilistic inference and
decision-theoretic control to flexible, resource-constrained problem-solving.
This paper focuses on the Bayesian inference mechanism in BPS, and contrasts it
with those of traditional heuristic search techniques. By performing sound
inference, BPS can outperform traditional techniques with significantly less
computational effort. Empirical tests on the Eight Puzzle show that after only
a few hundred node expansions, BPS makes better decisions than does the best
existing algorithm after several million node expansions
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:35 GMT""}]","2013-04-08"
"1304.1510","David Heckerman","David Heckerman, John S. Breese, Eric J. Horvitz","The Compilation of Decision Models","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-162-173","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce and analyze the problem of the compilation of decision models
from a decision-theoretic perspective. The techniques described allow us to
evaluate various configurations of compiled knowledge given the nature of
evidential relationships in a domain, the utilities associated with alternative
actions, the costs of run-time delays, and the costs of memory. We describe
procedures for selecting a subset of the total observations available to be
incorporated into a compiled situation-action mapping, in the context of a
binary decision with conditional independence of evidence. The methods allow us
to incrementally select the best pieces of evidence to add to the set of
compiled knowledge in an engineering setting. After presenting several
approaches to compilation, we exercise one of the methods to provide insight
into the relationship between the distribution over weights of evidence and the
preferred degree of compilation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:41 GMT""}]","2013-04-08"
"1304.1511","David Heckerman","David Heckerman","A Tractable Inference Algorithm for Diagnosing Multiple Diseases","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-174-181","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We examine a probabilistic model for the diagnosis of multiple diseases. In
the model, diseases and findings are represented as binary variables. Also,
diseases are marginally independent, features are conditionally independent
given disease instances, and diseases interact to produce findings via a noisy
OR-gate. An algorithm for computing the posterior probability of each disease,
given a set of observed findings, called quickscore, is presented. The time
complexity of the algorithm is O(nm-2m+), where n is the number of diseases, m+
is the number of positive findings and m- is the number of negative findings.
Although the time complexity of quickscore i5 exponential in the number of
positive findings, the algorithm is useful in practice because the number of
observed positive findings is usually far less than the number of diseases
under consideration. Performance results for quickscore applied to a
probabilistic version of Quick Medical Reference (QMR) are provided.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:47 GMT""},{""version"":""v2"",""created"":""Mon, 5 Dec 2022 23:49:18 GMT""}]","2022-12-07"
"1304.1512","Eric J. Horvitz","Eric J. Horvitz, Jaap Suermondt, Gregory F. Cooper","Bounded Conditioning: Flexible Inference for Decisions under Scarce
  Resources","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-182-193","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce a graceful approach to probabilistic inference called bounded
conditioning. Bounded conditioning monotonically refines the bounds on
posterior probabilities in a belief network with computation, and converges on
final probabilities of interest with the allocation of a complete resource
fraction. The approach allows a reasoner to exchange arbitrary quantities of
computational resource for incremental gains in inference quality. As such,
bounded conditioning holds promise as a useful inference technique for
reasoning under the general conditions of uncertain and varying reasoning
resources. The algorithm solves a probabilistic bounding problem in complex
belief networks by breaking the problem into a set of mutually exclusive,
tractable subproblems and ordering their solution by the expected effect that
each subproblem will have on the final answer. We introduce the algorithm,
discuss its characterization, and present its performance on several belief
networks, including a complex model for reasoning about problems in
intensive-care medicine.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:53 GMT""}]","2013-04-08"
"1304.1513","A. C. Kak","A. C. Kak, K. M. Andress, C. Lopez-Abadia, M. S. Carroll, J. R. Lewis","Hierarchical Evidence Accumulation in the Pseiki System and Experiments
  in Model-Driven Mobile Robot Navigation","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-194-207","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we will review the process of evidence accumulation in the
PSEIKI system for expectation-driven interpretation of images of 3-D scenes.
Expectations are presented to PSEIKI as a geometrical hierarchy of
abstractions. PSEIKI's job is then to construct abstraction hierarchies in the
perceived image taking cues from the abstraction hierarchies in the
expectations. The Dempster-Shafer formalism is used for associating belief
values with the different possible labels for the constructed abstractions in
the perceived image. This system has been used successfully for autonomous
navigation of a mobile robot in indoor environments.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:38:59 GMT""}]","2013-04-08"
"1304.1514","Harold P. Lehmann","Harold P. Lehmann","A Decision-Theoretic Model for Using Scientific Data","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-208-215","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Many Artificial Intelligence systems depend on the agent's updating its
beliefs about the world on the basis of experience. Experiments constitute one
type of experience, so scientific methodology offers a natural environment for
examining the issues attendant to using this class of evidence. This paper
presents a framework which structures the process of using scientific data from
research reports for the purpose of making decisions, using decision analysis
as the basis for the structure and using medical research as the general
scientific domain. The structure extends the basic influence diagram for
updating belief in an object domain parameter of interest by expanding the
parameter into four parts: those of the patient, the population, the study
sample, and the effective study sample. The structure uses biases to perform
the transformation of one parameter into another, so that, for instance,
selection biases, in concert with the population parameter, yield the study
sample parameter. The influence diagram structure provides decision theoretic
justification for practices of good clinical research such as randomized
assignment and blindfolding of care providers. The model covers most research
designs used in medicine: case-control studies, cohort studies, and controlled
clinical trials, and provides an architecture to separate clearly between
statistical knowledge and domain knowledge. The proposed general model can be
the basis for clinical epidemiological advisory systems, when coupled with
heuristic pruning of irrelevant biases; of statistical workstations, when the
computational machinery for calculation of posterior distributions is added;
and of meta-analytic reviews, when multiple studies may impact on a single
population parameter.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:05 GMT""}]","2013-04-08"
"1304.1515","Paul E. Lehner","Paul E. Lehner, Theresa M. Mullin, Marvin S. Cohen","When Should a Decision Maker Ignore the Advice of a Decision Aid?","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-216-223","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper argues that the principal difference between decision aids and
most other types of information systems is the greater reliance of decision
aids on fallible algorithms--algorithms that sometimes generate incorrect
advice. It is shown that interactive problem solving with a decision aid that
is based on a fallible algorithm can easily result in aided performance which
is poorer than unaided performance, even if the algorithm, by itself, performs
significantly better than the unaided decision maker. This suggests that unless
certain conditions are satisfied, using a decision aid as an aid is
counterproductive. Some conditions under which a decision aid is best used as
an aid are derived.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:11 GMT""}]","2013-04-08"
"1304.1516","Paul E. Lehner","Paul E. Lehner","Inference Policies","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-224-232","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is suggested that an AI inference system should reflect an inference
policy that is tailored to the domain of problems to which it is applied -- and
furthermore that an inference policy need not conform to any general theory of
rational inference or induction. We note, for instance, that Bayesian reasoning
about the probabilistic characteristics of an inference domain may result in
the specification of an nonBayesian procedure for reasoning within the
inference domain. In this paper, the idea of an inference policy is explored in
some detail. To support this exploration, the characteristics of some standard
and nonstandard inference policies are examined.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:17 GMT""}]","2013-04-08"
"1304.1517","Tod S. Levitt","Tod S. Levitt, John Mark Agosta, Thomas O. Binford","Model-based Influence Diagrams for Machine Vision","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-233-244","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We show an approach to automated control of machine vision systems based on
incremental creation and evaluation of a particular family of influence
diagrams that represent hypotheses of imagery interpretation and possible
subsequent processing decisions. In our approach, model-based machine vision
techniques are integrated with hierarchical Bayesian inference to provide a
framework for representing and matching instances of objects and relationships
in imagery and for accruing probabilities to rank order conflicting scene
interpretations. We extend a result of Tatman and Shachter to show that the
sequence of processing decisions derived from evaluating the diagrams at each
stage is the same as the sequence that would have been derived by evaluating
the final influence diagram that contains all random variables created during
the run of the vision system.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:23 GMT""}]","2013-04-08"
"1304.1518","Ronald P. Loui","Ronald P. Loui","Defeasible Decisions: What the Proposal is and isn't","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-245-252","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In two recent papers, I have proposed a description of decision analysis that
differs from the Bayesian picture painted by Savage, Jeffrey and other classic
authors. Response to this view has been either overly enthusiastic or unduly
pessimistic. In this paper I try to place the idea in its proper place, which
must be somewhere in between. Looking at decision analysis as defeasible
reasoning produces a framework in which planning and decision theory can be
integrated, but work on the details has barely begun. It also produces a
framework in which the meta-decision regress can be stopped in a reasonable
way, but it does not allow us to ignore meta-level decisions. The heuristics
for producing arguments that I have presented are only supposed to be
suggestive; but they are not open to the egregious errors about which some have
worried. And though the idea is familiar to those who have studied heuristic
search, it is somewhat richer because the control of dialectic is more
interesting than the deepening of search.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:29 GMT""}]","2013-04-08"
"1304.1519","Mary McLeish","Mary McLeish, P. Yao, M. Cecile, T. Stirtzinger","Experiments Using Belief Functions and Weights of Evidence incorporating
  Statistical Data and Expert Opinions","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-253-264","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents some ideas and results of using uncertainty management
methods in the presence of data in preference to other statistical and machine
learning methods. A medical domain is used as a test-bed with data available
from a large hospital database system which collects symptom and outcome
information about patients. Data is often missing, of many variable types and
sample sizes for particular outcomes is not large. Uncertainty management
methods are useful for such domains and have the added advantage of allowing
for expert modification of belief values originally obtained from data.
Methodological considerations for using belief functions on statistical data
are dealt with in some detail. Expert opinions are Incorporated at various
levels of the project development and results are reported on an application to
liver disease diagnosis. Recent results contrasting the use of weights of
evidence and logistic regression on another medical domain are also presented.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:35 GMT""}]","2013-04-08"
"1304.1520","W. R. Moninger","W. R. Moninger, J. A. Flueck, C. Lusk, W. F. Roberts","Shootout-89: A Comparative Evaluation of Knowledge-based Systems that
  Forecast Severe Weather","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-265-271","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  During the summer of 1989, the Forecast Systems Laboratory of the National
Oceanic and Atmospheric Administration sponsored an evaluation of artificial
intelligence-based systems that forecast severe convective storms. The
evaluation experiment, called Shootout-89, took place in Boulder, and focussed
on storms over the northeastern Colorado foothills and plains (Moninger, et
al., 1990). Six systems participated in Shootout-89. These included traditional
expert systems, an analogy-based system, and a system developed using methods
from the cognitive science/judgment analysis tradition. Each day of the
exercise, the systems generated 2 to 9 hour forecasts of the probabilities of
occurrence of: non significant weather, significant weather, and severe
weather, in each of four regions in northeastern Colorado. A verification
coordinator working at the Denver Weather Service Forecast Office gathered
ground-truth data from a network of observers. Systems were evaluated on the
basis of several measures of forecast skill, and on other metrics such as
timeliness, ease of learning, and ease of use. Systems were generally easy to
operate, however the various systems required substantially different levels of
meteorological expertise on the part of their users--reflecting the various
operational environments for which the systems had been designed. Systems
varied in their statistical behavior, but on this difficult forecast problem,
the systems generally showed a skill approximately equal to that of persistence
forecasts and climatological (historical frequency) forecasts. The two systems
that appeared best able to discriminate significant from non significant
weather events were traditional expert systems. Both of these systems required
the operator to make relatively sophisticated meteorological judgments. We are
unable, based on only one summer's worth of data, to determine the extent to
which the greater skill of the two systems was due to the content of their
knowledge bases, or to the subjective judgments of the operator. A follow-on
experiment, Shootout-91, is currently being planned. Interested potential
participants are encouraged to contact the author at the address above.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:40 GMT""}]","2013-04-08"
"1304.1521","Eric Neufeld","Eric Neufeld, J. D. Horton","Conditioning on Disjunctive Knowledge: Defaults and Probabilities","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-272-278","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Many writers have observed that default logics appear to contain the ""lottery
paradox"" of probability theory. This arises when a default ""proof by
contradiction"" lets us conclude that a typical X is not a Y where Y is an
unusual subclass of X. We show that there is a similar problem with default
""proof by cases"" and construct a setting where we might draw a different
conclusion knowing a disjunction than we would knowing any particular disjunct.
Though Reiter's original formalism is capable of representing this distinction,
other approaches are not. To represent and reason about this case, default
logicians must specify how a ""typical"" individual is selected. The problem is
closely related to Simpson's paradox of probability theory. If we accept a
simple probabilistic account of defaults based on the notion that one
proposition may favour or increase belief in another, the ""multiple extension
problem"" for both conjunctive and disjunctive knowledge vanishes.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:46 GMT""}]","2013-04-08"
"1304.1522","Michael Pittarelli","Michael Pittarelli","Maximum Uncertainty Procedures for Interval-Valued Probability
  Distributions","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-279-286","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Measures of uncertainty and divergence are introduced for interval-valued
probability distributions and are shown to have desirable mathematical
properties. A maximum uncertainty inference procedure for marginal interval
distributions is presented. A technique for reconstruction of interval
distributions from projections is developed based on this inference procedure
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:51 GMT""}]","2013-04-08"
"1304.1523","Gregory M. Provan","Gregory M. Provan","A Logical Interpretation of Dempster-Shafer Theory, with Application to
  Visual Recognition","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-287-294","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We formulate Dempster Shafer Belief functions in terms of Propositional Logic
using the implicit notion of provability underlying Dempster Shafer Theory.
Given a set of propositional clauses, assigning weights to certain
propositional literals enables the Belief functions to be explicitly computed
using Network Reliability techniques. Also, the logical procedure corresponding
to updating Belief functions using Dempster's Rule of Combination is shown.
This analysis formalizes the implementation of Belief functions within an
Assumption-based Truth Maintenance System (ATMS). We describe the extension of
an ATMS-based visual recognition system, VICTORS, with this logical formulation
of Dempster Shafer theory. Without Dempster Shafer theory, VICTORS computes all
possible visual interpretations (i.e. all logical models) without determining
the best interpretation(s). Incorporating Dempster Shafer theory enables
optimal visual interpretations to be computed and a logical semantics to be
maintained.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:39:57 GMT""}]","2013-04-08"
"1304.1524","Peter Sember","Peter Sember, Ingrid Zukerman","Strategies for Generating Micro Explanations for Bayesian Belief
  Networks","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-295-302","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Bayesian Belief Networks have been largely overlooked by Expert Systems
practitioners on the grounds that they do not correspond to the human inference
mechanism. In this paper, we introduce an explanation mechanism designed to
generate intuitive yet probabilistically sound explanations of inferences drawn
by a Bayesian Belief Network. In particular, our mechanism accounts for the
results obtained due to changes in the causal and the evidential support of a
node.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:04 GMT""}]","2013-04-08"
"1304.1525","Ross D. Shachter","Ross D. Shachter","Evidence Absorption and Propagation through Evidence Reversals","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-303-310","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The arc reversal/node reduction approach to probabilistic inference is
extended to include the case of instantiated evidence by an operation called
""evidence reversal."" This not only provides a technique for computing posterior
joint distributions on general belief networks, but also provides insight into
the methods of Pearl [1986b] and Lauritzen and Spiegelhalter [1988]. Although
it is well understood that the latter two algorithms are closely related, in
fact all three algorithms are identical whenever the belief network is a
forest.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:10 GMT""}]","2013-04-08"
"1304.1526","Ross D. Shachter","Ross D. Shachter, Mark Alan Peot","Simulation Approaches to General Probabilistic Inference on Belief
  Networks","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-311-318","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A number of algorithms have been developed to solve probabilistic inference
problems on belief networks. These algorithms can be divided into two main
groups: exact techniques which exploit the conditional independence revealed
when the graph structure is relatively sparse, and probabilistic sampling
techniques which exploit the ""conductance"" of an embedded Markov chain when the
conditional probabilities have non-extreme values. In this paper, we
investigate a family of ""forward"" Monte Carlo sampling techniques similar to
Logic Sampling [Henrion, 1988] which appear to perform well even in some
multiply connected networks with extreme conditional probabilities, and thus
would be generally applicable. We consider several enhancements which reduce
the posterior variance using this approach and propose a framework and criteria
for choosing when to use those enhancements.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:16 GMT""}]","2013-04-08"
"1304.1527","Philippe Smets","Philippe Smets","Decision under Uncertainty","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-319-326","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We derive axiomatically the probability function that should be used to make
decisions given any form of underlying uncertainty.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:21 GMT""}]","2013-04-08"
"1304.1528","Michael Smithson","Michael Smithson","Freedom: A Measure of Second-order Uncertainty for Intervalic
  Probability Schemes","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-327-334","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper discusses a new measure that is adaptable to certain intervalic
probability frameworks, possibility theory, and belief theory. As such, it has
the potential for wide use in knowledge engineering, expert systems, and
related problems in the human sciences. This measure (denoted here by F) has
been introduced in Smithson (1988) and is more formally discussed in Smithson
(1989a)o Here, I propose to outline the conceptual basis for F and compare its
properties with other measures of second-order uncertainty. I will argue that F
is an indicator of nonspecificity or alternatively, of freedom, as
distinguished from either ambiguity or vagueness.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:27 GMT""}]","2013-04-08"
"1304.1529","David J. Spiegelhalter","David J. Spiegelhalter, Rodney C. Franklin, Kate Bull","Assessment, Criticism and Improvement of Imprecise Subjective
  Probabilities for a Medical Expert System","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-335-342","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Three paediatric cardiologists assessed nearly 1000 imprecise subjective
conditional probabilities for a simple belief network representing congenital
heart disease, and the quality of the assessments has been measured using
prospective data on 200 babies. Quality has been assessed by a Brier scoring
rule, which decomposes into terms measuring lack of discrimination and
reliability. The results are displayed for each of 27 diseases and 24
questions, and generally the assessments are reliable although there was a
tendency for the probabilities to be too extreme. The imprecision allows the
judgements to be converted to implicit samples, and by combining with the
observed data the probabilities naturally adapt with experience. This appears
to be a practical procedure even for reasonably large expert systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:33 GMT""}]","2013-04-08"
"1304.1530","Sampath Srinivas","Sampath Srinivas, Stuart Russell, Alice M. Agogino","Automated Construction of Sparse Bayesian Networks from Unstructured
  Probabilistic Models and Domain Information","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-343-350","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An algorithm for automated construction of a sparse Bayesian network given an
unstructured probabilistic model and causal domain information from an expert
has been developed and implemented. The goal is to obtain a network that
explicitly reveals as much information regarding conditional independence as
possible. The network is built incrementally adding one node at a time. The
expert's information and a greedy heuristic that tries to keep the number of
arcs added at each step to a minimum are used to guide the search for the next
node to add. The probabilistic model is a predicate that can answer queries
about independencies in the domain. In practice the model can be implemented in
various ways. For example, the model could be a statistical independence test
operating on empirical data or a deductive prover operating on a set of
independence statements about the domain.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:38 GMT""}]","2013-04-08"
"1304.1531","Thomas M. Strat","Thomas M. Strat","Making Decisions with Belief Functions","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-351-360","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A primary motivation for reasoning under uncertainty is to derive decisions
in the face of inconclusive evidence. However, Shafer's theory of belief
functions, which explicitly represents the underconstrained nature of many
reasoning problems, lacks a formal procedure for making decisions. Clearly,
when sufficient information is not available, no theory can prescribe actions
without making additional assumptions. Faced with this situation, some
assumption must be made if a clearly superior choice is to emerge. In this
paper we offer a probabilistic interpretation of a simple assumption that
disambiguates decision problems represented with belief functions. We prove
that it yields expected values identical to those obtained by a probabilistic
analysis that makes the same assumption. In addition, we show how the decision
analysis methodology frequently employed in probabilistic reasoning can be
extended for use with belief functions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:45 GMT""}]","2013-04-08"
"1304.1532","Michael J. Swain","Michael J. Swain, Lambert E. Wixson, Paul B. Chou","Efficient Parallel Estimation for Markov Random Fields","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-361-368","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a new, deterministic, distributed MAP estimation algorithm for
Markov Random Fields called Local Highest Confidence First (Local HCF). The
algorithm has been applied to segmentation problems in computer vision and its
performance compared with stochastic algorithms. The experiments show that
Local HCF finds better estimates than stochastic algorithms with much less
computation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:50 GMT""}]","2013-04-08"
"1304.1533","David S. Vaughan","David S. Vaughan, Bruce M. Perrin, Robert M. Yadrick","Comparing Expert Systems Built Using Different Uncertain Inference
  Systems","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-369-376","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This study compares the inherent intuitiveness or usability of the most
prominent methods for managing uncertainty in expert systems, including those
of EMYCIN, PROSPECTOR, Dempster-Shafer theory, fuzzy set theory, simplified
probability theory (assuming marginal independence), and linear regression
using probability estimates. Participants in the study gained experience in a
simple, hypothetical problem domain through a series of learning trials. They
were then randomly assigned to develop an expert system using one of the six
Uncertain Inference Systems (UISs) listed above. Performance of the resulting
systems was then compared. The results indicate that the systems based on the
PROSPECTOR and EMYCIN models were significantly less accurate for certain types
of problems compared to systems based on the other UISs. Possible reasons for
these differences are discussed.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:40:56 GMT""}]","2013-04-08"
"1304.1534","Wilson X. Wen","Wilson X. Wen","Directed Cycles in Belief Networks","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-377-384","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The most difficult task in probabilistic reasoning may be handling directed
cycles in belief networks. To the best knowledge of this author, there is no
serious discussion of this problem at all in the literature of probabilistic
reasoning so far.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:02 GMT""}]","2013-04-08"
"1304.1535","Yang Xiang","Yang Xiang, Michael P. Beddoes, David L Poole","Can Uncertainty Management be Realized in a Finite Totally Ordered
  Probability Algebra?","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-385-393","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, the feasibility of using finite totally ordered probability
models under Alelinnas's Theory of Probabilistic Logic [Aleliunas, 1988] is
investigated. The general form of the probability algebra of these models is
derived and the number of possible algebras with given size is deduced. Based
on this analysis, we discuss problems of denominator-indifference and
ambiguity-generation that arise in reasoning by cases and abductive reasoning.
An example is given that illustrates how these problems arise. The
investigation shows that a finite probability model may be of very limited
usage.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:07 GMT""}]","2013-04-08"
"1304.1536","Ronald R. Yager","Ronald R. Yager","Normalization and the Representation of Nonmonotonic Knowledge in the
  Theory of Evidence","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)",,,"UAI-P-1989-PG-394-403","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We discuss the Dempster-Shafer theory of evidence. We introduce a concept of
monotonicity which is related to the diminution of the range between belief and
plausibility. We show that the accumulation of knowledge in this framework
exhibits a nonmonotonic property. We show how the belief structure can be used
to represent typical or commonsense knowledge.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:14 GMT""}]","2013-04-08"
"1304.1785","Farzad Hessar","Farzad Hessar and Sumit Roy","Capacity Considerations for Secondary Networks in TV White Space",,,,,"cs.NI cs.IT math.IT","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The so-called `TV white spaces' (TVWS) - representing unused TV channels in
any given location as the result of the transition to digital broadcasting -
designated by U.S. Federal Communications Commission (FCC) for unlicensed use
presents significant new opportunities within the context of emerging 4G
networks for developing new wireless access technologies that meet the goals of
the US National Broadband Plan (notably true broadband access for an increasing
fraction of the population). There are multiple challenges in realizing this
goal; the most fundamental being the fact that the available WS capacity is
currently not accurately known, since it depends on a multiplicity of factors -
including system parameters of existing incumbents (broadcasters), propagation
characteristics of local terrain as well as FCC rules. In this paper, we
explore the capacity of white space networks by developing a detailed model
that includes all the major variables, and is cognizant of FCC regulations that
provide constraints on incumbent protection. Real terrain information and
propagation models for the primary broadcaster and adjacent channel
interference from TV transmitters are included to estimate their impact on
achievable WS capacity. The model is later used to explore various trade-offs
between network capacity and system parameters and suggest possible amendments
to FCC's incumbent protection rules in the favor of furthering white space
capacity.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 03:14:07 GMT""}]","2013-04-08"
"1304.2277","Alexei Nesteruk Dr","Alexei V. Nesteruk","A Participatory Universe of J. A. Wheeler as an Intentional Correlate of
  Embodied Subjects and an Example of Purposiveness in Physics","22 pages, 1 figure","Journal of Siberian Federal University. Humanities & Social
  Sciences, vol. 6, n. 3, 2013, pp. 415-437",,,"physics.gen-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper investigates the role of human subjectivity and its delimiters in
articulating the universe in physics and cosmology. As a case study, we reflect
upon the complex of ideas of the so called Participatory Universe by later J.
A. Wheeler. The objective of the paper is to explicate the role of the human
agency as a centre of disclosure and manifestation of the universe as well the
as teleology of scientific representation of the world implied by the intrinsic
purposiveness of human actions.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 14:06:36 GMT""}]","2013-04-09"
"1304.2339","John Mark Agosta","John Mark Agosta","The structure of Bayes nets for vision recognition","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-1-7","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper is part of a study whose goal is to show the effciency of using
Bayes networks to carry out model based vision calculations. [Binford et al.
1987] Recognition proceeds by drawing up a network model from the object's
geometric and functional description that predicts the appearance of an object.
Then this network is used to find the object within a photographic image. Many
existing and proposed techniques for vision recognition resemble the
uncertainty calculations of a Bayes net. In contrast, though, they lack a
derivation from first principles, and tend to rely on arbitrary parameters that
we hope to avoid by a network model. The connectedness of the network depends
on what independence considerations can be identified in the vision problem.
Greater independence leads to easier calculations, at the expense of the net's
expressiveness. Once this trade-off is made and the structure of the network is
determined, it should be possible to tailor a solution technique for it. This
paper explores the use of a network with multiply connected paths, drawing on
both techniques of belief networks [Pearl 86] and influence diagrams. We then
demonstrate how one formulation of a multiply connected network can be solved.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:36 GMT""}]","2013-04-10"
"1304.2340","Romas Aleliunas","Romas Aleliunas","Summary of A New Normative Theory of Probabilistic Logic","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-8-14","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  By probabilistic logic I mean a normative theory of belief that explains how
a body of evidence affects one's degree of belief in a possible hypothesis. A
new axiomatization of such a theory is presented which avoids a finite
additivity axiom, yet which retains many useful inference rules. Many of the
examples of this theory--its models do not use numerical probabilities. Put
another way, this article gives sharper answers to the two questions: 1.What
kinds of sets can used as the range of a probability function? 2.Under what
conditions is the range set of a probability function isomorphic to the set of
real numbers in the interval 10,1/ with the usual arithmetical operations?
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:42 GMT""}]","2013-04-10"
"1304.2341","Fahiem Bacchus","Fahiem Bacchus","Probability Distributions Over Possible Worlds","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-15-21","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In Probabilistic Logic Nilsson uses the device of a probability distribution
over a set of possible worlds to assign probabilities to the sentences of a
logical language. In his paper Nilsson concentrated on inference and associated
computational issues. This paper, on the other hand, examines the probabilistic
semantics in more detail, particularly for the case of first-order languages,
and attempts to explain some of the features and limitations of this form of
probability logic. It is pointed out that the device of assigning probabilities
to logical sentences has certain expressive limitations. In particular,
statistical assertions are not easily expressed by such a device. This leads to
certain difficulties with attempts to give probabilistic semantics to default
reasoning using probabilities assigned to logical sentences.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:48 GMT""}]","2013-04-10"
"1304.2342","Paul K. Black","Paul K. Black, Kathryn Blackmond Laskey","Hierarchical Evidence and Belief Functions","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-22-29","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dempster/Shafer (D/S) theory has been advocated as a way of representing
incompleteness of evidence in a system's knowledge base. Methods now exist for
propagating beliefs through chains of inference. This paper discusses how rules
with attached beliefs, a common representation for knowledge in automated
reasoning systems, can be transformed into the joint belief functions required
by propagation algorithms. A rule is taken as defining a conditional belief
function on the consequent given the antecedents. It is demonstrated by example
that different joint belief functions may be consistent with a given set of
rules. Moreover, different representations of the same rules may yield
different beliefs on the consequent hypotheses.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:53 GMT""}]","2013-04-10"
"1304.2343","John S. Breese","John S. Breese, Michael R. Fehling","Decision-Theoretic Control of Problem Solving: Principles and
  Architecture","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-30-37","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents an approach to the design of autonomous, real-time
systems operating in uncertain environments. We address issues of problem
solving and reflective control of reasoning under uncertainty in terms of two
fundamental elements: l) a set of decision-theoretic models for selecting among
alternative problem-solving methods and 2) a general computational architecture
for resource-bounded problem solving. The decisiontheoretic models provide a
set of principles for choosing among alternative problem-solving methods based
on their relative costs and benefits, where benefits are characterized in terms
of the value of information provided by the output of a reasoning activity. The
output may be an estimate of some uncertain quantity or a recommendation for
action. The computational architecture, called Schemer-ll, provides for
interleaving of and communication among various problem-solving subsystems.
These subsystems provide alternative approaches to information gathering,
belief refinement, solution construction, and solution execution. In
particular, the architecture provides a mechanism for interrupting the
subsystems in response to critical events. We provide a decision theoretic
account for scheduling problem-solving elements and for critical-event-driven
interruption of activities in an architecture such as Schemer-II.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:41:57 GMT""}]","2013-04-10"
"1304.2344","M. Cecile","M. Cecile, Mary McLeish, P. Pascoe, W. Taylor","Induction and Uncertainty Management Techniques Applied to Veterinary
  Medical Diagnosis","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-38-48","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper discusses a project undertaken between the Departments of
Computing Science, Statistics, and the College of Veterinary Medicine to design
a medical diagnostic system. On-line medical data has been collected in the
hospital database system for several years. A number of induction methods are
being used to extract knowledge from the data in an attempt to improve upon
simple diagnostic charts used by the clinicians. They also enhance the results
of classical statistical methods - finding many more significant variables. The
second part of the paper describes an essentially Bayesian method of evidence
combination using fuzzy events at an initial step. Results are presented and
comparisons are made with other methods.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:03 GMT""}]","2013-04-10"
"1304.2345","R. Martin Chavez","R. Martin Chavez, Gregory F. Cooper","KNET: Integrating Hypermedia and Bayesian Modeling","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-49-54","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  KNET is a general-purpose shell for constructing expert systems based on
belief networks and decision networks. Such networks serve as graphical
representations for decision models, in which the knowledge engineer must
define clearly the alternatives, states, preferences, and relationships that
constitute a decision basis. KNET contains a knowledge-engineering core written
in Object Pascal and an interface that tightly integrates HyperCard, a
hypertext authoring tool for the Apple Macintosh computer, into a novel
expert-system architecture. Hypertext and hypermedia have become increasingly
important in the storage management, and retrieval of information. In broad
terms, hypermedia deliver heterogeneous bits of information in dynamic,
extensively cross-referenced packages. The resulting KNET system features a
coherent probabilistic scheme for managing uncertainty, an objectoriented
graphics editor for drawing and manipulating decision networks, and HyperCard's
potential for quickly constructing flexible and friendly user interfaces. We
envision KNET as a useful prototyping tool for our ongoing research on a
variety of Bayesian reasoning problems, including tractable representation,
inference, and explanation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:09 GMT""}]","2013-04-10"
"1304.2346","Gregory F. Cooper","Gregory F. Cooper","A Method for Using Belief Networks as Influence Diagrams","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-55-63","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper demonstrates a method for using belief-network algorithms to solve
influence diagram problems. In particular, both exact and approximation
belief-network algorithms may be applied to solve influence-diagram problems.
More generally, knowing the relationship between belief-network and
influence-diagram problems may be useful in the design and development of more
efficient influence diagram algorithms.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:15 GMT""}]","2013-04-10"
"1304.2347","Bruce D'Ambrosio","Bruce D'Ambrosio","Process, Structure, and Modularity in Reasoning with Uncertainty","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-64-72","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Computational mechanisms for uncertainty management must support interactive
and incremental problem formulation, inference, hypothesis testing, and
decision making. However, most current uncertainty inference systems
concentrate primarily on inference, and provide no support for the larger
issues. We present a computational approach to uncertainty management which
provides direct support for the dynamic, incremental aspect of this task, while
at the same time permitting direct representation of the structure of
evidential relationships. At the same time, we show that this approach responds
to the modularity concerns of Heckerman and Horvitz [Heck87]. This paper
emphasizes examples of the capabilities of this approach. Another paper
[D'Am89] details the representations and algorithms involved.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:20 GMT""}]","2013-04-10"
"1304.2348","Thomas L. Dean","Thomas L. Dean, Keiji Kanazawa","Probabilistic Causal Reasoning","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-73-80","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Predicting the future is an important component of decision making. In most
situations, however, there is not enough information to make accurate
predictions. In this paper, we develop a theory of causal reasoning for
predictive inference under uncertainty. We emphasize a common type of
prediction that involves reasoning about persistence: whether or not a
proposition once made true remains true at some later time. We provide a
decision procedure with a polynomial-time algorithm for determining the
probability of the possible consequences of a set events and initial
conditions. The integration of simple probability theory with temporal
projection enables us to circumvent problems that nonmonotonic temporal
reasoning schemes have in dealing with persistence. The ideas in this paper
have been implemented in a prototype system that refines a database of causal
rules in the course of applying those rules to construct and carry out plans in
a manufacturing domain.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:26 GMT""}]","2013-04-10"
"1304.2349","Didier Dubois","Didier Dubois, Henri Prade","Modeling uncertain and vague knowledge in possibility and evidence
  theories","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-81-89","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper advocates the usefulness of new theories of uncertainty for the
purpose of modeling some facets of uncertain knowledge, especially vagueness,
in AI. It can be viewed as a partial reply to Cheeseman's (among others)
defense of probability.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:31 GMT""}]","2013-04-10"
"1304.2350","Soumitra Dutta","Soumitra Dutta","A Temporal Logic for Uncertain Events and An Outline of A Possible
  Implementation in An Extension of PROLOG","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-90-97","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There is uncertainty associated with the occurrence of many events in real
life. In this paper we develop a temporal logic to deal with such uncertain
events and outline a possible implementation in an extension of PROLOG. Events
are represented as fuzzy sets with the membership function giving the
possibility of occurrence of the event in a given interval of time. The
developed temporal logic is simple but powerful. It can determine effectively
the various temporal relations between uncertain events or their combinations.
PROLOG provides a uniform substrate on which to effectively implement such a
temporal logic for uncertain events
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:37 GMT""}]","2013-04-10"
"1304.2351","Christoph F. Eick","Christoph F. Eick","Uncertainty Management for Fuzzy Decision Support Systems","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-98-108","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A new approach for uncertainty management for fuzzy, rule based decision
support systems is proposed: The domain expert's knowledge is expressed by a
set of rules that frequently refer to vague and uncertain propositions. The
certainty of propositions is represented using intervals [a, b] expressing that
the proposition's probability is at least a and at most b. Methods and
techniques for computing the overall certainty of fuzzy compound propositions
that have been defined by using logical connectives 'and', 'or' and 'not' are
introduced. Different inference schemas for applying fuzzy rules by using modus
ponens are discussed. Different algorithms for combining evidence that has been
received from different rules for the same proposition are provided. The
relationship of the approach to other approaches is analyzed and its problems
of knowledge acquisition and knowledge representation are discussed in some
detail. The basic concepts of a rule-based programming language called PICASSO,
for which the approach is a theoretical foundation, are outlined.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:43 GMT""}]","2013-04-10"
"1304.2352","Alan M. Frisch","Alan M. Frisch, Peter Haddawy","Probability as a Modal Operator","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-109-118","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper argues for a modal view of probability. The syntax and semantics
of one particularly strong probability logic are discussed and some examples of
the use of the logic are provided. We show that it is both natural and useful
to think of probability as a modal operator. Contrary to popular belief in AI,
a probability ranging between 0 and 1 represents a continuum between
impossibility and necessity, not between simple falsity and truth. The present
work provides a clear semantics for quantification into the scope of the
probability operator and for higher-order probabilities. Probability logic is a
language for expressing both probabilistic and logical concepts.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:49 GMT""}]","2013-04-10"
"1304.2353","Li-Min Fu","Li-Min Fu","Truth Maintenance Under Uncertainty","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-119-126","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper addresses the problem of resolving errors under uncertainty in a
rule-based system. A new approach has been developed that reformulates this
problem as a neural-network learning problem. The strength and the fundamental
limitations of this approach are explored and discussed. The main result is
that neural heuristics can be applied to solve some but not all problems in
rule-based systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:42:55 GMT""}]","2013-04-10"
"1304.2354","Stephen I. Gallant","Stephen I. Gallant","Bayesian Assessment of a Connectionist Model for Fault Detection","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-127-135","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A previous paper [2] showed how to generate a linear discriminant network
(LDN) that computes likely faults for a noisy fault detection problem by using
a modification of the perceptron learning algorithm called the pocket
algorithm. Here we compare the performance of this connectionist model with
performance of the optimal Bayesian decision rule for the example that was
previously described. We find that for this particular problem the
connectionist model performs about 97% as well as the optimal Bayesian
procedure. We then define a more general class of noisy single-pattern boolean
(NSB) fault detection problems where each fault corresponds to a single
:pattern of boolean instrument readings and instruments are independently
noisy. This is equivalent to specifying that instrument readings are
probabilistic but conditionally independent given any particular fault. We
prove:
  1. The optimal Bayesian decision rule for every NSB fault detection problem
is representable by an LDN containing no intermediate nodes. (This slightly
extends a result first published by Minsky & Selfridge.) 2. Given an NSB fault
detection problem, then with arbitrarily high probability after sufficient
iterations the pocket algorithm will generate an LDN that computes an optimal
Bayesian decision rule for that problem. In practice we find that a reasonable
number of iterations of the pocket algorithm produces a network with good, but
not optimal, performance.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:01 GMT""}]","2013-04-10"
"1304.2355","Dan Geiger","Dan Geiger, Judea Pearl","On the Logic of Causal Models","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-136-147","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper explores the role of Directed Acyclic Graphs (DAGs) as a
representation of conditional independence relationships. We show that DAGs
offer polynomially sound and complete inference mechanisms for inferring
conditional independence relationships from a given causal set of such
relationships. As a consequence, d-separation, a graphical criterion for
identifying independencies in a DAG, is shown to uncover more valid
independencies then any other criterion. In addition, we employ the Armstrong
property of conditional independence to show that the dependence relationships
displayed by a DAG are inherently consistent, i.e. for every DAG D there exists
some probability distribution P that embodies all the conditional
independencies displayed in D and none other.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:07 GMT""}]","2013-04-10"
"1304.2356","Othar Hansson","Othar Hansson, Andy Mayer","The Optimality of Satisficing Solutions","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-148-157","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper addresses a prevailing assumption in single-agent heuristic search
theory- that problem-solving algorithms should guarantee shortest-path
solutions, which are typically called optimal. Optimality implies a metric for
judging solution quality, where the optimal solution is the solution with the
highest quality. When path-length is the metric, we will distinguish such
solutions as p-optimal.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:12 GMT""}]","2013-04-10"
"1304.2357","David Heckerman","David Heckerman","An Empirical Comparison of Three Inference Methods","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988). LaTex errors corrected in this version",,,"UAI-P-1988-PG-158-169","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, an empirical evaluation of three inference methods for
uncertain reasoning is presented in the context of Pathfinder, a large expert
system for the diagnosis of lymph-node pathology. The inference procedures
evaluated are (1) Bayes' theorem, assuming evidence is conditionally
independent given each hypothesis; (2) odds-likelihood updating, assuming
evidence is conditionally independent given each hypothesis and given the
negation of each hypothesis; and (3) a inference method related to the
Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic
metrics are used to compare the diagnostic accuracy of the inference methods.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:18 GMT""},{""version"":""v2"",""created"":""Sun, 17 May 2015 00:00:15 GMT""},{""version"":""v3"",""created"":""Tue, 24 Jan 2023 21:10:19 GMT""}]","2023-01-26"
"1304.2358","Daniel Hunter","Daniel Hunter","Parallel Belief Revision","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-170-177","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes a formal system of belief revision developed by Wolfgang
Spohn and shows that this system has a parallel implementation that can be
derived from an influence diagram in a manner similar to that in which Bayesian
networks are derived. The proof rests upon completeness results for an
axiomatization of the notion of conditional independence, with the Spohn system
being used as a semantics for the relation of conditional independence.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:24 GMT""}]","2013-04-10"
"1304.2359","Pramod Jain","Pramod Jain, Alice M. Agogino","Stochastic Sensitivity Analysis Using Fuzzy Influence Diagrams","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-178-188","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The practice of stochastic sensitivity analysis described in the decision
analysis literature is a testimonial to the need for considering deviations
from precise point estimates of uncertainty. We propose the use of Bayesian
fuzzy probabilities within an influence diagram computational scheme for
performing sensitivity analysis during the solution of probabilistic inference
and decision problems. Unlike other parametric approaches, the proposed scheme
does not require resolving the problem for the varying probability point
estimates. We claim that the solution to fuzzy influence diagrams provides as
much information as the classical point estimate approach plus additional
information concerning stochastic sensitivity. An example based on diagnostic
decision making in microcomputer assembly is used to illustrate this idea. We
claim that the solution to fuzzy influence diagrams provides as much
information as the classical point estimate approach plus additional interval
information that is useful for stochastic sensitivity analysis.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:30 GMT""}]","2013-04-10"
"1304.2360","Holly B. Jimison","Holly B. Jimison","A Representation of Uncertainty to Aid Insight into Decision Models","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-189-196","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Many real world models can be characterized as weak, meaning that there is
significant uncertainty in both the data input and inferences. This lack of
determinism makes it especially difficult for users of computer decision aids
to understand and have confidence in the models. This paper presents a
representation for uncertainty and utilities that serves as a framework for
graphical summary and computer-generated explanation of decision models. The
application described that tests the methodology is a computer decision aid
designed to enhance the clinician-patient consultation process for patients
with angina (chest pain due to lack of blood flow to the heart muscle). The
angina model is represented as a Bayesian decision network. Additionally, the
probabilities and utilities are treated as random variables with probability
distributions on their range of possible values. The initial distributions
represent information on all patients with anginal symptoms, and the approach
allows for rapid tailoring to more patientspecific distributions. This
framework provides a metric for judging the importance of each variable in the
model dynamically.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:36 GMT""}]","2013-04-10"
"1304.2361","Carl Kadie","Carl Kadie","Rational Nonmonotonic Reasoning","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-197-204","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Nonmonotonic reasoning is a pattern of reasoning that allows an agent to make
and retract (tentative) conclusions from inconclusive evidence. This paper
gives a possible-worlds interpretation of the nonmonotonic reasoning problem
based on standard decision theory and the emerging probability logic. The
system's central principle is that a tentative conclusion is a decision to make
a bet, not an assertion of fact. The system is rational, and as sound as the
proof theory of its underlying probability log.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:41 GMT""}]","2013-04-10"
"1304.2362","Jayant Kalagnanam","Jayant Kalagnanam, Max Henrion","A Comparison of Decision Analysis and Expert Rules for Sequential
  Diagnosis","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-205-212","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There has long been debate about the relative merits of decision theoretic
methods and heuristic rule-based approaches for reasoning under uncertainty. We
report an experimental comparison of the performance of the two approaches to
troubleshooting, specifically to test selection for fault diagnosis. We use as
experimental testbed the problem of diagnosing motorcycle engines. The first
approach employs heuristic test selection rules obtained from expert mechanics.
We compare it with the optimal decision analytic algorithm for test selection
which employs estimated component failure probabilities and test costs. The
decision analytic algorithm was found to reduce the expected cost (i.e. time)
to arrive at a diagnosis by an average of 14% relative to the expert rules.
Sensitivity analysis shows the results are quite robust to inaccuracy in the
probability and cost estimates. This difference suggests some interesting
implications for knowledge acquisition.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:47 GMT""}]","2013-04-10"
"1304.2363","Suk Wah Kwok","Suk Wah Kwok, Chris Carter","Multiple decision trees","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-213-220","cs.LG cs.AI stat.ML","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes experiments, on two domains, to investigate the effect
of averaging over predictions of multiple decision trees, instead of using a
single tree. Other authors have pointed out theoretical and commonsense reasons
for preferring the multiple tree approach. Ideally, we would like to consider
predictions from all trees, weighted by their probability. However, there is a
vast number of different trees, and it is difficult to estimate the probability
of each tree. We sidestep the estimation problem by using a modified version of
the ID3 algorithm to build good trees, and average over only these trees. Our
results are encouraging. For each domain, we managed to produce a small number
of good trees. We find that it is best to average across sets of trees with
different structure; this usually gives better performance than any of the
constituent trees, including the ID3 tree.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:53 GMT""}]","2013-04-10"
"1304.2364","Henry E. Kyburg Jr.","Henry E. Kyburg Jr","Probabilistic Inference and Probabilistic Reasoning","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-221-228","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Uncertainty enters into human reasoning and inference in at least two ways.
It is reasonable to suppose that there will be roles for these distinct uses of
uncertainty also in automated reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:43:58 GMT""}]","2013-04-10"
"1304.2365","Henry E. Kyburg Jr.","Henry E. Kyburg Jr","Probabilistic and Non-Monotonic Inference","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-229-236","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  (l) I have enough evidence to render the sentence S probable. (la) So,
relative to what I know, it is rational of me to believe S. (2) Now that I have
more evidence, S may no longer be probable. (2a) So now, relative to what I
know, it is not rational of me to believe S. These seem a perfectly ordinary,
common sense, pair of situations. Generally and vaguely, I take them to embody
what I shall call probabilistic inference. This form of inference is clearly
non-monotonic. Relatively few people have taken this form of inference, based
on high probability, to serve as a foundation for non-monotonic logic or for a
logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks
that sometimes probability has something to do with non-monotonic reasoning.
Judea Pearl [ 17] has recently been exploring the possibility. There are any
number of people whom one might call probability enthusiasts who feel that
probability provides all the answers by itself, with no need of help from
logic. Cheeseman [1], Henrion [5] and others think it useful to look at a
distribution of probabilities over a whole algebra of statements, to update
that distribution in the light of new evidence, and to use the latest updated
distribution of probability over the algebra as a basis for planning and
decision making. A slightly weaker form of this approach is captured by Nilsson
[15], where one assumes certain probabilities for certain statements, and
infers the probabilities, or constraints on the probabilities of other
statement. None of this corresponds to what I call probabilistic inference. All
of the inference that is taking place, either in Bayesian updating, or in
probabilistic logic, is strictly deductive. Deductive inference, particularly
that concerned with the distribution of classical probabilities or chances, is
of great importance. But this is not to say that there is no important role for
what earlier logicians have called ""ampliative"" or ""inductive"" or ""scientific""
inference, in which the conclusion goes beyond the premises, asserts more than
do the premises. This depends on what David Israel [6] has called ""real rules
of inference"". It is characteristic of any such logic or inference procedure
that it can go wrong: that statements accepted at one point may be rejected at
a later point. Research underlying the results reported here has been partially
supported by the Signals Warfare Center of the United States Army.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:04 GMT""}]","2016-11-26"
"1304.2366","Henry E. Kyburg Jr.","Henry E. Kyburg Jr","Epistemological Relevance and Statistical Knowledge","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-237-244","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  For many years, at least since McCarthy and Hayes (1969), writers have
lamented, and attempted to compensate for, the alleged fact that we often do
not have adequate statistical knowledge for governing the uncertainty of
belief, for making uncertain inferences, and the like. It is hardly ever
spelled out what ""adequate statistical knowledge"" would be, if we had it, and
how adequate statistical knowledge could be used to control and regulate
epistemic uncertainty.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:10 GMT""}]","2013-04-10"
"1304.2367","Tod S. Levitt","Tod S. Levitt, Thomas O. Binford, Gil J. Ettinger, Patrice Gelband","Utility-Based Control for Computer Vision","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-245-256","cs.CV cs.AI cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Several key issues arise in implementing computer vision recognition of world
objects in terms of Bayesian networks. Computational efficiency is a driving
force. Perceptual networks are very deep, typically fifteen levels of
structure. Images are wide, e.g., an unspecified-number of edges may appear
anywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamically
instantiate hypotheses of observed objects. The network is not fixed, but is
created incrementally at runtime. Generation of hypotheses of world objects and
indexing of models for recognition are important, but they are not considered
here [4,11]. This work is aimed at near-term implementation with parallel
computation in a radar surveillance system, ADRIES [5, 15], and a system for
industrial part recognition, SUCCESSOR [2]. For many applications, vision must
be faster to be practical and so efficiently controlling the machine vision
process is critical. Perceptual operators may scan megapixels and may require
minutes of computation time. It is necessary to avoid unnecessary sensor
actions and computation. Parallel computation is available at several levels of
processor capability. The potential for parallel, distributed computation for
high-level vision means distributing non-homogeneous computations. This paper
addresses the problem of task control in machine vision systems based on
Bayesian probability models. We separate control and inference to extend the
previous work [3] to maximize utility instead of probability. Maximizing
utility allows adopting perceptual strategies for efficient information
gathering with sensors and analysis of sensor data. Results of controlling
machine vision via utility to recognize military situations are presented in
this paper. Future work extends this to industrial part recognition for
SUCCESSOR.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:16 GMT""}]","2013-04-10"
"1304.2368","Ronald P. Loui","Ronald P. Loui","Evidential Reasoning in a Network Usage Prediction Testbed","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-257-265","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper reports on empirical work aimed at comparing evidential reasoning
techniques. While there is prima facie evidence for some conclusions, this i6
work in progress; the present focus is methodology, with the goal that
subsequent results be meaningful. The domain is a network of UNIX* cycle
servers, and the task is to predict properties of the state of the network from
partial descriptions of the state. Actual data from the network are taken and
used for blindfold testing in a betting game that allows abstention. The focal
technique has been Kyburg's method for reasoning with data of varying relevance
to a particular query, though the aim is to be able eventually to compare
various uncertainty calculi. The conclusions are not novel, but are
instructive. 1. All of the calculi performed better than human subjects, so
unbiased access to sample experience is apparently of value. 2. Performance
depends on metric: (a) when trials are repeated, net = gains - losses favors
methods that place many bets, if the probability of placing a correct bet is
sufficiently high; that is, it favors point-valued formalisms; (b) yield =
gains/(gains + lossee) favors methods that bet only when sure to bet correctly;
that is, it favors interval-valued formalisms. 3. Among the calculi, there were
no clear winners or losers. Methods are identified for eliminating the bias of
the net as a performance criterion and for separating the calculi effectively:
in both cases by posting odds for the betting game in the appropriate way.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:22 GMT""}]","2013-04-10"
"1304.2369","Richard E. Neapolitan","Richard E. Neapolitan, James Kenevan","Justifying the Principle of Interval Constraints","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-266-274","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  When knowledge is obtained from a database, it is only possible to deduce
confidence intervals for probability values. With confidence intervals
replacing point values, the results in the set covering model include interval
constraints for the probabilities of mutually exclusive and exhaustive
explanations. The Principle of Interval Constraints ranks these explanations by
determining the expected values of the probabilities based on distributions
determined from the interval, constraints. This principle was developed using
the Classical Approach to probability. This paper justifies the Principle of
Interval Constraints with a more rigorous statement of the Classical Approach
and by defending the concept of probabilities of probabilities.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:27 GMT""}]","2013-04-10"
"1304.2370","Eric Neufeld","Eric Neufeld, David L Poole","Probabilistic Semantics and Defaults","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-275-282","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There is much interest in providing probabilistic semantics for defaults but
most approaches seem to suffer from one of two problems: either they require
numbers, a problem defaults were intended to avoid, or they generate peculiar
side effects. Rather than provide semantics for defaults, we address the
problem defaults were intended to solve: that of reasoning under uncertainty
where numeric probability distributions are not available. We describe a
non-numeric formalism called an inference graph based on standard probability
theory, conditional independence and sentences of favouring where a favours b -
favours(a, b) - p(a|b) > p(a). The formalism seems to handle the examples from
the nonmonotonic literature. Most importantly, the sentences of our system can
be verified by performing an appropriate experiment in the semantic domain.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:33 GMT""}]","2013-04-10"
"1304.2371","Michael Pittarelli","Michael Pittarelli","Decision Making with Linear Constraints on Probabilities","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-283-290","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Techniques for decision making with knowledge of linear constraints on
condition probabilities are examined. These constraints arise naturally in many
situations: upper and lower condition probabilities are known; an ordering
among the probabilities is determined; marginal probabilities or bounds on such
probabilities are known, e.g., data are available in the form of a
probabilistic database (Cavallo and Pittarelli, 1987a); etc. Standard
situations of decision making under risk and uncertainty may also be
characterized by linear constraints. Each of these types of information may be
represented by a convex polyhedron of numerically determinate condition
probabilities. A uniform approach to decision making under risk, uncertainty,
and partial uncertainty based on a generalized version of a criterion of
Hurwicz is proposed, Methods for processing marginal probabilities to improve
decision making using any of the criteria discussed are presented.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:38 GMT""}]","2013-04-10"
"1304.2372","Thomas F. Reid","Thomas F. Reid, Gregory S. Parnell","Maintenance in Probabilistic Knowledge-Based Systems","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-291-298","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Recent developments using directed acyclical graphs (i.e., influence diagrams
and Bayesian networks) for knowledge representation have lessened the problems
of using probability in knowledge-based systems (KBS). Most current research
involves the efficient propagation of new evidence, but little has been done
concerning the maintenance of domain-specific knowledge, which includes the
probabilistic information about the problem domain. By making use of
conditional independencies represented in she graphs, however, probability
assessments are required only for certain variables when the knowledge base is
updated. The purpose of this study was to investigate, for those variables
which require probability assessments, ways to reduce the amount of new
knowledge required from the expert when updating probabilistic information in a
probabilistic knowledge-based system. Three special cases (ignored outcome,
split outcome, and assumed constraint outcome) were identified under which many
of the original probabilities (those already in the knowledge-base) do not need
to be reassessed when maintenance is required.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:46 GMT""}]","2013-04-10"
"1304.2373","Ross D. Shachter","Ross D. Shachter","A Linear Approximation Method for Probabilistic Inference","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-299-306","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An approximation method is presented for probabilistic inference with
continuous random variables. These problems can arise in many practical
problems, in particular where there are ""second order"" probabilities. The
approximation, based on the Gaussian influence diagram, iterates over linear
approximations to the inference problem.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:52 GMT""}]","2013-04-10"
"1304.2374","Prakash P. Shenoy","Prakash P. Shenoy, Glenn Shafer","An Axiomatic Framework for Bayesian and Belief-function Propagation","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-307-314","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we describe an abstract framework and axioms under which exact
local computation of marginals is possible. The primitive objects of the
framework are variables and valuations. The primitive operators of the
framework are combination and marginalization. These operate on valuations. We
state three axioms for these operators and we derive the possibility of local
computation from the axioms. Next, we describe a propagation scheme for
computing marginals of a valuation when we have a factorization of the
valuation on a hypertree. Finally we show how the problem of computing
marginals of joint probability distributions and joint belief functions fits
the general framework.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:44:57 GMT""}]","2013-04-10"
"1304.2375","Wolfgang Spohn","Wolfgang Spohn","A General Non-Probabilistic Theory of Inductive Reasoning","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-315-322","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Probability theory, epistemically interpreted, provides an excellent, if not
the best available account of inductive reasoning. This is so because there are
general and definite rules for the change of subjective probabilities through
information or experience; induction and belief change are one and same topic,
after all. The most basic of these rules is simply to conditionalize with
respect to the information received; and there are similar and more general
rules. 1 Hence, a fundamental reason for the epistemological success of
probability theory is that there at all exists a well-behaved concept of
conditional probability. Still, people have, and have reasons for, various
concerns over probability theory. One of these is my starting point:
Intuitively, we have the notion of plain belief; we believe propositions2 to be
true (or to be false or neither). Probability theory, however, offers no formal
counterpart to this notion. Believing A is not the same as having probability 1
for A, because probability 1 is incorrigible3; but plain belief is clearly
corrigible. And believing A is not the same as giving A a probability larger
than some 1 - c, because believing A and believing B is usually taken to be
equivalent to believing A & B.4 Thus, it seems that the formal representation
of plain belief has to take a non-probabilistic route. Indeed, representing
plain belief seems easy enough: simply represent an epistemic state by the set
of all propositions believed true in it or, since I make the common assumption
that plain belief is deductively closed, by the conjunction of all propositions
believed true in it. But this does not yet provide a theory of induction, i.e.
an answer to the question how epistemic states so represented are changed
tbrough information or experience. There is a convincing partial answer: if the
new information is compatible with the old epistemic state, then the new
epistemic state is simply represented by the conjunction of the new information
and the old beliefs. This answer is partial because it does not cover the quite
common case where the new information is incompatible with the old beliefs. It
is, however, important to complete the answer and to cover this case, too;
otherwise, we would not represent plain belief as conigible. The crucial
problem is that there is no good completion. When epistemic states are
represented simply by the conjunction of all propositions believed true in it,
the answer cannot be completed; and though there is a lot of fruitful work, no
other representation of epistemic states has been proposed, as far as I know,
which provides a complete solution to this problem. In this paper, I want to
suggest such a solution. In [4], I have more fully argued that this is the only
solution, if certain plausible desiderata are to be satisfied. Here, in section
2, I will be content with formally defining and intuitively explaining my
proposal. I will compare my proposal with probability theory in section 3. It
will turn out that the theory I am proposing is structurally homomorphic to
probability theory in important respects and that it is thus equally easily
implementable, but moreover computationally simpler. Section 4 contains a very
brief comparison with various kinds of logics, in particular conditional logic,
with Shackle's functions of potential surprise and related theories, and with
the Dempster - Shafer theory of belief functions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:03 GMT""}]","2013-04-10"
"1304.2376","Spencer Star","Spencer Star","Generating Decision Structures and Causal Explanations for Decision
  Making","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-323-334","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines two related problems that are central to developing an
autonomous decision-making agent, such as a robot. Both problems require
generating structured representafions from a database of unstructured
declarative knowledge that includes many facts and rules that are irrelevant in
the problem context. The first problem is how to generate a well structured
decision problem from such a database. The second problem is how to generate,
from the same database, a well-structured explanation of why some possible
world occurred. In this paper it is shown that the problem of generating the
appropriate decision structure or explanation is intractable without
introducing further constraints on the knowledge in the database. The paper
proposes that the problem search space can be constrained by adding knowledge
to the database about causal relafions between events. In order to determine
the causal knowledge that would be most useful, causal theories for
deterministic and indeterministic universes are proposed. A program that uses
some of these causal constraints has been used to generate explanations about
faulty plans. The program shows the expected increase in efficiency as the
causal constraints are introduced.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:09 GMT""}]","2013-04-10"
"1304.2377","Jaap Suermondt","Jaap Suermondt, Gregory F. Cooper","Updating Probabilities in Multiply-Connected Belief Networks","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-335-343","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper focuses on probability updates in multiply-connected belief
networks. Pearl has designed the method of conditioning, which enables us to
apply his algorithm for belief updates in singly-connected networks to
multiply-connected belief networks by selecting a loop-cutset for the network
and instantiating these loop-cutset nodes. We discuss conditions that need to
be satisfied by the selected nodes. We present a heuristic algorithm for
finding a loop-cutset that satisfies these conditions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:15 GMT""}]","2013-04-10"
"1304.2378","Bjornar Tessem","Bjornar Tessem, Lars Johan Ersland","Handling uncertainty in a system for text-symbol context analysis","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-344-351","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In pattern analysis, information regarding an object can often be drawn from
its surroundings. This paper presents a method for handling uncertainty when
using context of symbols and texts for analyzing technical drawings. The method
is based on Dempster-Shafer theory and possibility theory.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:21 GMT""}]","2013-04-10"
"1304.2379","Tom S. Verma","Tom S. Verma, Judea Pearl","Causal Networks: Semantics and Expressiveness","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-352-359","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dependency knowledge of the form ""x is independent of y once z is known""
invariably obeys the four graphoid axioms, examples include probabilistic and
database dependencies. Often, such knowledge can be represented efficiently
with graphical structures such as undirected graphs and directed acyclic graphs
(DAGs). In this paper we show that the graphical criterion called d-separation
is a sound rule for reading independencies from any DAG based on a causal input
list drawn from a graphoid. The rule may be extended to cover DAGs that
represent functional dependencies as well as conditional dependencies.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:27 GMT""}]","2013-04-10"
"1304.2380","Wilson X. Wen","Wilson X. Wen","MCE Reasoning in Recursive Causal Networks","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-360-367","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A probabilistic method of reasoning under uncertainty is proposed based on
the principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal
Model (RCM). The dependency and correlations among the variables are described
in a special language BNDL (Belief Networks Description Language). Beliefs are
propagated among the clauses of the BNDL programs representing the underlying
probabilistic distributions. BNDL interpreters in both Prolog and C has been
developed and the performance of the method is compared with those of the
others.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:33 GMT""}]","2013-04-10"
"1304.2381","Ronald R. Yager","Ronald R. Yager","Nonmonotonic Reasoning via Possibility Theory","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-368-373","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We introduce the operation of possibility qualification and show how. this
modal-like operator can be used to represent ""typical"" or default knowledge in
a theory of nonmonotonic reasoning. We investigate the representational power
of this approach by looking at a number of prototypical problems from the
nonmonotonic reasoning literature. In particular we look at the so called Yale
shooting problem and its relation to priority in default reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:38 GMT""}]","2013-04-10"
"1304.2382","Alexander Yeh","Alexander Yeh","Predicting the Likely Behaviors of Continuous Nonlinear Systems in
  Equilibrium","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-374-381","cs.SY cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper introduces a method for predicting the likely behaviors of
continuous nonlinear systems in equilibrium in which the input values can vary.
The method uses a parameterized equation model and a lower bound on the input
joint density to bound the likelihood that some behavior will occur, such as a
state variable being inside a given numeric range. Using a bound on the density
instead of the density itself is desirable because often the input density's
parameters and shape are not exactly known. The new method is called SAB after
its basic operations: split the input value space into smaller regions, and
then bound those regions' possible behaviors and the probability of being in
them. SAB finds rough bounds at first, and then refines them as more time is
given. In contrast to other researchers' methods, SAB can (1) find all the
possible system behaviors, and indicate how likely they are, (2) does not
approximate the distribution of possible outcomes without some measure of the
error magnitude, (3) does not use discretized variable values, which limit the
events one can find probability bounds for, (4) can handle density bounds, and
(5) can handle such criteria as two state variables both being inside a numeric
range.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:44 GMT""}]","2013-04-10"
"1304.2383","John Yen","John Yen","Generalizing the Dempster-Shafer Theory to Fuzzy Sets","Appears in Proceedings of the Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI1988)",,,"UAI-P-1988-PG-382-391","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  With the desire to apply the Dempster-Shafer theory to complex real world
problems where the evidential strength is often imprecise and vague, several
attempts have been made to generalize the theory. However, the important
concept in the D-S theory that the belief and plausibility functions are lower
and upper probabilities is no longer preserved in these generalizations. In
this paper, we describe a generalized theory of evidence where the degree of
belief in a fuzzy set is obtained by minimizing the probability of the fuzzy
set under the constraints imposed by a basic probability assignment. To
formulate the probabilistic constraint of a fuzzy focal element, we decompose
it into a set of consonant non-fuzzy focal elements. By generalizing the
compatibility relation to a possibility theory, we are able to justify our
generalization to Dempster's rule based on possibility distribution. Our
generalization not only extends the application of the D-S theory but also
illustrates a way that probability theory and fuzzy set theory can be combined
to deal with different kinds of uncertain information in AI systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:45:50 GMT""}]","2013-04-10"
"1304.2711","Paul K. Black","Paul K. Black","Is Shafer General Bayes?","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-2-9","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines the relationship between Shafer's belief functions and
convex sets of probability distributions. Kyburg's (1986) result showed that
belief function models form a subset of the class of closed convex probability
distributions. This paper emphasizes the importance of Kyburg's result by
looking at simple examples involving Bernoulli trials. Furthermore, it is shown
that many convex sets of probability distributions generate the same belief
function in the sense that they support the same lower and upper values. This
has implications for a decision theoretic extension. Dempster's rule of
combination is also compared with Bayes' rule of conditioning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:18 GMT""}]","2013-04-11"
"1304.2712","Paul Cohen","Paul Cohen, Glenn Shafer, Prakash P. Shenoy","Modifiable Combining Functions","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-10-21","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Modifiable combining functions are a synthesis of two common approaches to
combining evidence. They offer many of the advantages of these approaches and
avoid some disadvantages. Because they facilitate the acquisition,
representation, explanation, and modification of knowledge about combinations
of evidence, they are proposed as a tool for knowledge engineers who build
systems that reason under uncertainty, not as a normative theory of evidence.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:23 GMT""}]","2013-04-11"
"1304.2713","Daniel Hunter","Daniel Hunter","Dempster-Shafer vs. Probabilistic Logic","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-22-29","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The combination of evidence in Dempster-Shafer theory is compared with the
combination of evidence in probabilistic logic. Sufficient conditions are
stated for these two methods to agree. It is then shown that these conditions
are minimal in the sense that disagreement can occur when any one of them is
removed. An example is given in which the traditional assumption of conditional
independence of evidence on hypotheses holds and a uniform prior is assumed,
but probabilistic logic and Dempster's rule give radically different results
for the combination of two evidence events.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:27 GMT""}]","2013-04-11"
"1304.2714","Henry E. Kyburg Jr.","Henry E. Kyburg Jr","Higher Order Probabilities","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-30-38","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A number of writers have supposed that for the full specification of belief,
higher order probabilities are required. Some have even supposed that there may
be an unending sequence of higher order probabilities of probabilities of
probabilities.... In the present paper we show that higher order probabilities
can always be replaced by the marginal distributions of joint probability
distributions. We consider both the case in which higher order probabilities
are of the same sort as lower order probabilities and that in which higher
order probabilities are distinct in character, as when lower order
probabilities are construed as frequencies and higher order probabilities are
construed as subjective degrees of belief. In neither case do higher order
probabilities appear to offer any advantages, either conceptually or
computationally.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:32 GMT""}]","2013-04-11"
"1304.2715","Kathryn Blackmond Laskey","Kathryn Blackmond Laskey","Belief in Belief Functions: An Examination of Shafer's Canonical
  Examples","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-39-46","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the canonical examples underlying Shafer-Dempster theory, beliefs over the
hypotheses of interest are derived from a probability model for a set of
auxiliary hypotheses. Beliefs are derived via a compatibility relation
connecting the auxiliary hypotheses to subsets of the primary hypotheses. A
belief function differs from a Bayesian probability model in that one does not
condition on those parts of the evidence for which no probabilities are
specified. The significance of this difference in conditioning assumptions is
illustrated with two examples giving rise to identical belief functions but
different Bayesian probability distributions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:37 GMT""}]","2013-04-11"
"1304.2716","Judea Pearl","Judea Pearl","Do We Need Higher-Order Probabilities and, If So, What Do They Mean?","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-47-60","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The apparent failure of individual probabilistic expressions to distinguish
uncertainty about truths from uncertainty about probabilistic assessments have
prompted researchers to seek formalisms where the two types of uncertainties
are given notational distinction. This paper demonstrates that the desired
distinction is already a built-in feature of classical probabilistic models,
thus, specialized notations are unnecessary.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:42 GMT""}]","2013-04-11"
"1304.2717","Matthew Self","Matthew Self, Peter Cheeseman","Bayesian Prediction for Artificial Intelligence","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-61-69","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper shows that the common method used for making predictions under
uncertainty in A1 and science is in error. This method is to use currently
available data to select the best model from a given class of models-this
process is called abduction-and then to use this model to make predictions
about future data. The correct method requires averaging over all the models to
make a prediction-we call this method transduction. Using transduction, an AI
system will not give misleading results when basing predictions on small
amounts of data, when no model is clearly best. For common classes of models we
show that the optimal solution can be given in closed form.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:47 GMT""}]","2013-04-11"
"1304.2718","John Yen","John Yen","Can Evidence Be Combined in the Dempster-Shafer Theory","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-70-76","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Dempster's rule of combination has been the most controversial part of the
Dempster-Shafer (D-S) theory. In particular, Zadeh has reached a conjecture on
the noncombinability of evidence from a relational model of the D-S theory. In
this paper, we will describe another relational model where D-S masses are
represented as conditional granular distributions. By comparing it with Zadeh's
relational model, we will show how Zadeh's conjecture on combinability does not
affect the applicability of Dempster's rule in our model.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:52 GMT""}]","2013-04-11"
"1304.2719","John B. Bacon","John B. Bacon","An Interesting Uncertainty-Based Combinatoric Problem in Spare Parts
  Forecasting: The FRED System","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-78-85","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The domain of spare parts forecasting is examined, and is found to present
unique uncertainty based problems in the architectural design of a
knowledge-based system. A mixture of different uncertainty paradigms is
required for the solution, with an intriguing combinatoric problem arising from
an uncertain choice of inference engines. Thus, uncertainty in the system is
manifested in two different meta-levels. The different uncertainty paradigms
and meta-levels must be integrated into a functioning whole. FRED is an example
of a difficult real-world domain to which no existing uncertainty approach is
completely appropriate. This paper discusses the architecture of FRED,
highlighting: the points of uncertainty and other interesting features of the
domain, the specific implications of those features on the system design
(including the combinatoric explosions), their current implementation & future
plans,and other problems and issues with the architecture.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:46:57 GMT""}]","2013-04-11"
"1304.2720","Thomas O. Binford","Thomas O. Binford, Tod S. Levitt, Wallace B. Mann","Bayesian Inference in Model-Based Machine Vision","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-86-97","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This is a preliminary version of visual interpretation integrating multiple
sensors in SUCCESSOR, an intelligent, model-based vision system. We pursue a
thorough integration of hierarchical Bayesian inference with comprehensive
physical representation of objects and their relations in a system for
reasoning with geometry, surface materials and sensor models in machine vision.
Bayesian inference provides a framework for accruing_ probabilities to rank
order hypotheses.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:03 GMT""}]","2013-04-11"
"1304.2721","Gautam Biswas","Gautam Biswas, Teywansh S. Anand","Using the Dempster-Shafer Scheme in a Diagnostic Expert System Shell","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-98-105","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper discusses an expert system shell that integrates rule-based
reasoning and the Dempster-Shafer evidence combination scheme. Domain knowledge
is stored as rules with associated belief functions. The reasoning component
uses a combination of forward and backward inferencing mechanisms to allow
interaction with users in a mixed-initiative format.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:07 GMT""}]","2013-04-11"
"1304.2722","Homer L. Chin","Homer L. Chin, Gregory F. Cooper","Stochastic Simulation of Bayesian Belief Networks","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-106-113","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines Bayesian belief network inference using simulation as a
method for computing the posterior probabilities of network variables.
Specifically, it examines the use of a method described by Henrion, called
logic sampling, and a method described by Pearl, called stochastic simulation.
We first review the conditions under which logic sampling is computationally
infeasible. Such cases motivated the development of the Pearl's stochastic
simulation algorithm. We have found that this stochastic simulation algorithm,
when applied to certain networks, leads to much slower than expected
convergence to the true posterior probabilities. This behavior is a result of
the tendency for local areas in the network to become fixed through many
simulation cycles. The time required to obtain significant convergence can be
made arbitrarily long by strengthening the probabilistic dependency between
nodes. We propose the use of several forms of graph modification, such as graph
pruning, arc reversal, and node reduction, in order to convert some networks
into formats that are computationally more efficient for simulation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:13 GMT""}]","2013-04-11"
"1304.2723","Steve Hanks","Steve Hanks","Temporal Reasoning About Uncertain Worlds","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-114-122","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a program that manages a database of temporally scoped beliefs.
The basic functionality of the system includes maintaining a network of
constraints among time points, supporting a variety of fetches, mediating the
application of causal rules, monitoring intervals of time for the addition of
new facts, and managing data dependencies that keep the database consistent. At
this level the system operates independent of any measure of belief or belief
calculus. We provide an example of how an application program mi9ght use this
functionality to implement a belief calculus.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:17 GMT""}]","2013-04-11"
"1304.2724","David Heckerman","David Heckerman, Holly B. Jimison","A Perspective on Confidence and Its Use in Focusing Attention During
  Knowledge Acquisition","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-123-131","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a representation of partial confidence in belief and preference
that is consistent with the tenets of decision-theory. The fundamental insight
underlying the representation is that if a person is not completely confident
in a probability or utility assessment, additional modeling of the assessment
may improve decisions to which it is relevant. We show how a traditional
decision-analytic approach can be used to balance the benefits of additional
modeling with associated costs. The approach can be used during knowledge
acquisition to focus the attention of a knowledge engineer or expert on parts
of a decision model that deserve additional refinement.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:22 GMT""}]","2013-04-11"
"1304.2725","Max Henrion","Max Henrion","Practical Issues in Constructing a Bayes' Belief Network","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-132-139","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Bayes belief networks and influence diagrams are tools for constructing
coherent probabilistic representations of uncertain knowledge. The process of
constructing such a network to represent an expert's knowledge is used to
illustrate a variety of techniques which can facilitate the process of
structuring and quantifying uncertain relationships. These include some
generalizations of the ""noisy OR gate"" concept. Sensitivity analysis of generic
elements of Bayes' networks provides insight into when rough probability
assessments are sufficient and when greater precision may be important.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:27 GMT""}]","2013-04-11"
"1304.2726","Michael C. Higgins","Michael C. Higgins","NAIVE: A Method for Representing Uncertainty and Temporal Relationships
  in an Automated Reasoner","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-140-147","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes NAIVE, a low-level knowledge representation language and
inferencing process. NAIVE has been designed for reasoning about
nondeterministic dynamic systems like those found in medicine. Knowledge is
represented in a graph structure consisting of nodes, which correspond to the
variables describing the system of interest, and arcs, which correspond to the
procedures used to infer the value of a variable from the values of other
variables. The value of a variable can be determined at an instant in time,
over a time interval or for a series of times. Information about the value of a
variable is expressed as a probability density function which quantifies the
likelihood of each possible value. The inferencing process uses these
probability density functions to propagate uncertainty. NAIVE has been used to
develop medical knowledge bases including over 100 variables.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:31 GMT""}]","2013-04-11"
"1304.2727","Henry E. Kyburg Jr.","Henry E. Kyburg Jr","Objective Probability","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-148-155","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A distinction is sometimes made between ""statistical"" and ""subjective""
probabilities. This is based on a distinction between ""unique"" events and
""repeatable"" events. We argue that this distinction is untenable, since all
events are ""unique"" and all events belong to ""kinds"", and offer a conception of
probability for A1 in which (1) all probabilities are based on -- possibly
vague -- statistical knowledge, and (2) every statement in the language has a
probability. This conception of probability can be applied to very rich
languages.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:36 GMT""}]","2013-04-11"
"1304.2728","Silvio Ursic","Silvio Ursic","Coefficients of Relations for Probabilistic Reasoning","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-156-162","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Definitions and notations with historical references are given for some
numerical coefficients commonly used to quantify relations among collections of
objects for the purpose of expressing approximate knowledge and probabilistic
reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:41 GMT""}]","2013-04-11"
"1304.2729","Ben P. Wise","Ben P. Wise","Satisfaction of Assumptions is a Weak Predictor of Performance","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-163-169","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper demonstrates a methodology for examining the accuracy of uncertain
inference systems (UIS), after their parameters have been optimized, and does
so for several common UIS's. This methodology may be used to test the accuracy
when either the prior assumptions or updating formulae are not exactly
satisfied. Surprisingly, these UIS's were revealed to be no more accurate on
the average than a simple linear regression. Moreover, even on prior
distributions which were deliberately biased so as give very good accuracy,
they were less accurate than the simple probabilistic model which assumes
marginal independence between inputs. This demonstrates that the importance of
updating formulae can outweigh that of prior assumptions. Thus, when UIS's are
judged by their final accuracy after optimization, we get completely different
results than when they are judged by whether or not their prior assumptions are
perfectly satisfied.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:45 GMT""}]","2013-04-11"
"1304.2730","Lei Xu","Lei Xu, Judea Pearl","Structuring Causal Tree Models with Continuous Variables","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-170-179","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper considers the problem of invoking auxiliary, unobservable
variables to facilitate the structuring of causal tree models for a given set
of continuous variables. Paralleling the treatment of bi-valued variables in
[Pearl 1986], we show that if a collection of coupled variables are governed by
a joint normal distribution and a tree-structured representation exists, then
both the topology and all internal relationships of the tree can be uncovered
by observing pairwise dependencies among the observed variables (i.e., the
leaves of the tree). Furthermore, the conditions for normally distributed
variables are less restrictive than those governing bi-valued variables. The
result extends the applications of causal tree models which were found useful
in evidential reasoning tasks.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:50 GMT""}]","2013-04-11"
"1304.2731","John Yen","John Yen","Implementing Evidential Reasoning in Expert Systems","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-180-188","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The Dempster-Shafer theory has been extended recently for its application to
expert systems. However, implementing the extended D-S reasoning model in
rule-based systems greatly complicates the task of generating informative
explanations. By implementing GERTIS, a prototype system for diagnosing
rheumatoid arthritis, we show that two kinds of knowledge are essential for
explanation generation: (l) taxonomic class relationships between hypotheses
and (2) pointers to the rules that significantly contribute to belief in the
hypothesis. As a result, the knowledge represented in GERTIS is richer and more
complex than that of conventional rule-based systems. GERTIS not only
demonstrates the feasibility of rule-based evidential-reasoning systems, but
also suggests ways to generate better explanations, and to explicitly represent
various useful relationships among hypotheses and rules.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:47:55 GMT""}]","2013-04-11"
"1304.2732","Wray L. Buntine","Wray L. Buntine","Decision Tree Induction Systems: A Bayesian Analysis","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-190-197","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Decision tree induction systems are being used for knowledge acquisition in
noisy domains. This paper develops a subjective Bayesian interpretation of the
task tackled by these systems and the heuristic methods they use. It is argued
that decision tree systems implicitly incorporate a prior belief that the
simpler (in terms of decision tree complexity) of two hypotheses be preferred,
all else being equal, and that they perform a greedy search of the space of
decision rules to find one in which there is strong posterior belief. A number
of improvements to these systems are then suggested.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:00 GMT""}]","2013-04-11"
"1304.2733","Richard A. Caruana","Richard A. Caruana","The Automatic Training of Rule Bases that Use Numerical Uncertainty
  Representations","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-198-204","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The use of numerical uncertainty representations allows better modeling of
some aspects of human evidential reasoning. It also makes knowledge acquisition
and system development, test, and modification more difficult. We propose that
where possible, the assignment and/or refinement of rule weights should be
performed automatically. We present one approach to performing this training -
numerical optimization - and report on the results of some preliminary tests in
training rule bases. We also show that truth maintenance can be used to make
training more efficient and ask some epistemological questions raised by
training rule weights.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:04 GMT""}]","2013-04-11"
"1304.2734","Norman C. Dalkey","Norman C. Dalkey","The Inductive Logic of Information Systems","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-205-211","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An inductive logic can be formulated in which the elements are not
propositions or probability distributions, but information systems. The logic
is complete for information systems with binary hypotheses, i.e., it applies to
all such systems. It is not complete for information systems with more than two
hypotheses, but applies to a subset of such systems. The logic is inductive in
that conclusions are more informative than premises. Inferences using the
formalism have a strong justification in terms of the expected value of the
derived information system.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:09 GMT""}]","2013-04-11"
"1304.2735","Stephen I. Gallant","Stephen I. Gallant","Automated Generation of Connectionist Expert Systems for Problems
  Involving Noise and Redundancy","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-212-221","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  When creating an expert system, the most difficult and expensive task is
constructing a knowledge base. This is particularly true if the problem
involves noisy data and redundant measurements. This paper shows how to modify
the MACIE process for generating connectionist expert systems from training
examples so that it can accommodate noisy and redundant data. The basic idea is
to dynamically generate appropriate training examples by constructing both a
'deep' model and a noise model for the underlying problem. The use of
winner-take-all groups of variables is also discussed. These techniques are
illustrated with a small example that would be very difficult for standard
expert system approaches.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:14 GMT""}]","2013-04-11"
"1304.2736","George Rebane","George Rebane, Judea Pearl","The Recovery of Causal Poly-Trees from Statistical Data","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-222-228","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Poly-trees are singly connected causal networks in which variables may arise
from multiple causes. This paper develops a method of recovering ply-trees from
empirically measured probability distributions of pairs of variables. The
method guarantees that, if the measured distributions are generated by a causal
process structured as a ply-tree then the topological structure of such tree
can be recovered precisely and, in addition, the causal directionality of the
branches can be determined up to the maximum extent possible. The method also
pinpoints the minimum (if any) external semantics required to determine the
causal relationships among the variables considered.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:18 GMT""}]","2013-04-11"
"1304.2737","Ross D. Shachter","Ross D. Shachter, David M. Eddy, Vic Hasselblad, Robert Wolpert","A Heuristic Bayesian Approach to Knowledge Acquisition: Application to
  Analysis of Tissue-Type Plasminogen Activator","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-229-236","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes a heuristic Bayesian method for computing probability
distributions from experimental data, based upon the multivariate normal form
of the influence diagram. An example illustrates its use in medical technology
assessment. This approach facilitates the integration of results from different
studies, and permits a medical expert to make proper assessments without
considerable statistical training.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:23 GMT""}]","2013-04-11"
"1304.2738","Spencer Star","Spencer Star","Theory-Based Inductive Learning: An Integration of Symbolic and
  Quantitative Methods","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-237-248","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The objective of this paper is to propose a method that will generate a
causal explanation of observed events in an uncertain world and then make
decisions based on that explanation. Feedback can cause the explanation and
decisions to be modified. I call the method Theory-Based Inductive Learning
(T-BIL). T-BIL integrates deductive learning, based on a technique called
Explanation-Based Generalization (EBG) from the field of machine learning, with
inductive learning methods from Bayesian decision theory. T-BIL takes as inputs
(1) a decision problem involving a sequence of related decisions over time, (2)
a training example of a solution to the decision problem in one period, and (3)
the domain theory relevant to the decision problem. T-BIL uses these inputs to
construct a probabilistic explanation of why the training example is an
instance of a solution to one stage of the sequential decision problem. This
explanation is then generalized to cover a more general class of instances and
is used as the basis for making the next-stage decisions. As the outcomes of
each decision are observed, the explanation is revised, which in turn affects
the subsequent decisions. A detailed example is presented that uses T-BIL to
solve a very general stochastic adaptive control problem for an autonomous
mobile robot.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:28 GMT""}]","2013-04-11"
"1304.2739","Piero P. Bonissone","Piero P. Bonissone","Using T-Norm Based Uncertainty Calculi in a Naval Situation Assessment
  Application","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-250-261","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  RUM (Reasoning with Uncertainty Module), is an integrated software tool based
on a KEE, a frame system implemented in an object oriented language. RUM's
architecture is composed of three layers: representation, inference, and
control. The representation layer is based on frame-like data structures that
capture the uncertainty information used in the inference layer and the
uncertainty meta-information used in the control layer. The inference layer
provides a selection of five T-norm based uncertainty calculi with which to
perform the intersection, detachment, union, and pooling of information. The
control layer uses the meta-information to select the appropriate calculus for
each context and to resolve eventual ignorance or conflict in the information.
This layer also provides a context mechanism that allows the system to focus on
the relevant portion of the knowledge base, and an uncertain-belief revision
system that incrementally updates the certainty values of well-formed formulae
(wffs) in an acyclic directed deduction graph. RUM has been tested and
validated in a sequence of experiments in both naval and aerial situation
assessment (SA), consisting of correlating reports and tracks, locating and
classifying platforms, and identifying intents and threats. An example of naval
situation assessment is illustrated. The testbed environment for developing
these experiments has been provided by LOTTA, a symbolic simulator implemented
in Flavors. This simulator maintains time-varying situations in a multi-player
antagonistic game where players must make decisions in light of uncertain and
incomplete data. RUM has been used to assist one of the LOTTA players to
perform the SA task.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:34 GMT""}]","2013-04-11"
"1304.2740","Yizong Cheng","Yizong Cheng, Rangasami L. Kashyap","A Study of Associative Evidential Reasoning","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-262-269","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Evidential reasoning is cast as the problem of simplifying the
evidence-hypothesis relation and constructing combination formulas that possess
certain testable properties. Important classes of evidence as identifiers,
annihilators, and idempotents and their roles in determining binary operations
on intervals of reals are discussed. The appropriate way of constructing
formulas for combining evidence and their limitations, for instance, in
robustness, are presented.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:39 GMT""}]","2013-04-11"
"1304.2741","I. R. Goodman","I. R. Goodman","A Measure-Free Approach to Conditioning","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-270-277","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In an earlier paper, a new theory of measurefree ""conditional"" objects was
presented. In this paper, emphasis is placed upon the motivation of the theory.
The central part of this motivation is established through an example involving
a knowledge-based system. In order to evaluate combination of evidence for this
system, using observed data, auxiliary at tribute and diagnosis variables, and
inference rules connecting them, one must first choose an appropriate algebraic
logic description pair (ALDP): a formal language or syntax followed by a
compatible logic or semantic evaluation (or model). Three common choices- for
this highly non-unique choice - are briefly discussed, the logics being
Classical Logic, Fuzzy Logic, and Probability Logic. In all three,the key
operator representing implication for the inference rules is interpreted as the
often-used disjunction of a negation (b => a) = (b'v a), for any events a,b.
  However, another reasonable interpretation of the implication operator is
through the familiar form of probabilistic conditioning. But, it can be shown -
quite surprisingly - that the ALDP corresponding to Probability Logic cannot be
used as a rigorous basis for this interpretation! To fill this gap, a new ALDP
is constructed consisting of ""conditional objects"", extending ordinary
Probability Logic, and compatible with the desired conditional probability
interpretation of inference rules. It is shown also that this choice of ALDP
leads to feasible computations for the combination of evidence evaluation in
the example. In addition, a number of basic properties of conditional objects
and the resulting Conditional Probability Logic are given, including a
characterization property and a developed calculus of relations.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:45 GMT""}]","2013-04-11"
"1304.2742","Peter Haddawy","Peter Haddawy, Alan M. Frisch","Convergent Deduction for Probabilistic Logic","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-278-286","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper discusses the semantics and proof theory of Nilsson's
probabilistic logic, outlining both the benefits of its well-defined model
theory and the drawbacks of its proof theory. Within Nilsson's semantic
framework, we derive a set of inference rules which are provably sound. The
resulting proof system, in contrast to Nilsson's approach, has the important
feature of convergence - that is, the inference process proceeds by computing
increasingly narrow probability intervals which converge from above and below
on the smallest entailed probability interval. Thus the procedure can be
stopped at any time to yield partial information concerning the smallest
entailed interval.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:49 GMT""}]","2013-04-11"
"1304.2743","Ze-Nian Li","Ze-Nian Li","Comparisons of Reasoning Mechanisms for Computer Vision","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-287-294","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An evidential reasoning mechanism based on the Dempster-Shafer theory of
evidence is introduced. Its performance in real-world image analysis is
compared with other mechanisms based on the Bayesian formalism and a simple
weight combination method.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:54 GMT""}]","2013-04-11"
"1304.2744","Donald H. Mitchell","Donald H. Mitchell, Steven A. Harp, David K. Simkin","A Knowledge Engineer's Comparison of Three Evidence Aggregation Methods","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-297-304","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The comparisons of uncertainty calculi from the last two Uncertainty
Workshops have all used theoretical probabilistic accuracy as the sole metric.
While mathematical correctness is important, there are other factors which
should be considered when developing reasoning systems. These other factors
include, among other things, the error in uncertainty measures obtainable for
the problem and the effect of this error on the performance of the resulting
system.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:48:59 GMT""}]","2013-04-11"
"1304.2745","Eric Neufeld","Eric Neufeld, David L Poole","Towards Solving the Multiple Extension Problem: Combining Defaults and
  Probabilities","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-305-312","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The multiple extension problem arises frequently in diagnostic and default
inference. That is, we can often use any of a number of sets of defaults or
possible hypotheses to explain observations or make Predictions. In default
inference, some extensions seem to be simply wrong and we use qualitative
techniques to weed out the unwanted ones. In the area of diagnosis, however,
the multiple explanations may all seem reasonable, however improbable. Choosing
among them is a matter of quantitative preference. Quantitative preference
works well in diagnosis when knowledge is modelled causally. Here we suggest a
framework that combines probabilities and defaults in a single unified
framework that retains the semantics of diagnosis as construction of
explanations from a fixed set of possible hypotheses. We can then compute
probabilities incrementally as we construct explanations. Here we describe a
branch and bound algorithm that maintains a set of all partial explanations
while exploring a most promising one first. A most probable explanation is
found first if explanations are partially ordered.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:04 GMT""}]","2013-04-11"
"1304.2746","Richard M. Tong","Richard M. Tong, Lee A. Appelbaum","Problem Structure and Evidential Reasoning","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-313-320","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In our previous series of studies to investigate the role of evidential
reasoning in the RUBRIC system for full-text document retrieval (Tong et al.,
1985; Tong and Shapiro, 1985; Tong and Appelbaum, 1987), we identified the
important role that problem structure plays in the overall performance of the
system. In this paper, we focus on these structural elements (which we now call
""semantic structure"") and show how explicit consideration of their properties
reduces what previously were seen as difficult evidential reasoning problems to
more tractable questions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:09 GMT""}]","2013-04-11"
"1304.2747","Michael P. Wellman","Michael P. Wellman, David Heckerman","The Role of Calculi in Uncertain Inference Systems","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-321-331","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Much of the controversy about methods for automated decision making has
focused on specific calculi for combining beliefs or propagating uncertainty.
We broaden the debate by (1) exploring the constellation of secondary tasks
surrounding any primary decision problem, and (2) identifying knowledge
engineering concerns that present additional representational tradeoffs. We
argue on pragmatic grounds that the attempt to support all of these tasks
within a single calculus is misguided. In the process, we note several
uncertain reasoning objectives that conflict with the Bayesian ideal of
complete specification of probabilities and utilities. In response, we advocate
treating the uncertainty calculus as an object language for reasoning
mechanisms that support the secondary tasks. Arguments against Bayesian
decision theory are weakened when the calculus is relegated to this role.
Architectures for uncertainty handling that take statements in the calculus as
objects to be reasoned about offer the prospect of retaining normative status
with respect to decision making while supporting the other tasks in uncertain
reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:15 GMT""}]","2013-04-11"
"1304.2748","Ben P. Wise","Ben P. Wise, Bruce M. Perrin, David S. Vaughan, Robert M. Yadrick","The Role of Tuning Uncertain Inference Systems","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-332-339","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This study examined the effects of ""tuning"" the parameters of the incremental
function of MYCIN, the independent function of PROSPECTOR, a probability model
that assumes independence, and a simple additive linear equation. me parameters
of each of these models were optimized to provide solutions which most nearly
approximated those from a full probability model for a large set of simple
networks. Surprisingly, MYCIN, PROSPECTOR, and the linear equation performed
equivalently; the independence model was clearly more accurate on the networks
studied.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:20 GMT""}]","2013-04-11"
"1304.2749","Minchuan Zhang","Minchuan Zhang, Su-shing Chen","Evidential Reasoning in Image Understanding","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-340-346","cs.CV cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we present some results of evidential reasoning in
understanding multispectral images of remote sensing systems. The
Dempster-Shafer approach of combination of evidences is pursued to yield
contextual classification results, which are compared with previous results of
the Bayesian context free classification, contextual classifications of dynamic
programming and stochastic relaxation approaches.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:25 GMT""}]","2013-04-11"
"1304.2750","Lashon B. Booker","Lashon B. Booker, Naveen Hota, Gavin Hemphill","Implementing a Bayesian Scheme for Revising Belief Commitments","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-348-354","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Our previous work on classifying complex ship images [1,2] has evolved into
an effort to develop software tools for building and solving generic
classification problems. Managing the uncertainty associated with feature data
and other evidence is an important issue in this endeavor. Bayesian techniques
for managing uncertainty [7,12,13] have proven to be useful for managing
several of the belief maintenance requirements of classification problem
solving. One such requirement is the need to give qualitative explanations of
what is believed. Pearl [11] addresses this need by computing what he calls a
belief commitment-the most probable instantiation of all hypothesis variables
given the evidence available. Before belief commitments can be computed, the
straightforward implementation of Pearl's procedure involves finding an
analytical solution to some often difficult optimization problems. We describe
an efficient implementation of this procedure using tensor products that solves
these problems enumeratively and avoids the need for case by case analysis. The
procedure is thereby made more practical to use in the general case.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:30 GMT""}]","2013-04-11"
"1304.2751","John S. Breese","John S. Breese, Edison Tse","Integrating Logical and Probabilistic Reasoning for Decision Making","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-355-362","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We describe a representation and a set of inference methods that combine
logic programming techniques with probabilistic network representations for
uncertainty (influence diagrams). The techniques emphasize the dynamic
construction and solution of probabilistic and decision-theoretic models for
complex and uncertain domains. Given a query, a logical proof is produced if
possible; if not, an influence diagram based on the query and the knowledge of
the decision domain is produced and subsequently solved. A uniform declarative,
first-order, knowledge representation is combined with a set of integrated
inference procedures for logical, probabilistic, and decision-theoretic
reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:35 GMT""}]","2013-04-11"
"1304.2752","Stephen Chiu","Stephen Chiu, Masaki Togai","Compiling Fuzzy Logic Control Rules to Hardware Implementations","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-363-371","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A major aspect of human reasoning involves the use of approximations.
Particularly in situations where the decision-making process is under stringent
time constraints, decisions are based largely on approximate, qualitative
assessments of the situations. Our work is concerned with the application of
approximate reasoning to real-time control. Because of the stringent processing
speed requirements in such applications, hardware implementations of fuzzy
logic inferencing are being pursued. We describe a programming environment for
translating fuzzy control rules into hardware realizations. Two methods of
hardware realizations are possible. The First is based on a special purpose
chip for fuzzy inferencing. The second is based on a simple memory chip. The
ability to directly translate a set of decision rules into hardware
implementations is expected to make fuzzy control an increasingly practical
approach to the control of complex systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:40 GMT""}]","2013-04-11"
"1304.2753","Paul Cohen","Paul Cohen","Steps Towards Programs that Manage Uncertainty","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-372-379","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Reasoning under uncertainty in Al hats come to mean assessing the credibility
of hypotheses inferred from evidence. But techniques for assessing credibility
do not tell a problem solver what to do when it is uncertain. This is the focus
of our current research. We have developed a medical expert system called MUM,
for Managing Uncertainty in Medicine, that plans diagnostic sequences of
questions, tests, and treatments. This paper describes the kinds of problems
that MUM was designed to solve and gives a brief description of its
architecture. More recently, we have built an empty version of MUM called MU,
and used it to reimplement MUM and a small diagnostic system for plant
pathology. The latter part of the paper describes the features of MU that make
it appropriate for building expert systems that manage uncertainty.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:44 GMT""}]","2013-04-11"
"1304.2754","Gregory F. Cooper","Gregory F. Cooper","An Algorithm for Computing Probabilistic Propositions","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-380-385","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A method for computing probabilistic propositions is presented. It assumes
the availability of a single external routine for computing the probability of
one instantiated variable, given a conjunction of other instantiated variables.
In particular, the method allows belief network algorithms to calculate general
probabilistic propositions over nodes in the network. Although in the worst
case the time complexity of the method is exponential in the size of a query,
it is polynomial in the size of a number of common types of queries.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:49 GMT""}]","2013-04-11"
"1304.2755","Bruce D'Ambrosio","Bruce D'Ambrosio","Combining Symbolic and Numeric Approaches to Uncertainty Management","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-386-393","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A complete approach to reasoning under uncertainty requires support for
incremental and interactive formulation and revision of, as well as reasoning
with, models of the problem domain capable of representing our uncertainty. We
present a hybrid reasoning scheme which combines symbolic and numeric methods
for uncertainty management to provide efficient and effective support for each
of these tasks. The hybrid is based on symbolic techniques adapted from
Assumption-based Truth Maintenance systems (ATMS), combined with numeric
methods adapted from the Dempster/Shafer theory of evidence, as extended in
Baldwin's Support Logic Programming system. The hybridization is achieved by
viewing an ATMS as a symbolic algebra system for uncertainty calculations. This
technique has several major advantages over conventional methods for performing
inference with numeric certainty estimates in addition to the ability to
dynamically determine hypothesis spaces, including improved management of
dependent and partially independent evidence, faster run-time evaluation of
propositional certainties, the ability to query the certainty value of a
proposition from multiple perspectives, and the ability to incrementally extend
or revise domain models.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:54 GMT""}]","2013-04-11"
"1304.2756","Christopher Elsaesser","Christopher Elsaesser","Explanation of Probabilistic Inference for Decision Support Systems","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-394-403","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  An automated explanation facility for Bayesian conditioning aimed at
improving user acceptance of probability-based decision support systems has
been developed. The domain-independent facility is based on an information
processing perspective on reasoning about conditional evidence that accounts
both for biased and normative inferences. Experimental results indicate that
the facility is both acceptable to naive users and effective in improving
understanding.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:49:58 GMT""}]","2013-04-11"
"1304.2757","Greg Hager","Greg Hager, Max Mintz","Estimation Procedures for Robust Sensor Control","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-404-411","cs.SY cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Many robotic sensor estimation problems can characterized in terms of
nonlinear measurement systems. These systems are contaminated with noise and
may be underdetermined from a single observation. In order to get reliable
estimation results, the system must choose views which result in an
overdetermined system. This is the sensor control problem. Accurate and
reliable sensor control requires an estimation procedure which yields both
estimates and measures of its own performance. In the case of nonlinear
measurement systems, computationally simple closed-form estimation solutions
may not exist. However, approximation techniques provide viable alternatives.
In this paper, we evaluate three estimation techniques: the extended Kalman
filter, a discrete Bayes approximation, and an iterative Bayes approximation.
We present mathematical results and simulation statistics illustrating
operating conditions where the extended Kalman filter is inappropriate for
sensor control, and discuss issues in the use of the discrete Bayes
approximation.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:50:04 GMT""}]","2013-04-11"
"1304.2758","Ross D. Shachter","Ross D. Shachter, Leonard Bertrand","Efficient Inference on Generalized Fault Diagrams","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-413-420","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The generalized fault diagram, a data structure for failure analysis based on
the influence diagram, is defined. Unlike the fault tree, this structure allows
for dependence among the basic events and replicated logical elements. A
heuristic procedure is developed for efficient processing of these structures.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:50:09 GMT""}]","2013-04-11"
"1304.2759","Eric J. Horvitz","Eric J. Horvitz","Reasoning About Beliefs and Actions Under Computational Resource
  Constraints","Appears in Proceedings of the Third Conference on Uncertainty in
  Artificial Intelligence (UAI1987)",,,"UAI-P-1987-PG-429-447","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Although many investigators affirm a desire to build reasoning systems that
behave consistently with the axiomatic basis defined by probability theory and
utility theory, limited resources for engineering and computation can make a
complete normative analysis impossible. We attempt to move discussion beyond
the debate over the scope of problems that can be handled effectively to cases
where it is clear that there are insufficient computational resources to
perform an analysis deemed as complete. Under these conditions, we stress the
importance of considering the expected costs and benefits of applying
alternative approximation procedures and heuristics for computation and
knowledge acquisition. We discuss how knowledge about the structure of user
utility can be used to control value tradeoffs for tailoring inference to
alternative contexts. We address the notion of real-time rationality, focusing
on the application of knowledge about the expected timewise-refinement
abilities of reasoning strategies to balance the benefits of additional
computation with the costs of acting with a partial result. We discuss the
benefits of applying decision theory to control the solution of difficult
problems given limitations and uncertainty in reasoning resources.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:50:20 GMT""}]","2013-04-11"
"1304.3075","Shoshana Abel","Shoshana Abel","Application of Evidential Reasoning to Helicopter Flight Path Control","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-1-6","cs.AI cs.RO cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents a methodology for research and development of the
inferencing and knowledge representation aspects of an Expert System approach
for performing reasoning under uncertainty in support of a real time vehicle
guidance and navigation system. Such a system could be of major benefit for
non-terrain following low altitude flight systems operating in foreign hostile
environments such as might be experienced by NOE helicopter or similar mission
craft. An innovative extension of the evidential reasoning methodology, termed
the Sum-and-Lattice-Points Method, has been developed. The research and
development effort presented in this paper consists of a formal mathematical
development of the Sum-and-Lattice-Points Method, its formulation and
representation in a parallel environment, prototype software development of the
method within an expert system, and initial testing of the system within the
confines of the vehicle guidance system.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:00 GMT""}]","2013-04-12"
"1304.3076","Stephen W. Barth","Stephen W. Barth, Steven W. Norton","Knowledge Engineering Within A Generalized Bayesian Framework","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-7-16","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  During the ongoing debate over the representation of uncertainty in
Artificial Intelligence, Cheeseman, Lemmer, Pearl, and others have argued that
probability theory, and in particular the Bayesian theory, should be used as
the basis for the inference mechanisms of Expert Systems dealing with
uncertainty. In order to pursue the issue in a practical setting, sophisticated
tools for knowledge engineering are needed that allow flexible and
understandable interaction with the underlying knowledge representation
schemes. This paper describes a Generalized Bayesian framework for building
expert systems which function in uncertain domains, using algorithms proposed
by Lemmer. It is neither rule-based nor frame-based, and requires a new system
of knowledge engineering tools. The framework we describe provides a
knowledge-based system architecture with an inference engine, explanation
capability, and a unique aid for building consistent knowledge bases.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:06 GMT""}]","2013-04-12"
"1304.3077","Moshe Ben-Bassat","Moshe Ben-Bassat","Taxonomy, Structure, and Implementation of Evidential Reasoning","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-17-28","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The fundamental elements of evidential reasoning problems are described,
followed by a discussion of the structure of various types of problems.
Bayesian inference networks and state space formalism are used as the tool for
problem representation.
  A human-oriented decision making cycle for solving evidential reasoning
problems is described and illustrated for a military situation assessment
problem. The implementation of this cycle may serve as the basis for an expert
system shell for evidential reasoning; i.e. a situation assessment processor.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:12 GMT""}]","2013-04-12"
"1304.3078","Lashon B. Booker","Lashon B. Booker, Naveen Hota","Probabilistic Reasoning About Ship Images","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-29-36","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  One of the most important aspects of current expert systems technology is the
ability to make causal inferences about the impact of new evidence. When the
domain knowledge and problem knowledge are uncertain and incomplete Bayesian
reasoning has proven to be an effective way of forming such inferences [3,4,8].
While several reasoning schemes have been developed based on Bayes Rule, there
has been very little work examining the comparative effectiveness of these
schemes in a real application. This paper describes a knowledge based system
for ship classification [1], originally developed using the PROSPECTOR updating
method [2], that has been reimplemented to use the inference procedure
developed by Pearl and Kim [4,5]. We discuss our reasons for making this
change, the implementation of the new inference engine, and the comparative
performance of the two versions of the system.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:17 GMT""}]","2013-04-12"
"1304.3079","Kaihu Chen","Kaihu Chen","Towards The Inductive Acquisition of Temporal Knowledge","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-37-42","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The ability to predict the future in a given domain can be acquired by
discovering empirically from experience certain temporal patterns that tend to
repeat unerringly. Previous works in time series analysis allow one to make
quantitative predictions on the likely values of certain linear variables.
Since certain types of knowledge are better expressed in symbolic forms, making
qualitative predictions based on symbolic representations require a different
approach. A domain independent methodology called TIM (Time based Inductive
Machine) for discovering potentially uncertain temporal patterns from real time
observations using the technique of inductive inference is described here.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:22 GMT""}]","2013-04-12"
"1304.3080","Su-shing Chen","Su-shing Chen","Some Extensions of Probabilistic Logic","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-43-48","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In [12], Nilsson proposed the probabilistic logic in which the truth values
of logical propositions are probability values between 0 and 1. It is
applicable to any logical system for which the consistency of a finite set of
propositions can be established. The probabilistic inference scheme reduces to
the ordinary logical inference when the probabilities of all propositions are
either 0 or 1. This logic has the same limitations of other probabilistic
reasoning systems of the Bayesian approach. For common sense reasoning,
consistency is not a very natural assumption. We have some well known examples:
{Dick is a Quaker, Quakers are pacifists, Republicans are not pacifists, Dick
is a Republican}and {Tweety is a bird, birds can fly, Tweety is a penguin}. In
this paper, we shall propose some extensions of the probabilistic logic. In the
second section, we shall consider the space of all interpretations, consistent
or not. In terms of frames of discernment, the basic probability assignment
(bpa) and belief function can be defined. Dempster's combination rule is
applicable. This extension of probabilistic logic is called the evidential
logic in [ 1]. For each proposition s, its belief function is represented by an
interval [Spt(s), Pls(s)]. When all such intervals collapse to single points,
the evidential logic reduces to probabilistic logic (in the generalized version
of not necessarily consistent interpretations). Certainly, we get Nilsson's
probabilistic logic by further restricting to consistent interpretations. In
the third section, we shall give a probabilistic interpretation of
probabilistic logic in terms of multi-dimensional random variables. This
interpretation brings the probabilistic logic into the framework of probability
theory. Let us consider a finite set S = {sl, s2, ..., Sn) of logical
propositions. Each proposition may have true or false values; and may be
considered as a random variable. We have a probability distribution for each
proposition. The e-dimensional random variable (sl,..., Sn) may take values in
the space of all interpretations of 2n binary vectors. We may compute absolute
(marginal), conditional and joint probability distributions. It turns out that
the permissible probabilistic interpretation vector of Nilsson [12] consists of
the joint probabilities of S. Inconsistent interpretations will not appear, by
setting their joint probabilities to be zeros. By summing appropriate joint
probabilities, we get probabilities of individual propositions or subsets of
propositions. Since the Bayes formula and other techniques are valid for
e-dimensional random variables, the probabilistic logic is actually very close
to the Bayesian inference schemes. In the last section, we shall consider a
relaxation scheme for probabilistic logic. In this system, not only new
evidences will update the belief measures of a collection of propositions, but
also constraint satisfaction among these propositions in the relational network
will revise these measures. This mechanism is similar to human reasoning which
is an evaluative process converging to the most satisfactory result. The main
idea arises from the consistent labeling problem in computer vision. This
method is originally applied to scene analysis of line drawings. Later, it is
applied to matching, constraint satisfaction and multi sensor fusion by several
authors [8], [16] (and see references cited there). Recently, this method is
used in knowledge aggregation by Landy and Hummel [9].
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:27 GMT""}]","2013-04-12"
"1304.3081","Ping-Chung Chi","Ping-Chung Chi, Dana Nau","Predicting The Performance of Minimax and Product in Game-Tree","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-49-56","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The discovery that the minimax decision rule performs poorly in some games
has sparked interest in possible alternatives to minimax. Until recently, the
only games in which minimax was known to perform poorly were games which were
mainly of theoretical interest. However, this paper reports results showing
poor performance of minimax in a more common game called kalah. For the kalah
games tested, a non-minimax decision rule called the product rule performs
significantly better than minimax.
  This paper also discusses a possible way to predict whether or not minimax
will perform well in a game when compared to product. A parameter called the
rate of heuristic flaw (rhf) has been found to correlate positively with the.
performance of product against minimax. Both analytical and experimental
results are given that appear to support the predictive power of rhf.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:32 GMT""}]","2013-04-12"
"1304.3082","A. Julian Craddock","A. Julian Craddock, Roger A. Browse","Reasoning With Uncertain Knowledge","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-57-62","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A model of knowledge representation is described in which propositional facts
and the relationships among them can be supported by other facts. The set of
knowledge which can be supported is called the set of cognitive units, each
having associated descriptions of their explicit and implicit support
structures, summarizing belief and reliability of belief. This summary is
precise enough to be useful in a computational model while remaining
descriptive of the underlying symbolic support structure. When a fact supports
another supportive relationship between facts we call this meta-support. This
facilitates reasoning about both the propositional knowledge. and the support
structures underlying it.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:38 GMT""}]","2013-04-12"
"1304.3083","Norman C. Dalkey","Norman C. Dalkey","Models vs. Inductive Inference for Dealing With Probabilistic Knowledge","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-63-70","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Two different approaches to dealing with probabilistic knowledge are examined
-models and inductive inference. Examples of the first are: influence diagrams
[1], Bayesian networks [2], log-linear models [3, 4]. Examples of the second
are: games-against nature [5, 6] varieties of maximum-entropy methods [7, 8,
9], and the author's min-score induction [10]. In the modeling approach, the
basic issue is manageability, with respect to data elicitation and computation.
Thus, it is assumed that the pertinent set of users in some sense knows the
relevant probabilities, and the problem is to format that knowledge in a way
that is convenient to input and store and that allows computation of the
answers to current questions in an expeditious fashion. The basic issue for the
inductive approach appears at first sight to be very different. In this
approach it is presumed that the relevant probabilities are only partially
known, and the problem is to extend that incomplete information in a reasonable
way to answer current questions. Clearly, this approach requires that some form
of induction be invoked. Of course, manageability is an important additional
concern. Despite their seeming differences, the two approaches have a fair
amount in common, especially with respect to the structural framework they
employ. Roughly speaking, this framework involves identifying clusters of
variables which strongly interact, establishing marginal probability
distributions on the clusters, and extending the subdistributions to a more
complete distribution, usually via a product formalism. The product extension
is justified on the modeling approach in terms of assumed conditional
independence; in the inductive approach the product form arises from an
inductive rule.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:44 GMT""}]","2013-04-12"
"1304.3084","Brian Falkenhainer","Brian Falkenhainer","Towards a General-Purpose Belief Maintenance System","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-71-76","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There currently exists a gap between the theories proposed by the probability
and uncertainty and the needs of Artificial Intelligence research. These
theories primarily address the needs of expert systems, using knowledge
structures which must be pre-compiled and remain static in structure during
runtime. Many Al systems require the ability to dynamically add and remove
parts of the current knowledge structure (e.g., in order to examine what the
world would be like for different causal theories). This requires more
flexibility than existing uncertainty systems display. In addition, many Al
researchers are only interested in using ""probabilities"" as a means of
obtaining an ordering, rather than attempting to derive an accurate
probabilistic account of a situation. This indicates the need for systems which
stress ease of use and don't require extensive probability information when one
cannot (or doesn't wish to) provide such information. This paper attempts to
help reconcile the gap between approaches to uncertainty and the needs of many
AI systems by examining the control issues which arise, independent of a
particular uncertainty calculus. when one tries to satisfy these needs. Truth
Maintenance Systems have been used extensively in problem solving tasks to help
organize a set of facts and detect inconsistencies in the believed state of the
world. These systems maintain a set of true/false propositions and their
associated dependencies. However, situations often arise in which we are unsure
of certain facts or in which the conclusions we can draw from available
information are somewhat uncertain. The non-monotonic TMS 12] was an attempt at
reasoning when all the facts are not known, but it fails to take into account
degrees of belief and how available evidence can combine to strengthen a
particular belief. This paper addresses the problem of probabilistic reasoning
as it applies to Truth Maintenance Systems. It describes a belief Maintenance
System that manages a current set of beliefs in much the same way that a TMS
manages a set of true/false propositions. If the system knows that belief in
fact is dependent in some way upon belief in fact2, then it automatically
modifies its belief in facts when new information causes a change in belief of
fact2. It models the behavior of a TMS, replacing its 3-valued logic (true,
false, unknown) with an infinite valued logic, in such a way as to reduce to a
standard TMS if all statements are given in absolute true/false terms. Belief
Maintenance Systems can, therefore, be thought of as a generalization of Truth
Maintenance Systems, whose possible reasoning tasks are a superset of those for
a TMS.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:49 GMT""}]","2013-04-12"
"1304.3085","B. R. Fox","B. R. Fox, Karl G. Kempf","Planning, Scheduling, and Uncertainty in the Sequence of Future Events","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-77-84","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Scheduling in the factory setting is compounded by computational complexity
and temporal uncertainty. Together, these two factors guarantee that the
process of constructing an optimal schedule will be costly and the chances of
executing that schedule will be slight. Temporal uncertainty in the task
execution time can be offset by several methods: eliminate uncertainty by
careful engineering, restore certainty whenever it is lost, reduce the
uncertainty by using more accurate sensors, and quantify and circumscribe the
remaining uncertainty. Unfortunately, these methods focus exclusively on the
sources of uncertainty and fail to apply knowledge of the tasks which are to be
scheduled. A complete solution must adapt the schedule of activities to be
performed according to the evolving state of the production world. The example
of vision-directed assembly is presented to illustrate that the principle of
least commitment, in the creation of a plan, in the representation of a
schedule, and in the execution of a schedule, enables a robot to operate
intelligently and efficiently, even in the presence of considerable uncertainty
in the sequence of future events.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:51:55 GMT""}]","2013-04-12"
"1304.3086","Pascal Fua","Pascal Fua","Deriving And Combining Continuous Possibility Functions in the Framework
  of Evidential Reasoning","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-85-90","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  To develop an approach to utilizing continuous statistical information within
the Dempster- Shafer framework, we combine methods proposed by Strat and by
Shafero We first derive continuous possibility and mass functions from
probability-density functions. Then we propose a rule for combining such
evidence that is simpler and more efficiently computed than Dempster's rule. We
discuss the relationship between Dempster's rule and our proposed rule for
combining evidence over continuous frames.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:00 GMT""}]","2013-04-12"
"1304.3087","Benjamin N. Grosof","Benjamin N. Grosof","Non-Monotonicity in Probabilistic Reasoning","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-91-98","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We start by defining an approach to non-monotonic probabilistic reasoning in
terms of non-monotonic categorical (true-false) reasoning. We identify a type
of non-monotonic probabilistic reasoning, akin to default inheritance, that is
commonly found in practice, especially in ""evidential"" and ""Bayesian""
reasoning. We formulate this in terms of the Maximization of Conditional
Independence (MCI), and identify a variety of applications for this sort of
default. We propose a formalization using Pointwise Circumscription. We compare
MCI to Maximum Entropy, another kind of non-monotonic principle, and conclude
by raising a number of open questions
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:05 GMT""}]","2013-04-12"
"1304.3088","Greg Hager","Greg Hager, Hugh F. Durrant-Whyte","Information and Multi-Sensor Coordination","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-99-108","cs.SY cs.AI cs.MA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The control and integration of distributed, multi-sensor perceptual systems
is a complex and challenging problem. The observations or opinions of different
sensors are often disparate incomparable and are usually only partial views.
Sensor information is inherently uncertain and in addition the individual
sensors may themselves be in error with respect to the system as a whole. The
successful operation of a multi-sensor system must account for this uncertainty
and provide for the aggregation of disparate information in an intelligent and
robust manner. We consider the sensors of a multi-sensor system to be members
or agents of a team, able to offer opinions and bargain in group decisions. We
will analyze the coordination and control of this structure using a theory of
team decision-making. We present some new analytic results on multi-sensor
aggregation and detail a simulation which we use to investigate our ideas. This
simulation provides a basis for the analysis of complex agent structures
cooperating in the presence of uncertainty. The results of this study are
discussed with reference to multi-sensor robot systems, distributed Al and
decision making under uncertainty.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:12 GMT""}]","2013-04-12"
"1304.3089","Shohara L. Hardt","Shohara L. Hardt","Flexible Interpretations: A Computational Model for Dynamic Uncertainty
  Assessment","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-109-114","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The investigations reported in this paper center on the process of dynamic
uncertainty assessment during interpretation tasks in real domain. In
particular, we are interested here in the nature of the control structure of
computer programs that can support multiple interpretation and smooth
transitions between them, in real time. Each step of the processing involves
the interpretation of one input item and the appropriate re-establishment of
the system's confidence of the correctness of its interpretation(s).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:17 GMT""}]","2013-04-12"
"1304.3090","David Heckerman","David Heckerman, Eric J. Horvitz","The Myth of Modularity in Rule-Based Systems","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-115-122","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we examine the concept of modularity, an often cited advantage
of the ruled-based representation methodology. We argue that the notion of
modularity consists of two distinct concepts which we call syntactic modularity
and semantic modularity. We argue that when reasoning under certainty, it is
reasonable to regard the rule-based approach as both syntactically and
semantically modular. However, we argue that in the case of plausible
reasoning, rules are syntactically modular but are rarely semantically modular.
To illustrate this point, we examine a particular approach for managing
uncertainty in rule-based systems called the MYCIN certainty factor model. We
formally define the concept of semantic modularity with respect to the
certainty factor model and discuss logical consequences of the definition. We
show that the assumption of semantic modularity imposes strong restrictions on
rules in a knowledge base. We argue that such restrictions are rarely valid in
practical applications. Finally, we suggest how the concept of semantic
modularity can be relaxed in a manner that makes it appropriate for plausible
reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:23 GMT""}]","2013-04-12"
"1304.3091","David Heckerman","David Heckerman","An Axiomatic Framework for Belief Updates","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-123-128","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the 1940's, a physicist named Cox provided the first formal justification
for the axioms of probability based on the subjective or Bayesian
interpretation. He showed that if a measure of belief satisfies several
fundamental properties, then the measure must be some monotonic transformation
of a probability. In this paper, measures of change in belief or belief updates
are examined. In the spirit of Cox, properties for a measure of change in
belief are enumerated. It is shown that if a measure satisfies these
properties, it must satisfy other restrictive conditions. For example, it is
shown that belief updates in a probabilistic context must be equal to some
monotonic transformation of a likelihood ratio. It is hoped that this formal
explication of the belief update paradigm will facilitate critical discussion
and useful extensions of the approach.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:28 GMT""}]","2013-04-12"
"1304.3092","Steven J. Henkind","Steven J. Henkind","Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based
  Systems","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-129-134","cs.AI cs.CL","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  There has been a considerable amount of work on uncertainty in
knowledge-based systems. This work has generally been concerned with
uncertainty arising from the strength of inferences and the weight of evidence.
In this paper we discuss another type of uncertainty: that which is due to
imprecision in the underlying primitives used to represent the knowledge of the
system. In particular, a given word may denote many similar but not identical
entities. Such words are said to be lexically imprecise. Lexical imprecision
has caused widespread problems in many areas. Unless this phenomenon is
recognized and appropriately handled, it can degrade the performance of
knowledge-based systems. In particular, it can lead to difficulties with the
user interface, and with the inferencing processes of these systems. Some
techniques are suggested for coping with this phenomenon.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:35 GMT""}]","2013-04-12"
"1304.3093","Robert Hummel","Robert Hummel, Michael Landy","Evidence as Opinions of Experts","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-135-144","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We describe a viewpoint on the Dempster/Shafer 'Theory of Evidence', and
provide an interpretation which regards the combination formulas as statistics
of the opinions of ""experts"". This is done by introducing spaces with binary
operations that are simpler to interpret or simpler to implement than the
standard combination formula, and showing that these spaces can be mapped
homomorphically onto the Dempster/Shafer theory of evidence space. The experts
in the space of ""opinions of experts"" combine information in a Bayesian
fashion. We present alternative spaces for the combination of evidence
suggested by this viewpoint.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:40 GMT""}]","2013-04-12"
"1304.3094","Charles I. Kalme","Charles I. Kalme","Decision Under Uncertainty in Diagnosis","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-145-150","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes the incorporation of uncertainty in diagnostic reasoning
based on the set covering model of Reggia et. al. extended to what in the
Artificial Intelligence dichotomy between deep and compiled (shallow, surface)
knowledge based diagnosis may be viewed as the generic form at the compiled end
of the spectrum. A major undercurrent in this is advocating the need for a
strong underlying model and an integrated set of support tools for carrying
such a model in order to deal with uncertainty.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:45 GMT""}]","2013-04-12"
"1304.3095","Henry E. Kyburg Jr.","Henry E. Kyburg Jr","Knowledge and Uncertainty","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-151-158","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  One purpose -- quite a few thinkers would say the main purpose -- of seeking
knowledge about the world is to enhance our ability to make good decisions. An
item of knowledge that can make no conceivable difference with regard to
anything we might do would strike many as frivolous. Whether or not we want to
be philosophical pragmatists in this strong sense with regard to everything we
might want to enquire about, it seems a perfectly appropriate attitude to adopt
toward artificial knowledge systems. If is granted that we are ultimately
concerned with decisions, then some constraints are imposed on our measures of
uncertainty at the level of decision making. If our measure of uncertainty is
real-valued, then it isn't hard to show that it must satisfy the classical
probability axioms. For example, if an act has a real-valued utility U(E) if
the event E obtains, and the same real-valued utility if the denial of E
obtains, so that U(E) = U(-E), then the expected utility of that act must be
U(E), and that must be the same as the uncertainty-weighted average of the
returns of the act, p-U(E) + q-U('E), where p and q represent the uncertainty
of E and-E respectively. But then we must have p + q = 1.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:50 GMT""}]","2013-04-12"
"1304.3096","Kathryn Blackmond Laskey","Kathryn Blackmond Laskey, Marvin S. Cohen","An Application of Non-Monotonic Probabilistic Reasoning to Air Force
  Threat Correlation","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-159-166","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Current approaches to expert systems' reasoning under uncertainty fail to
capture the iterative revision process characteristic of intelligent human
reasoning. This paper reports on a system, called the Non-monotonic
Probabilist, or NMP (Cohen, et al., 1985). When its inferences result in
substantial conflict, NMP examines and revises the assumptions underlying the
inferences until conflict is reduced to acceptable levels. NMP has been
implemented in a demonstration computer-based system, described below, which
supports threat correlation and in-flight route replanning by Air Force pilots.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:52:56 GMT""}]","2013-04-12"
"1304.3097","Tod S. Levitt","Tod S. Levitt","Bayesian Inference for Radar Imagery Based Surveillance","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-167-174","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We are interested in creating an automated or semi-automated system with the
capability of taking a set of radar imagery, collection parameters and a priori
map and other tactical data, and producing likely interpretations of the
possible military situations given the available evidence. This paper is
concerned with the problem of the interpretation and computation of certainty
or belief in the conclusions reached by such a system.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:02 GMT""}]","2013-04-12"
"1304.3098","Ze-Nian Li","Ze-Nian Li, Leonard Uhr","Evidential Reasoning in Parallel Hierarchical Vision Programs","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-175-182","cs.AI cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper presents an efficient adaptation and application of the
Dempster-Shafer theory of evidence, one that can be used effectively in a
massively parallel hierarchical system for visual pattern perception. It
describes the techniques used, and shows in an extended example how they serve
to improve the system's performance as it applies a multiple-level set of
processes.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:08 GMT""}]","2013-04-12"
"1304.3099","Ronald P. Loui","Ronald P. Loui","Computing Reference Classes","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-183-188","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  For any system with limited statistical knowledge, the combination of
evidence and the interpretation of sampling information require the
determination of the right reference class (or of an adequate one). The present
note (1) discusses the use of reference classes in evidential reasoning, and
(2) discusses implementations of Kyburg's rules for reference classes. This
paper contributes the first frank discussion of how much of Kyburg's system is
needed to be powerful, how much can be computed effectively, and how much is
philosophical fat.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:13 GMT""}]","2013-04-12"
"1304.3100","Uttam Mukhopadhyay","Uttam Mukhopadhyay","An Uncertainty Management Calculus for Ordering Searches in Distributed
  Dynamic Databases","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-189-192","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  MINDS is a distributed system of cooperating query engines that customize,
document retrieval for each user in a dynamic environment. It improves its
performance and adapts to changing patterns of document distribution by
observing system-user interactions and modifying the appropriate certainty
factors, which act as search control parameters. It argued here that the
uncertainty management calculus must account for temporal precedence,
reliability of evidence, degree of support for a proposition, and saturation
effects. The calculus presented here possesses these features. Some results
obtained with this scheme are discussed.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:17 GMT""}]","2013-04-12"
"1304.3101","Steven W. Norton","Steven W. Norton","An Explanation Mechanism for Bayesian Inferencing Systems","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-193-200","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Explanation facilities are a particularly important feature of expert system
frameworks. It is an area in which traditional rule-based expert system
frameworks have had mixed results. While explanations about control are well
handled, facilities are needed for generating better explanations concerning
knowledge base content. This paper approaches the explanation problem by
examining the effect an event has on a variable of interest within a symmetric
Bayesian inferencing system. We argue that any effect measure operating in this
context must satisfy certain properties. Such a measure is proposed. It forms
the basis for an explanation facility which allows the user of the Generalized
Bayesian Inferencing System to question the meaning of the knowledge base. That
facility is described in detail.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:23 GMT""}]","2013-04-12"
"1304.3102","Judea Pearl","Judea Pearl","Distributed Revision of Belief Commitment in Multi-Hypothesis
  Interpretations","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-201-210","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper extends the applications of belief-networks to include the
revision of belief commitments, i.e., the categorical acceptance of a subset of
hypotheses which, together, constitute the most satisfactory explanation of the
evidence at hand. A coherent model of non-monotonic reasoning is established
and distributed algorithms for belief revision are presented. We show that, in
singly connected networks, the most satisfactory explanation can be found in
linear time by a message-passing algorithm similar to the one used in belief
updating. In multiply-connected networks, the problem may be exponentially hard
but, if the network is sparse, topological considerations can be used to render
the interpretation task tractable. In general, finding the most probable
combination of hypotheses is no more complex than computing the degree of
belief for any individual hypothesis. Applications to medical diagnosis are
illustrated.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:29 GMT""}]","2013-04-12"
"1304.3103","Igor Roizer","Igor Roizer, Judea Pearl","Learning Link-Probabilities in Causal Trees","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-211-214","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A learning algorithm is presented which given the structure of a causal tree,
will estimate its link probabilities by sequential measurements on the leaves
only. Internal nodes of the tree represent conceptual (hidden) variables
inaccessible to observation. The method described is incremental, local,
efficient, and remains robust to measurement imprecisions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:34 GMT""}]","2013-04-12"
"1304.3104","Enrique H. Ruspini","Enrique H. Ruspini","Approximate Deduction in Single Evidential Bodies","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-215-222","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Results on approximate deduction in the context of the calculus of evidence
of Dempster-Shafer and the theory of interval probabilities are reported.
Approximate conditional knowledge about the truth of conditional propositions
was assumed available and expressed as sets of possible values (actually
numeric intervals) of conditional probabilities. Under different
interpretations of this conditional knowledge, several formulas were produced
to integrate unconditioned estimates (assumed given as sets of possible values
of unconditioned probabilities) with conditional estimates. These formulas are
discussed together with the computational characteristics of the methods
derived from them. Of particular importance is one such evidence integration
formulation, produced under a belief oriented interpretation, which
incorporates both modus ponens and modus tollens inferential mechanisms, allows
integration of conditioned and unconditioned knowledge without resorting to
iterative or sequential approximations, and produces elementary mass
distributions as outputs using similar distributions as inputs.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:39 GMT""}]","2013-04-12"
"1304.3105","Shimon Schocken","Shimon Schocken","The Rational and Computational Scope of Probabilistic Rule-Based Expert
  Systems","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-223-228","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Belief updating schemes in artificial intelligence may be viewed as three
dimensional languages, consisting of a syntax (e.g. probabilities or certainty
factors), a calculus (e.g. Bayesian or CF combination rules), and a semantics
(i.e. cognitive interpretations of competing formalisms). This paper studies
the rational scope of those languages on the syntax and calculus grounds. In
particular, the paper presents an endomorphism theorem which highlights the
limitations imposed by the conditional independence assumptions implicit in the
CF calculus. Implications of the theorem to the relationship between the CF and
the Bayesian languages and the Dempster-Shafer theory of evidence are
presented. The paper concludes with a discussion of some implications on
rule-based knowledge engineering in uncertain domains.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:46 GMT""}]","2013-04-12"
"1304.3106","Stanley M. Schwartz","Stanley M. Schwartz, Jonathan Baron, John R. Clarke","A Causal Bayesian Model for the Diagnosis of Appendicitis","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-229-236","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The causal Bayesian approach is based on the assumption that effects (e.g.,
symptoms) that are not conditionally independent with respect to some causal
agent (e.g., a disease) are conditionally independent with respect to some
intermediate state caused by the agent, (e.g., a pathological condition). This
paper describes the development of a causal Bayesian model for the diagnosis of
appendicitis. The paper begins with a description of the standard Bayesian
approach to reasoning about uncertainty and the major critiques it faces. The
paper then lays the theoretical groundwork for the causal extension of the
Bayesian approach, and details specific improvements we have developed. The
paper then goes on to describe our knowledge engineering and implementation and
the results of a test of the system. The paper concludes with a discussion of
how the causal Bayesian approach deals with the criticisms of the standard
Bayesian model and why it is superior to alternative approaches to reasoning
about uncertainty popular in the Al community.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:52 GMT""}]","2013-04-12"
"1304.3107","Ross D. Shachter","Ross D. Shachter, David Heckerman","A Backwards View for Assessment","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-237-242","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Much artificial intelligence research focuses on the problem of deducing the
validity of unobservable propositions or hypotheses from observable evidence.!
Many of the knowledge representation techniques designed for this problem
encode the relationship between evidence and hypothesis in a directed manner.
Moreover, the direction in which evidence is stored is typically from evidence
to hypothesis.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:53:57 GMT""}]","2013-04-12"
"1304.3108","Ross D. Shachter","Ross D. Shachter","DAVID: Influence Diagram Processing System for the Macintosh","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-243-248","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Influence diagrams are a directed graph representation for uncertainties as
probabilities. The graph distinguishes between those variables which are under
the control of a decision maker (decisions, shown as rectangles) and those
which are not (chances, shown as ovals), as well as explicitly denoting a goal
for solution (value, shown as a rounded rectangle.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:03 GMT""}]","2013-04-12"
"1304.3109","Prakash P. Shenoy","Prakash P. Shenoy, Glenn Shafer, Khaled Mellouli","Propagation of Belief Functions: A Distributed Approach","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-249-260","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we describe a scheme for propagating belief functions in
certain kinds of trees using only local computations. This scheme generalizes
the computational scheme proposed by Shafer and Logan1 for diagnostic trees of
the type studied by Gordon and Shortliffe, and the slightly more general scheme
given by Shafer for hierarchical evidence. It also generalizes the scheme
proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's
causal trees and Gordon and Shortliffe's diagnostic trees are both ways of
breaking the evidence that bears on a large problem down into smaller items of
evidence that bear on smaller parts of the problem so that these smaller
problems can be dealt with one at a time. This localization of effort is often
essential in order to make the process of probability judgment feasible, both
for the person who is making probability judgments and for the machine that is
combining them. The basic structure for our scheme is a type of tree that
generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this
general type permit localized computation in Pearl's sense. They are based on
qualitative judgments of conditional independence. We believe that the scheme
we describe here will prove useful in expert systems. It is now clear that the
successful propagation of probabilities or certainty factors in expert systems
requires much more structure than can be provided in a pure production-system
framework. Bayesian schemes, on the other hand, often make unrealistic demands
for structure. The propagation of belief functions in trees and more general
networks stands on a middle ground where some sensible and useful things can be
done. We would like to emphasize that the basic idea of local computation for
propagating probabilities is due to Judea Pearl. It is a very innovative idea;
we do not believe that it can be found in the Bayesian literature prior to
Pearl's work. We see our contribution as extending the usefulness of Pearl's
idea by generalizing it from Bayesian probabilities to belief functions. In the
next section, we give a brief introduction to belief functions. The notions of
qualitative independence for partitions and a qualitative Markov tree are
introduced in Section III. Finally, in Section IV, we describe a scheme for
propagating belief functions in qualitative Markov trees.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:09 GMT""}]","2013-04-12"
"1304.3110","David Sher","David Sher","Appropriate and Inappropriate Estimation Techniques","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-261-266","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Mode {also called MAP} estimation, mean estimation and median estimation are
examined here to determine when they can be safely used to derive {posterior)
cost minimizing estimates. (These are all Bayes procedures, using the mode.
mean. or median of the posterior distribution). It is found that modal
estimation only returns cost minimizing estimates when the cost function is
0-t. If the cost function is a function of distance then mean estimation only
returns cost minimizing estimates when the cost function is squared distance
from the true value and median estimation only returns cost minimizing
estimates when the cost function ts the distance from the true value. Results
are presented on the goodness or modal estimation with non 0-t cost functions
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:15 GMT""}]","2013-04-12"
"1304.3111","Randall Smith","Randall Smith, Matthew Self, Peter Cheeseman","Estimating Uncertain Spatial Relationships in Robotics","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-267-288","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In this paper, we describe a representation for spatial information, called
the stochastic map, and associated procedures for building it, reading
information from it, and revising it incrementally as new information is
obtained. The map contains the estimates of relationships among objects in the
map, and their uncertainties, given all the available information. The
procedures provide a general solution to the problem of estimating uncertain
relative spatial relationships. The estimates are probabilistic in nature, an
advance over the previous, very conservative, worst-case approaches to the
problem. Finally, the procedures are developed in the context of
state-estimation and filtering theory, which provides a solid basis for
numerous extensions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:21 GMT""}]","2013-04-12"
"1304.3112","Masaki Togai","Masaki Togai, Hiroyuki Watanabe","A VLSI Design and Implementation for a Real-Time Approximate Reasoning","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-289-296","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The role of inferencing with uncertainty is becoming more important in
rule-based expert systems (ES), since knowledge given by a human expert is
often uncertain or imprecise. We have succeeded in designing a VLSI chip which
can perform an entire inference process based on fuzzy logic. The design of the
VLSI fuzzy inference engine emphasizes simplicity, extensibility, and
efficiency (operational speed and layout area). It is fabricated in 2.5 um CMOS
technology. The inference engine consists of three major components; a rule set
memory, an inference processor, and a controller. In this implementation, a
rule set memory is realized by a read only memory (ROM). The controller
consists of two counters. In the inference processor, one data path is laid out
for each rule. The number of the inference rule can be increased adding more
data paths to the inference processor. All rules are executed in parallel, but
each rule is processed serially. The logical structure of fuzzy inference
proposed in the current paper maps nicely onto the VLSI structure. A two-phase
nonoverlapping clocking scheme is used. Timing tests indicate that the
inference engine can operate at approximately 20.8 MHz. This translates to an
execution speed of approximately 80,000 Fuzzy Logical Inferences Per Second
(FLIPS), and indicates that the inference engine is suitable for a demanding
real-time application. The potential applications include decision-making in
the area of command and control for intelligent robot systems, process control,
missile and aircraft guidance, and other high performance machines.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:27 GMT""}]","2013-04-12"
"1304.3113","Richard M. Tong","Richard M. Tong, Lee A. Appelbaum, D. G. Shapiro","A General Purpose Inference Engine for Evidential Reasoning Research","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-297-302","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The purpose of this paper is to report on the most recent developments in our
ongoing investigation of the representation and manipulation of uncertainty in
automated reasoning systems. In our earlier studies (Tong and Shapiro, 1985) we
described a series of experiments with RUBRIC (Tong et al., 1985), a system for
full-text document retrieval, that generated some interesting insights into the
effects of choosing among a class of scalar valued uncertainty calculi. [n
order to extend these results we have begun a new series of experiments with a
larger class of representations and calculi, and to help perform these
experiments we have developed a general purpose inference engine.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:33 GMT""}]","2013-04-12"
"1304.3114","Silvio Ursic","Silvio Ursic","Generalizing Fuzzy Logic Probabilistic Inferences","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-303-310","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Linear representations for a subclass of boolean symmetric functions selected
by a parity condition are shown to constitute a generalization of the linear
constraints on probabilities introduced by Boole. These linear constraints are
necessary to compute probabilities of events with relations between the.
arbitrarily specified with propositional calculus boolean formulas.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:38 GMT""}]","2013-04-12"
"1304.3115","Michael P. Wellman","Michael P. Wellman","Qualitative Probabilistic Networks for Planning Under Uncertainty","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-311-318","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Bayesian networks provide a probabilistic semantics for qualitative
assertions about likelihood. A qualitative reasoner based on an algebra over
these assertions can derive further conclusions about the influence of actions.
While the conclusions are much weaker than those computed from complete
probability distributions, they are still valuable for suggesting potential
actions, eliminating obviously inferior plans, identifying important tradeoffs,
and explaining probabilistic models.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:44 GMT""}]","2013-04-12"
"1304.3116","Ben P. Wise","Ben P. Wise","Experimentally Comparing Uncertain Inference Systems to Probability","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-319-332","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines the biases and performance of several uncertain inference
systems: Mycin, a variant of Mycin. and a simplified version of probability
using conditional independence assumptions. We present axiomatic arguments for
using Minimum Cross Entropy inference as the best way to do uncertain
inference. For Mycin and its variant we found special situations where its
performance was very good, but also situations where performance was worse than
random guessing, or where data was interpreted as having the opposite of its
true import We have found that all three of these systems usually gave accurate
results, and that the conditional independence assumptions gave the most robust
results. We illustrate how the Importance of biases may be quantitatively
assessed and ranked. Considerations of robustness might be a critical factor is
selecting UlS's for a given application.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:50 GMT""}]","2013-04-12"
"1304.3117","Robert M. Yadrick","Robert M. Yadrick, Bruce M. Perrin, David S. Vaughan, Peter D. Holden,
  Karl G. Kempf","Evaluation of Uncertain Inference Models I: PROSPECTOR","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-333-338","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines the accuracy of the PROSPECTOR model for uncertain
reasoning. PROSPECTOR's solutions for a large number of computer-generated
inference networks were compared to those obtained from probability theory and
minimum cross-entropy calculations. PROSPECTOR's answers were generally
accurate for a restricted subset of problems that are consistent with its
assumptions. However, even within this subset, we identified conditions under
which PROSPECTOR's performance deteriorates.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:54:56 GMT""}]","2013-04-12"
"1304.3118","Ronald R. Yager","Ronald R. Yager","On Implementing Usual Values","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-339-346","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In many cases commonsense knowledge consists of knowledge of what is usual.
In this paper we develop a system for reasoning with usual information. This
system is based upon the fact that these pieces of commonsense information
involve both a probabilistic aspect and a granular aspect. We implement this
system with the aid of possibility-probability granules.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:01 GMT""}]","2013-04-12"
"1304.3119","Lotfi Zadeh","Lotfi Zadeh, Anca Ralescu","On the Combinality of Evidence in the Dempster-Shafer Theory","Appears in Proceedings of the Second Conference on Uncertainty in
  Artificial Intelligence (UAI1986)",,,"UAI-P-1986-PG-347-349","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the current versions of the Dempster-Shafer theory, the only essential
restriction on the validity of the rule of combination is that the sources of
evidence must be statistically independent. Under this assumption, it is
permissible to apply the Dempster-Shafer rule to two or mere distinct
probability distributions.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:05 GMT""}]","2013-04-12"
"1304.3418","Benjamin N. Grosof","Benjamin N. Grosof","An Inequality Paradigm for Probabilistic Knowledge","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-1-8","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We propose an inequality paradigm for probabilistic reasoning based on a
logic of upper and lower bounds on conditional probabilities. We investigate a
family of probabilistic logics, generalizing the work of Nilsson [14]. We
develop a variety of logical notions for probabilistic reasoning, including
soundness, completeness justification; and convergence: reduction of a theory
to a simpler logical class. We argue that a bound view is especially useful for
describing the semantics of probabilistic knowledge representation and for
describing intermediate states of probabilistic inference and updating. We show
that the Dempster-Shafer theory of evidence is formally identical to a special
case of our generalized probabilistic logic. Our paradigm thus incorporates
both Bayesian ""rule-based"" approaches and avowedly non-Bayesian ""evidential""
approaches such as MYCIN and DempsterShafer. We suggest how to integrate the
two ""schools"", and explore some possibilities for novel synthesis of a variety
of ideas in probabilistic reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:33 GMT""}]","2013-04-15"
"1304.3419","David Heckerman","David Heckerman","Probabilistic Interpretations for MYCIN's Certainty Factors","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-9-20","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines the quantities used by MYCIN to reason with uncertainty,
called certainty factors. It is shown that the original definition of certainty
factors is inconsistent with the functions used in MYCIN to combine the
quantities. This inconsistency is used to argue for a redefinition of certainty
factors in terms of the intuitively appealing desiderata associated with the
combining functions. It is shown that this redefinition accommodates an
unlimited number of probabilistic interpretations. These interpretations are
shown to be monotonic transformations of the likelihood ratio p(EIH)/p(El H).
The construction of these interpretations provides insight into the assumptions
implicit in the certainty factor model. In particular, it is shown that if
uncertainty is to be propagated through an inference network in accordance with
the desiderata, evidence must be conditionally independent given the hypothesis
and its negation and the inference network must have a tree structure. It is
emphasized that assumptions implicit in the model are rarely true in practical
applications. Methods for relaxing the assumptions are suggested.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:40 GMT""}]","2013-04-15"
"1304.3420","Daniel Hunter","Daniel Hunter","Uncertain Reasoning Using Maximum Entropy Inference","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-21-27","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The use of maximum entropy inference in reasoning with uncertain information
is commonly justified by an information-theoretic argument. This paper
discusses a possible objection to this information-theoretic justification and
shows how it can be met. I then compare maximum entropy inference with certain
other currently popular methods for uncertain reasoning. In making such a
comparison, one must distinguish between static and dynamic theories of degrees
of belief: a static theory concerns the consistency conditions for degrees of
belief at a given time; whereas a dynamic theory concerns how one's degrees of
belief should change in the light of new information. It is argued that maximum
entropy is a dynamic theory and that a complete theory of uncertain reasoning
can be gotten by combining maximum entropy inference with probability theory,
which is a static theory. This total theory, I argue, is much better grounded
than are other theories of uncertain reasoning.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:46 GMT""}]","2013-04-15"
"1304.3421","Rodney W. Johnson","Rodney W. Johnson","Independence and Bayesian Updating Methods","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-28-30","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Duda, Hart, and Nilsson have set forth a method for rule-based inference
systems to use in updating the probabilities of hypotheses on the basis of
multiple items of new evidence. Pednault, Zucker, and Muresan claimed to give
conditions under which independence assumptions made by Duda et al. preclude
updating-that is, prevent the evidence from altering the probabilities of the
hypotheses. Glymour refutes Pednault et al.'s claim with a counterexample of a
rather special form (one item of evidence is incompatible with all but one of
the hypotheses); he raises, but leaves open, the question whether their result
would be true with an added assumption to rule out such special cases. We show
that their result does not hold even with the added assumption, but that it can
nevertheless be largely salvaged. Namely, under the conditions assumed by
Pednault et al., at most one of the items of evidence can alter the probability
of any given hypothesis; thus, although updating is possible, multiple updating
for any of the hypotheses is precluded.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:51 GMT""}]","2013-04-15"
"1304.3422","Judea Pearl","Judea Pearl","A Constraint Propagation Approach to Probabilistic Reasoning","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-31-42","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The paper demonstrates that strict adherence to probability theory does not
preclude the use of concurrent, self-activated constraint-propagation
mechanisms for managing uncertainty. Maintaining local records of
sources-of-belief allows both predictive and diagnostic inferences to be
activated simultaneously and propagate harmoniously towards a stable
equilibrium.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:55:56 GMT""}]","2013-04-15"
"1304.3423","John E. Shore","John E. Shore","Relative Entropy, Probabilistic Inference and AI","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-43-47","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Various properties of relative entropy have led to its widespread use in
information theory. These properties suggest that relative entropy has a role
to play in systems that attempt to perform inference in terms of probability
distributions. In this paper, I will review some basic properties of relative
entropy as well as its role in probabilistic inference. I will also mention
briefly a few existing and potential applications of relative entropy to
so-called artificial intelligence (AI).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:01 GMT""}]","2013-04-15"
"1304.3424","Ray Solomonoff","Ray Solomonoff","Foundations of Probability Theory for AI - The Application of
  Algorithmic Probability to Problems in Artificial Intelligence","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-48-56","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper covers two topics: first an introduction to Algorithmic Complexity
Theory: how it defines probability, some of its characteristic properties and
past successful applications. Second, we apply it to problems in A.I. - where
it promises to give near optimum search procedures for two very broad classes
of problems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:07 GMT""}]","2013-04-15"
"1304.3425","Piero P. Bonissone","Piero P. Bonissone, Keith S. Decker","Selecting Uncertainty Calculi and Granularity: An Experiment in
  Trading-Off Precision and Complexity","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-57-66","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The management of uncertainty in expert systems has usually been left to ad
hoc representations and rules of combinations lacking either a sound theory or
clear semantics. The objective of this paper is to establish a theoretical
basis for defining the syntax and semantics of a small subset of calculi of
uncertainty operating on a given term set of linguistic statements of
likelihood. Each calculus is defined by specifying a negation, a conjunction
and a disjunction operator. Families of Triangular norms and conorms constitute
the most general representations of conjunction and disjunction operators.
These families provide us with a formalism for defining an infinite number of
different calculi of uncertainty. The term set will define the uncertainty
granularity, i.e. the finest level of distinction among different
quantifications of uncertainty. This granularity will limit the ability to
differentiate between two similar operators. Therefore, only a small finite
subset of the infinite number of calculi will produce notably different
results. This result is illustrated by two experiments where nine and eleven
different calculi of uncertainty are used with three term sets containing five,
nine, and thirteen elements, respectively. Finally, the use of context
dependent rule set is proposed to select the most appropriate calculus for any
given situation. Such a rule set will be relatively small since it must only
describe the selection policies for a small number of calculi (resulting from
the analyzed trade-off between complexity and precision).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:13 GMT""}]","2013-04-15"
"1304.3426","Marvin S. Cohen","Marvin S. Cohen","A Framework for Non-Monotonic Reasoning About Probabilistic Assumptions","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-67-75","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Attempts to replicate probabilistic reasoning in expert systems have
typically overlooked a critical ingredient of that process. Probabilistic
analysis typically requires extensive judgments regarding interdependencies
among hypotheses and data, and regarding the appropriateness of various
alternative models. The application of such models is often an iterative
process, in which the plausibility of the results confirms or disconfirms the
validity of assumptions made in building the model. In current expert systems,
by contrast, probabilistic information is encapsulated within modular rules
(involving, for example, ""certainty factors""), and there is no mechanism for
reviewing the overall form of the probability argument or the validity of the
judgments entering into it.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:20 GMT""}]","2013-04-15"
"1304.3427","Robert Fung","Robert Fung, Chee Yee Chong","Metaprobability and Dempster-Shafer in Evidential Reasoning","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-76-83","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Evidential reasoning in expert systems has often used ad-hoc uncertainty
calculi. Although it is generally accepted that probability theory provides a
firm theoretical foundation, researchers have found some problems with its use
as a workable uncertainty calculus. Among these problems are representation of
ignorance, consistency of probabilistic judgements, and adjustment of a priori
judgements with experience. The application of metaprobability theory to
evidential reasoning is a new approach to solving these problems.
Metaprobability theory can be viewed as a way to provide soft or hard
constraints on beliefs in much the same manner as the Dempster-Shafer theory
provides constraints on probability masses on subsets of the state space. Thus,
we use the Dempster-Shafer theory, an alternative theory of evidential
reasoning to illuminate metaprobability theory as a theory of evidential
reasoning. The goal of this paper is to compare how metaprobability theory and
Dempster-Shafer theory handle the adjustment of beliefs with evidence with
respect to a particular thought experiment. Sections 2 and 3 give brief
descriptions of the metaprobability and Dempster-Shafer theories.
Metaprobability theory deals with higher order probabilities applied to
evidential reasoning. Dempster-Shafer theory is a generalization of probability
theory which has evolved from a theory of upper and lower probabilities.
Section 4 describes a thought experiment and the metaprobability and
DempsterShafer analysis of the experiment. The thought experiment focuses on
forming beliefs about a population with 6 types of members {1, 2, 3, 4, 5, 6}.
A type is uniquely defined by the values of three features: A, B, C. That is,
if the three features of one member of the population were known then its type
could be ascertained. Each of the three features has two possible values, (e.g.
A can be either ""a0"" or ""al""). Beliefs are formed from evidence accrued from
two sensors: sensor A, and sensor B. Each sensor senses the corresponding
defining feature. Sensor A reports that half of its observations are ""a0"" and
half the observations are 'al'. Sensor B reports that half of its observations
are ``b0,' and half are ""bl"". Based on these two pieces of evidence, what
should be the beliefs on the distribution of types in the population? Note that
the third feature is not observed by any sensor.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:26 GMT""}]","2013-04-15"
"1304.3428","Matthew L. Ginsberg","Matthew L. Ginsberg","Implementing Probabilistic Reasoning","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-84-90","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  General problems in analyzing information in a probabilistic database are
considered. The practical difficulties (and occasional advantages) of storing
uncertain data, of using it conventional forward- or backward-chaining
inference engines, and of working with a probabilistic version of resolution
are discussed. The background for this paper is the incorporation of uncertain
reasoning facilities in MRS, a general-purpose expert system building tool.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:32 GMT""}]","2013-04-15"
"1304.3429","Glenn Shafer","Glenn Shafer","Probability Judgement in Artificial Intelligence","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-91-98","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper is concerned with two theories of probability judgment: the
Bayesian theory and the theory of belief functions. It illustrates these
theories with some simple examples and discusses some of the issues that arise
when we try to implement them in expert systems. The Bayesian theory is well
known; its main ideas go back to the work of Thomas Bayes (1702-1761). The
theory of belief functions, often called the Dempster-Shafer theory in the
artificial intelligence community, is less well known, but it has even older
antecedents; belief-function arguments appear in the work of George Hooper
(16401723) and James Bernoulli (1654-1705). For elementary expositions of the
theory of belief functions, see Shafer (1976, 1985).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:37 GMT""}]","2013-04-15"
"1304.3430","Ben P. Wise","Ben P. Wise, Max Henrion","A Framework for Comparing Uncertain Inference Systems to Probability","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-99-108","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Several different uncertain inference systems (UISs) have been developed for
representing uncertainty in rule-based expert systems. Some of these, such as
Mycin's Certainty Factors, Prospector, and Bayes' Networks were designed as
approximations to probability, and others, such as Fuzzy Set Theory and
DempsterShafer Belief Functions were not. How different are these UISs in
practice, and does it matter which you use? When combining and propagating
uncertain information, each UIS must, at least by implication, make certain
assumptions about correlations not explicily specified. The maximum entropy
principle with minimum cross-entropy updating, provides a way of making
assumptions about the missing specification that minimizes the additional
information assumed, and thus offers a standard against which the other UISs
can be compared. We describe a framework for the experimental comparison of the
performance of different UISs, and provide some illustrative results.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:43 GMT""}]","2013-04-15"
"1304.3431","Norman C. Dalkey","Norman C. Dalkey","Inductive Inference and the Representation of Uncertainty","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-109-116","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The form and justification of inductive inference rules depend strongly on
the representation of uncertainty. This paper examines one generic
representation, namely, incomplete information. The notion can be formalized by
presuming that the relevant probabilities in a decision problem are known only
to the extent that they belong to a class K of probability distributions. The
concept is a generalization of a frequent suggestion that uncertainty be
represented by intervals or ranges on probabilities. To make the representation
useful for decision making, an inductive rule can be formulated which
determines, in a well-defined manner, a best approximation to the unknown
probability, given the set K. In addition, the knowledge set notion entails a
natural procedure for updating -- modifying the set K given new evidence.
Several non-intuitive consequences of updating emphasize the differences
between inference with complete and inference with incomplete information.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:49 GMT""}]","2013-04-15"
"1304.3432","Stephen Jose Hanson","Stephen Jose Hanson, Malcolm Bauer","Machine Learning, Clustering, and Polymorphy","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-117-128","cs.AI cs.CL cs.LG","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper describes a machine induction program (WITT) that attempts to
model human categorization. Properties of categories to which human subjects
are sensitive includes best or prototypical members, relative contrasts between
putative categories, and polymorphy (neither necessary or sufficient features).
This approach represents an alternative to usual Artificial Intelligence
approaches to generalization and conceptual clustering which tend to focus on
necessary and sufficient feature rules, equivalence classes, and simple search
and match schemes. WITT is shown to be more consistent with human
categorization while potentially including results produced by more traditional
clustering schemes. Applications of this approach in the domains of expert
systems and information retrieval are also discussed.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:56:55 GMT""}]","2013-04-15"
"1304.3433","Larry Rendell","Larry Rendell","Induction, of and by Probability","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-129-134","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper examines some methods and ideas underlying the author's successful
probabilistic learning systems(PLS), which have proven uniquely effective and
efficient in generalization learning or induction. While the emerging
principles are generally applicable, this paper illustrates them in heuristic
search, which demands noise management and incremental learning. In our
approach, both task performance and learning are guided by probability.
Probabilities are incrementally normalized and revised, and their errors are
located and corrected.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:01 GMT""}]","2013-04-15"
"1304.3434","David S. Vaughan","David S. Vaughan, Bruce M. Perrin, Robert M. Yadrick, Peter D. Holden,
  Karl G. Kempf","An Odds Ratio Based Inference Engine","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-135-142","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Expert systems applications that involve uncertain inference can be
represented by a multidimensional contingency table. These tables offer a
general approach to inferring with uncertain evidence, because they can embody
any form of association between any number of pieces of evidence and
conclusions. (Simpler models may be required, however, if the number of pieces
of evidence bearing on a conclusion is large.) This paper presents a method of
using these tables to make uncertain inferences without assumptions of
conditional independence among pieces of evidence or heuristic combining rules.
As evidence is accumulated, new joint probabilities are calculated so as to
maintain any dependencies among the pieces of evidence that are found in the
contingency table. The new conditional probability of the conclusion is then
calculated directly from these new joint probabilities and the conditional
probabilities in the contingency table.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:05 GMT""}]","2013-04-15"
"1304.3435","Moshe Ben-Bassat","Moshe Ben-Bassat, Oded Maler","A Framework for Control Strategies in Uncertain Inference Networks","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-143-151","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Control Strategies for hierarchical tree-like probabilistic inference
networks are formulated and investigated. Strategies that utilize staged
look-ahead and temporary focus on subgoals are formalized and refined using the
Depth Vector concept that serves as a tool for defining the 'virtual tree'
regarded by the control strategy. The concept is illustrated by four types of
control strategies for three-level trees that are characterized according to
their Depth Vector, and according to the way they consider intermediate nodes
and the role that they let these nodes play. INFERENTI is a computerized
inference system written in Prolog, which provides tools for exercising a
variety of control strategies. The system also provides tools for simulating
test data and for comparing the relative average performance under different
strategies.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:11 GMT""}]","2013-04-15"
"1304.3436","Henry Hamburger","Henry Hamburger","Combining Uncertain Estimates","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-152-159","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In a real expert system, one may have unreliable, unconfident, conflicting
estimates of the value for a particular parameter. It is important for decision
making that the information present in this aggregate somehow find its way into
use. We cast the problem of representing and combining uncertain estimates as
selection of two kinds of functions, one to determine an estimate, the other
its uncertainty. The paper includes a long list of properties that such
functions should satisfy, and it presents one method that satisfies them.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:16 GMT""}]","2013-04-15"
"1304.3437","John F. Lemmer","John F. Lemmer","Confidence Factors, Empiricism and the Dempster-Shafer Theory of
  Evidence","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-160-176","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The issue of confidence factors in Knowledge Based Systems has become
increasingly important and Dempster-Shafer (DS) theory has become increasingly
popular as a basis for these factors. This paper discusses the need for an
empirical lnterpretatlon of any theory of confidence factors applied to
Knowledge Based Systems and describes an empirical lnterpretatlon of DS theory
suggesting that the theory has been extensively misinterpreted. For the
essentially syntactic DS theory, a model is developed based on sample spaces,
the traditional semantic model of probability theory. This model is used to
show that, if belief functions are based on reasonably accurate sampling or
observation of a sample space, then the beliefs and upper probabilities as
computed according to DS theory cannot be interpreted as frequency ratios.
Since many proposed applications of DS theory use belief functions in
situations with statistically derived evidence (Wesley [1]) and seem to appeal
to statistical intuition to provide an lnterpretatlon of the results as has
Garvey [2], it may be argued that DS theory has often been misapplied.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:24 GMT""}]","2013-04-15"
"1304.3438","Alan Bundy","Alan Bundy","Incidence Calculus: A Mechanism for Probabilistic Reasoning","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-177-184","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Mechanisms for the automation of uncertainty are required for expert systems.
Sometimes these mechanisms need to obey the properties of probabilistic
reasoning. A purely numeric mechanism, like those proposed so far, cannot
provide a probabilistic logic with truth functional connectives. We propose an
alternative mechanism, Incidence Calculus, which is based on a representation
of uncertainty using sets of points, which might represent situations, models
or possible worlds. Incidence Calculus does provide a probabilistic logic with
truth functional connectives.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:29 GMT""}]","2013-04-15"
"1304.3439","Benjamin N. Grosof","Benjamin N. Grosof","Evidential Confirmation as Transformed Probability","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-185-192","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  A considerable body of work in AI has been concerned with aggregating
measures of confirmatory and disconfirmatory evidence for a common set of
propositions. Claiming classical probability to be inadequate or inappropriate,
several researchers have gone so far as to invent new formalisms and methods.
We show how to represent two major such alternative approaches to evidential
confirmation not only in terms of transformed (Bayesian) probability, but also
in terms of each other. This unifies two of the leading approaches to
confirmation theory, by showing that a revised MYCIN Certainty Factor method
[12] is equivalent to a special case of Dempster-Shafer theory. It yields a
well-understood axiomatic basis, i.e. conditional independence, to interpret
previous work on quantitative confirmation theory. It substantially resolves
the ""taxe-them-or-leave-them"" problem of priors: MYCIN had to leave them out,
while PROSPECTOR had to have them in. It recasts some of confirmation theory's
advantages in terms of the psychological accessibility of probabilistic
information in different (transformed) formats. Finally, it helps to unify the
representation of uncertain reasoning (see also [11]).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:35 GMT""}]","2013-04-15"
"1304.3440","Ronald P. Loui","Ronald P. Loui","Interval-Based Decisions for Reasoning Systems","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-193-200","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This essay looks at decision-making with interval-valued probability
measures. Existing decision methods have either supplemented expected utility
methods with additional criteria of optimality, or have attempted to supplement
the interval-valued measures. We advocate a new approach, which makes the
following questions moot: 1. which additional criteria to use, and 2. how wide
intervals should be. In order to implement the approach, we need more
epistemological information. Such information can be generated by a rule of
acceptance with a parameter that allows various attitudes toward error, or can
simply be declared. In sketch, the argument is: 1. probability intervals are
useful and natural in All. systems; 2. wide intervals avoid error, but are
useless in some risk sensitive decision-making; 3. one may obtain narrower
intervals if one is less cautious; 4. if bodies of knowledge can be ordered by
their caution, one should perform the decision analysis with the acceptable
body of knowledge that is the most cautious, of those that are useful. The
resulting behavior differs from that of a behavioral probabilist (a Bayesian)
because in the proposal, 5. intervals based on successive bodies of knowledge
are not always nested; 6. if the agent uses a probability for a particular
decision, she need not commit to that probability for credence or future
decision; and 7. there may be no acceptable body of knowledge that is useful;
hence, sometimes no decision is mandated.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:41 GMT""}]","2013-04-15"
"1304.3441","James E. Corter","James E. Corter, Mark A. Gluck","Machine Generalization and Human Categorization: An
  Information-Theoretic View","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-201-207","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In designing an intelligent system that must be able to explain its reasoning
to a human user, or to provide generalizations that the human user finds
reasonable, it may be useful to take into consideration psychological data on
what types of concepts and categories people naturally use. The psychological
literature on concept learning and categorization provides strong evidence that
certain categories are more easily learned, recalled, and recognized than
others. We show here how a measure of the informational value of a category
predicts the results of several important categorization experiments better
than standard alternative explanations. This suggests that information-based
approaches to machine generalization may prove particularly useful and natural
for human users of the systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:47 GMT""}]","2013-04-15"
"1304.3442","Samuel Holtzman","Samuel Holtzman, John S. Breese","Exact Reasoning Under Uncertainty","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-208-216","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper focuses on designing expert systems to support decision making in
complex, uncertain environments. In this context, our research indicates that
strictly probabilistic representations, which enable the use of
decision-theoretic reasoning, are highly preferable to recently proposed
alternatives (e.g., fuzzy set theory and Dempster-Shafer theory). Furthermore,
we discuss the language of influence diagrams and a corresponding methodology
-decision analysis -- that allows decision theory to be used effectively and
efficiently as a decision-making aid. Finally, we use RACHEL, a system that
helps infertile couples select medical treatments, to illustrate the
methodology of decision analysis as basis for expert decision systems.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:53 GMT""}]","2013-04-15"
"1304.3443","Alf C. Zimmer","Alf C. Zimmer","The Estimation of Subjective Probabilities via Categorical Judgments of
  Uncertainty","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-217-224","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Theoretically as well as experimentally it is investigated how people
represent their knowledge in order to make decisions or to share their
knowledge with others. Experiment 1 probes into the ways how people 6ather
information about the frequencies of events and how the requested response
mode, that is, numerical vs. verbal estimates interferes with this knowledge.
The least interference occurs if the subjects are allowed to give verbal
responses. From this it is concluded that processing knowledge about
uncertainty categorically, that is, by means of verbal expressions, imposes
less mental work load on the decision matter than numerical processing.
Possibility theory is used as a framework for modeling the individual usage of
verbal categories for grades of uncertainty. The 'elastic' constraints on the
verbal expressions for every sing1e subject are determined in Experiment 2 by
means of sequential calibration. In further experiments it is shown that the
superiority of the verbal processing of knowledge about uncertainty guise
generally reduces persistent biases reported in the literature: conservatism
(Experiment 3) and neg1igence of regression (Experiment 4). The reanalysis of
Hormann's data reveal that in verbal Judgments people exhibit sensitivity for
base rates and are not prone to the conjunction fallacy. In a final experiment
(5) about predictions in a real-life situation it turns out that in a numerical
forecasting task subjects restricted themselves to those parts of their
knowledge which are numerical. On the other hand subjects in a verbal
forecasting task accessed verbally as well as numerically stated knowledge.
Forecasting is structurally related to the estimation of probabilities for rare
events insofar as supporting and contradicting arguments have to be evaluated
and the choice of the final Judgment has to be Justified according to the
evidence brought forward. In order to assist people in such choice situations a
formal model for the interactive checking of arguments has been developed. The
model transforms the normal-language quantifiers used in the arguments into
fuzzy numbers and evaluates the given train of arguments by means of fuzzy
numerica1 operations. Ambiguities in the meanings of quantifiers are resolved
interactively.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:57:59 GMT""}]","2013-04-15"
"1304.3444","Bruce Abramson","Bruce Abramson","A Cure for Pathological Behavior in Games that Use Minimax","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-225-231","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The traditional approach to choosing moves in game-playing programs is the
minimax procedure. The general belief underlying its use is that increasing
search depth improves play. Recent research has shown that given certain
simplifying assumptions about a game tree's structure, this belief is
erroneous: searching deeper decreases the probability of making a correct move.
This phenomenon is called game tree pathology. Among these simplifying
assumptions is uniform depth of win/loss (terminal) nodes, a condition which is
not true for most real games. Analytic studies in [10] have shown that if every
node in a pathological game tree is made terminal with probability exceeding a
certain threshold, the resulting tree is nonpathological. This paper considers
a new evaluation function which recognizes increasing densities of forced wins
at deeper levels in the tree. This property raises two points that strengthen
the hypothesis that uniform win depth causes pathology. First, it proves
mathematically that as search deepens, an evaluation function that does not
explicitly check for certain forced win patterns becomes decreasingly likely to
force wins. This failing predicts the pathological behavior of the original
evaluation function. Second, it shows empirically that despite recognizing
fewer mid-game wins than the theoretically predicted minimum, the new function
is nonpathological.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:05 GMT""}]","2013-04-15"
"1304.3445","Dana Nau","Dana Nau, Paul Purdom, Chun-Hung Tzeng","An Evaluation of Two Alternatives to Minimax","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-232-236","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the field of Artificial Intelligence, traditional approaches to choosing
moves in games involve the we of the minimax algorithm. However, recent
research results indicate that minimizing may not always be the best approach.
In this paper we summarize the results of some measurements on several model
games with several different evaluation functions. These measurements, which
are presented in detail in [NPT], show that there are some new algorithms that
can make significantly better use of evaluation function values than the
minimax algorithm does.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:11 GMT""}]","2013-04-15"
"1304.3446","Ross D. Shachter","Ross D. Shachter","Intelligent Probabilistic Inference","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-237-244","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The analysis of practical probabilistic models on the computer demands a
convenient representation for the available knowledge and an efficient
algorithm to perform inference. An appealing representation is the influence
diagram, a network that makes explicit the random variables in a model and
their probabilistic dependencies. Recent advances have developed solution
procedures based on the influence diagram. In this paper, we examine the
fundamental properties that underlie those techniques, and the information
about the probabilistic structure that is available in the influence diagram
representation. The influence diagram is a convenient representation for
computer processing while also being clear and non-mathematical. It displays
probabilistic dependence precisely, in a way that is intuitive for decision
makers and experts to understand and communicate. As a result, the same
influence diagram can be used to build, assess and analyze a model,
facilitating changes in the formulation and feedback from sensitivity analysis.
The goal in this paper is to determine arbitrary conditional probability
distributions from a given probabilistic model. Given qualitative information
about the dependence of the random variables in the model we can, for a
specific conditional expression, specify precisely what quantitative
information we need to be able to determine the desired conditional probability
distribution. It is also shown how we can find that probability distribution by
performing operations locally, that is, over subspaces of the joint
distribution. In this way, we can exploit the conditional independence present
in the model to avoid having to construct or manipulate the full joint
distribution. These results are extended to include maximal processing when the
information available is incomplete, and optimal decision making in an
uncertain environment. Influence diagrams as a computer-aided modeling tool
were developed by Miller, Merkofer, and Howard [5] and extended by Howard and
Matheson [2]. Good descriptions of how to use them in modeling are in Owen [7]
and Howard and Matheson [2]. The notion of solving a decision problem through
influence diagrams was examined by Olmsted [6] and such an algorithm was
developed by Shachter [8]. The latter paper also shows how influence diagrams
can be used to perform a variety of sensitivity analyses. This paper extends
those results by developing a theory of the properties of the diagram that are
used by the algorithm, and the information needed to solve arbitrary
probability inference problems. Section 2 develops the notation and the
framework for the paper and the relationship between influence diagrams and
joint probability distributions. The general probabilistic inference problem is
posed in Section 3. In Section 4 the transformations on the diagram are
developed and then put together into a solution procedure in Section 5. In
Section 6, this procedure is used to calculate the information requirement to
solve an inference problem and the maximal processing that can be performed
with incomplete information. Section 7 contains a summary of results.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:16 GMT""}]","2013-04-15"
"1304.3447","David Sher","David Sher","Developing and Analyzing Boundary Detection Operators Using
  Probabilistic Models","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-245-252","cs.CV","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Most feature detectors such as edge detectors or circle finders are
statistical, in the sense that they decide at each point in an image about the
presence of a feature, this paper describes the use of Bayesian feature
detectors.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:23 GMT""}]","2013-04-15"
"1304.3448","John Fox","John Fox","Strong & Weak Methods: A Logical View of Uncertainty","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-253-257","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The last few years has seen a growing debate about techniques for managing
uncertainty in AI systems. Unfortunately this debate has been cast as a rivalry
between AI methods and classical probability based ones. Three arguments for
extending the probability framework of uncertainty are presented, none of which
imply a challenge to classical methods. These are (1) explicit representation
of several types of uncertainty, specifically possibility and plausibility, as
well as probability, (2) the use of weak methods for uncertainty management in
problems which are poorly defined, and (3) symbolic representation of different
uncertainty calculi and methods for choosing between them.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:28 GMT""}]","2013-04-15"
"1304.3449","Lester Ingber","Lester Ingber","Statistical Mechanics Algorithm for Response to Targets (SMART)","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-258-264","cs.CE cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  It is proposed to apply modern methods of nonlinear nonequilibrium
statistical mechanics to develop software algorithms that will optimally
respond to targets within short response times with minimal computer resources.
This Statistical Mechanics Algorithm for Response to Targets (SMART) can be
developed with a view towards its future implementation into a hardwired
Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed
of response to targets (SMART_SAM).
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:34 GMT""}]","2013-04-15"
"1304.3450","Tod S. Levitt","Tod S. Levitt","Probabilistic Conflict Resolution in Hierarchical Hypothesis Spaces","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-265-272","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  Artificial intelligence applications such as industrial robotics, military
surveillance, and hazardous environment clean-up, require situation
understanding based on partial, uncertain, and ambiguous or erroneous evidence.
It is necessary to evaluate the relative likelihood of multiple possible
hypotheses of the (current) situation faced by the decision making program.
Often, the evidence and hypotheses are hierarchical in nature. In image
understanding tasks, for example, evidence begins with raw imagery, from which
ambiguous features are extracted which have multiple possible aggregations
providing evidential support for the presence of multiple hypothesis of objects
and terrain, which in turn aggregate in multiple ways to provide partial
evidence for different interpretations of the ambient scene. Information fusion
for military situation understanding has a similar evidence/hypothesis
hierarchy from multiple sensor through message level interpretations, and also
provides evidence at multiple levels of the doctrinal hierarchy of military
forces.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:40 GMT""}]","2013-04-15"
"1304.3451","Gerald Shao-Hung Liu","Gerald Shao-Hung Liu","Knowledge Structures and Evidential Reasoning in Decision Analysis","Appears in Proceedings of the First Conference on Uncertainty in
  Artificial Intelligence (UAI1985)",,,"UAI-P-1985-PG-273-282","cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The roles played by decision factors in making complex subject are decisions
are characterized by how these factors affect the overall decision. Evidence
that partially matches a factor is evaluated, and then effective computational
rules are applied to these roles to form an appropriate aggregation of the
evidence. The use of this technique supports the expression of deeper levels of
causality, and may also preserve the cognitive structure of the decision maker
better than the usual weighting methods, certainty-factor or other
probabilistic models can.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 19:58:46 GMT""}]","2013-04-15"
"1304.7217","Oleg Kupervasser","Oleg Kupervasser, Alexander Rubinstein","Correction of inertial navigation system's errors by the help of
  video-based navigator based on Digital Terrarium Map","32 pages, 18 figures, Positioning Vol.4 No.1, February 2013. arXiv
  admin note: substantial text overlap with arXiv:1107.0399, arXiv:1107.1470,
  arXiv:1106.6341",,"10.4236/pos.2013.41010",,"cs.SY","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  This paper deals with the error analysis of a novel navigation algorithm that
uses as input the sequence of images acquired from a moving camera and a
Digital Terrain (or Elevation) Map (DTM/DEM). More specifically, it has been
shown that the optical flow derived from two consecutive camera frames can be
used in combination with a DTM to estimate the position, orientation and
ego-motion parameters of the moving camera. As opposed to previous works, the
proposed approach does not require an intermediate explicit reconstruction of
the 3D world. In the present work the sensitivity of the algorithm outlined
above is studied. The main sources for errors are identified to be the
optical-flow evaluation and computation, the quality of the information about
the terrain, the structure of the observed terrain and the trajectory of the
camera. By assuming appropriate characterization of these error sources, a
closed form expression for the uncertainty of the pose and motion of the camera
is first developed and then the influence of these factors is confirmed using
extensive numerical simulations. The main conclusion of this paper is to
establish that the proposed navigation algorithm generates accurate estimates
for reasonable scenarios and error sources, and thus can be effectively used as
part of a navigation system of autonomous vehicles.
","[{""version"":""v1"",""created"":""Wed, 27 Mar 2013 20:46:49 GMT""}]","2013-04-29"
"1304.7244","Henning Schnoor","Rudolf Berghammer and Henning Schnoor","Relation-algebraic and Tool-supported Control of Condorcet Voting",,,,,"cs.GT cs.AI","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  We present a relation-algebraic model of Condorcet voting and, based on it,
relation-algebraic solutions of the constructive control problem via the
removal of voters.
  We consider two winning conditions, viz. to be a Condorcet winner and to be
in the (Gilles resp. upward) uncovered set. For the first condition the control
problem is known to be NP-hard; for the second condition the NP-hardness of the
control problem is shown in the paper. All relation-algebraic specifications we
will develop in the paper immediately can be translated into the programming
language of the BDD-based computer system RelView. Our approach is very
flexible and especially appropriate for prototyping and experimentation, and as
such very instructive for educational purposes. It can easily be applied to
other voting rules and control problems.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 11:17:46 GMT""}]","2013-04-29"
"1304.8001","LiGang Hou","L. G. Hou and J. L. Han","The spiral structure of our Milky Way","To appear in IAUS 292, ""Molecular Gas, Dust, and Star Formation in
  Galaxies"", 1 page, 1 figure",,"10.1017/S1743921313000665",,"astro-ph.GA","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  The spiral structure of our Milky Way has not yet been well outlined. HII
regions, giant molecular clouds (GMCs) and 6.7-GHz methanol masers are primary
tracers for spiral arms. We collect and update the database of these tracers
which has been used in Hou, Han & Shi (2009) for mapping the spiral structure.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 02:06:01 GMT""}]","2013-05-01"
"1305.6505","Germano D'Abramo","Germano D'Abramo","The Demon in a vacuum tube","16 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1101.5056","Entropy 2013; 15(5):1916-1928","10.3390/e15051916",,"physics.class-ph","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","  In the present paper, several issues concerning the second law of
thermodynamics, Maxwell's demon and Landauer's principle are dealt with. I
argue that if the demon and the system on which it operates without dissipation
of external energy are made of atoms and molecules (gas, liquid or solid) in
thermal equilibrium (whose behaviour is described by a canonical distribution),
then the unavoidable reason why the demon cannot successfully operate resides
in the ubiquity of thermal fluctuations and friction. Landauer's principle
appears to be unnecessary. I also suggest that if the behaviour of the demon
and the system on which it acts is not always describable by a canonical
distribution, as would happen for instance with the ballistic motion of
electrons at early stages of thermionic emission, then a successful working
demon cannot be ruled out a priori. A critical review of two recent experiments
on thermionic emission Maxwell's demons is also given.
","[{""version"":""v1"",""created"":""Thu, 28 Mar 2013 09:41:28 GMT""},{""version"":""v2"",""created"":""Mon, 3 Jun 2013 14:11:17 GMT""}]","2013-06-04"
